[ns_server:info,2020-03-03T11:33:31.915+05:30,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-03-03T11:33:31.930+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-03-03T11:33:31.930+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.930+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.930+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.930+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.930+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.930+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.930+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.930+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.930+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.930+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.930+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.930+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.930+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.930+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.931+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.931+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.931+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.931+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.931+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.931+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.931+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.931+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.931+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.931+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.931+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-03-03T11:33:31.931+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-03-03T11:33:31.935+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-03-03T11:33:31.935+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-03T11:33:31.935+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:init:196]ip config not found. Looks like we're brand new node
[ns_server:info,2020-03-03T11:33:31.937+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2020-03-03T11:33:31.947+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:31.947+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:31.947+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.169.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:31.949+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:31.951+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2020-03-03T11:33:31.950+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.174.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:31.951+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.175.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:31.951+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:31.951+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.168.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:31.952+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.176.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:31.952+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.177.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:31.953+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:31.954+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-03-03T11:33:31.954+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-03-03T11:33:31.954+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[error_logger:info,2020-03-03T11:33:31.958+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:31.959+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-03-03T11:33:31.959+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.167.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-03-03T11:33:31.960+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-03-03T11:33:31.964+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-03-03T11:33:31.964+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2020-03-03T11:33:31.964+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:31.964+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:33:31.964+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1044975193.1441792003.213608>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:31.964+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1044975193.1441792003.213608>,
                                  inet_tcp_dist,<0.183.0>,
                                  #Ref<0.1044975193.1441792003.213613>}
[ns_server:debug,2020-03-03T11:33:31.969+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-03-03T11:33:31.969+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": ok
[ns_server:info,2020-03-03T11:33:31.969+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-03T11:33:31.971+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-03-03T11:33:31.971+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:31.975+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.186.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:31.977+05:30,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,3,3},{11,33,31}}},
               {memory,
                   [{total,26458608},
                    {processes,9668992},
                    {processes_used,9663680},
                    {system,16789616},
                    {atom,388625},
                    {atom_used,364408},
                    {binary,113400},
                    {code,8250921},
                    {ets,1504320}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    error_handler,application,application_master,heart,kernel,
                    application_controller,lists,file,code_server,file_server,
                    proc_lib,gen,erl_lint,code,supervisor,error_logger,
                    erl_parse,gen_server,file_io_server,ets,erl_eval,
                    gen_event,filename,erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{os_mon,"CPO  CXC 138 46","2.4.4"},
                    {sasl,"SASL  CXC 138 11","3.1.2"},
                    {ns_server,"Couchbase server","6.5.0-4960-enterprise"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ale,"Another Logger for Erlang","0.0.0"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,129},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,auth,httpd_sup,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    release_handler,lhttpc_sup,httpc_sup,lhttpc_manager,
                    alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    'sink-disk_xdcr',timer_server,'sink-disk_debug',ale_sup,
                    'sink-disk_error',inet_db,'sink-disk_default',
                    ssl_pem_cache_dist,ale_dynamic_sup,rex,global_group,
                    net_sup,kernel_sup,ssl_connection_sup,kernel_safe_sup,
                    global_name_server,ssl_admin_sup,tftp_sup,ssl_sup,
                    root_sup,erts_code_purger,os_mon_sup,file_server_2,
                    error_logger,cpu_sup,erl_epmd,init,memsup,
                    erl_signal_server,net_kernel,disksup,ale,dist_manager,
                    ssl_pem_cache,ssl_manager,ssl_dist_admin_sup,
                    ssl_dist_connection_sup,ssl_dist_sup,user,
                    ssl_tls_dist_proxy,ssl_manager_dist,sasl_safe_sup,
                    ssl_listen_tracker_sup,code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,0}]
[ns_server:info,2020-03-03T11:33:31.981+05:30,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"cfa0f75f28402d2e1aa254b2a374bead19433526\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" upstream=\"master\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"f2a16b53bb74146f20d18ba2c0443d5f10a9a550\" upstream=\"master\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4960\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" upstream=\"master\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" upstream=\"mad-hatter\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" upstream=\"master\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" upstream=\"master\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" upstream=\"master\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" upstream=\"mad-hatter\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" upstream=\"cb-master\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" upstream=\"master\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" upstream=\"master\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" upstream=\"mad-hatter\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" upstream=\"mad-hatter\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" upstream=\"mad-hatter\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" upstream=\"master\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" upstream=\"master\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" upstream=\"mad-hatter\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" upstream=\"mad-hatter\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" upstream=\"mad-hatter\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" upstream=\"mad-hatter\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" upstream=\"master\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" upstream=\"master\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" upstream=\"master\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" upstream=\"mad-hatter\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" upstream=\"master\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"7a3aa547b3f5eb3ea28d279a08384609cd2cea7c\" upstream=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" upstream=\"mad-hatter\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" upstream=\"master\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" upstream=\"master\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" upstream=\"master\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" upstream=\"v1.1.0-couchbase\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"ee64d41320d14fabe814a241a5cf4f6a6f6e827a\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" upstream=\"master\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" upstream=\"master\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-03-03T11:33:31.984+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.187.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:31.984+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:31.985+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:31.985+05:30,ns_1@cb.local:ns_config_sup<0.190.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-03-03T11:33:31.985+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:31.985+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:32.004+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-03-03T11:33:32.005+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:info,2020-03-03T11:33:32.005+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1125]No dynamic config file found. Assuming we're brand new node
[ns_server:debug,2020-03-03T11:33:32.008+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[]]
[ns_server:info,2020-03-03T11:33:32.011+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   false]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9110]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8095]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9140]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8096]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9130]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8094]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9100]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18093]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9999]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18092]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8092]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   11206]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9998]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18091]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,8091},
   {port_meta,global}]},
 {rest,[{port,8091}]},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {drop_request_memory_threshold_mib,undefined},
 {{request_limit,capi},undefined},
 {{request_limit,rest},undefined},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {replication,[{enabled,true}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
 {{node,'ns_1@cb.local',moxi},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,0}]},
 {secure_headers,[]},
 {buckets,[{configs,[]}]},
 {cbas_memory_quota,2174},
 {fts_memory_quota,512},
 {memory_quota,8886},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {memcached,[]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {remote_clusters,[]},
 {rest_creds,null},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {{node,'ns_1@cb.local',membership},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   active]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
 {quorum_nodes,['ns_1@cb.local']},
 {nodes_wanted,['ns_1@cb.local']},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {max_bucket_count,30},
 {index_aware_rebalance_disabled,false},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   true]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   true]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {6,5}]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   <<"e32a4d3bd8aa759a4b96cd6ac25889ee">>]}]
[error_logger:info,2020-03-03T11:33:32.014+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:32.014+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.199.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:32.015+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.200.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:32.015+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:33:32.017+05:30,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',erl_external_listeners} ->
[{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
 {inet,false},
 {inet6,false}]
[error_logger:info,2020-03-03T11:33:32.017+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.202.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:33:32.017+05:30,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',node_encryption} ->
[{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|false]
[ns_server:debug,2020-03-03T11:33:32.017+05:30,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',address_family} ->
[{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|inet]
[ns_server:debug,2020-03-03T11:33:32.017+05:30,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>} ->
[{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]
[error_logger:info,2020-03-03T11:33:32.019+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.205.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:32.024+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.208.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:32.025+05:30,ns_1@cb.local:menelaus_barrier<0.209.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-03-03T11:33:32.025+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.209.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:32.025+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:32.026+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:32.027+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.213.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:32.288+05:30,ns_1@cb.local:<0.218.0>:goport:handle_eof:582]Stream 'stdout' closed
[ns_server:debug,2020-03-03T11:33:32.288+05:30,ns_1@cb.local:<0.218.0>:goport:handle_eof:582]Stream 'stderr' closed
[ns_server:info,2020-03-03T11:33:32.288+05:30,ns_1@cb.local:<0.218.0>:goport:handle_process_exit:563]Port exited with status 0.
[ns_server:debug,2020-03-03T11:33:32.301+05:30,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_server_cert:generate_cert_and_pkey:83]Generated certificate and private key in 272585 us
[ns_server:debug,2020-03-03T11:33:32.301+05:30,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cert_and_pkey ->
[{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
 {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFfi2B3wIO/gwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAyYWJmMjVlZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMmFiZjI1\nZWUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDI7xEpYzw8VsEaLCx3\nQQVbkzsO6PmRhi08x2I8YCA1DbAT1zVEJIkEG1u91CWD7eAhWsCD3TWwBFZfcERe\n4yqxtt5zpsN84LQXkd18MWeFYeZCHlb"...>>,
  <<"*****">>}]
[ns_server:debug,2020-03-03T11:33:32.301+05:30,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>} ->
[{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{2,63750434612}}]}]
[ns_server:info,2020-03-03T11:33:32.305+05:30,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:maybe_generate_local_cert:620]Failed to read node certificate. Perhaps it wasn't created yet. Error: {error,
                                                                        {badmatch,
                                                                         {error,
                                                                          enoent}}}
[ns_server:debug,2020-03-03T11:33:32.495+05:30,ns_1@cb.local:<0.222.0>:goport:handle_eof:582]Stream 'stderr' closed
[ns_server:debug,2020-03-03T11:33:32.495+05:30,ns_1@cb.local:<0.222.0>:goport:handle_eof:582]Stream 'stdout' closed
[ns_server:info,2020-03-03T11:33:32.495+05:30,ns_1@cb.local:<0.222.0>:goport:handle_process_exit:563]Port exited with status 0.
[ns_server:info,2020-03-03T11:33:32.503+05:30,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:do_generate_local_cert:608]Saved local cert for node 'ns_1@cb.local'
[ns_server:debug,2020-03-03T11:33:32.510+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2020-03-03T11:33:32.510+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2020-03-03T11:33:32.510+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2020-03-03T11:33:32.520+05:30,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:init:462]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,248,182,7,124,8,59,
             248,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
             6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
             118,101,114,32,50,97,98,102,50,53,101,101,48,30,23,13,49,51,48,
             49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
             57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
             104,98,97,115,101,32,83,101,114,118,101,114,32,50,97,98,102,50,
             53,101,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,
             3,130,1,15,0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,60,60,86,
             193,26,44,44,119,65,5,91,147,59,14,232,249,145,134,45,60,199,98,
             60,96,32,53,13,176,19,215,53,68,36,137,4,27,91,189,212,37,131,
             237,224,33,90,192,131,221,53,176,4,86,95,112,68,94,227,42,177,
             182,222,115,166,195,124,224,180,23,145,221,124,49,103,133,97,
             230,66,30,86,238,151,131,123,94,27,56,61,171,209,206,53,165,109,
             57,60,66,30,45,76,210,27,138,129,108,228,155,143,205,230,58,114,
             198,150,68,170,210,222,61,249,145,141,251,150,25,143,207,193,14,
             128,174,48,57,221,69,163,187,221,180,65,120,194,21,213,252,165,
             191,127,194,73,189,142,183,240,180,156,43,97,132,100,172,245,16,
             255,71,110,103,220,9,135,254,252,221,45,169,65,241,218,72,51,
             247,194,65,155,116,191,232,65,27,197,107,213,172,80,40,176,72,
             79,108,13,208,111,177,148,188,139,143,17,168,71,64,58,161,47,
             154,81,195,225,255,41,211,168,44,88,2,177,165,152,147,201,226,1,
             159,67,57,50,235,12,36,8,36,65,51,19,63,177,155,8,245,159,208,
             23,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,
             164,48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,
             3,85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,134,72,134,247,
             13,1,1,11,5,0,3,130,1,1,0,162,140,210,87,119,97,248,23,114,150,
             69,187,249,72,156,70,55,139,123,172,82,139,226,69,218,245,228,
             109,173,246,181,9,49,203,83,240,183,251,106,41,26,19,240,211,
             126,120,73,53,230,19,140,218,5,236,233,231,195,56,53,104,213,
             129,90,62,185,251,98,143,77,57,199,76,191,117,164,20,183,76,201,
             95,172,148,121,94,120,237,28,101,169,175,226,195,107,186,72,229,
             145,50,31,155,120,222,47,94,33,119,79,118,95,33,102,114,199,76,
             233,27,210,39,208,73,174,8,72,186,22,5,105,195,119,145,168,53,
             48,123,157,157,15,29,13,23,53,103,173,149,77,40,77,218,26,83,
             222,38,29,85,38,30,152,178,132,3,177,58,244,159,153,3,114,62,
             197,223,4,148,157,75,214,79,25,128,121,122,61,32,116,135,0,172,
             225,170,9,103,193,164,52,121,181,151,55,240,180,221,156,134,79,
             19,141,156,195,178,163,120,64,156,142,88,132,178,20,2,92,191,
             156,40,102,105,148,195,189,224,203,229,50,219,137,240,63,92,189,
             212,41,239,149,130,27,100,76,202,197,74,244,38,188,204,52,63,3,
             57>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2020-03-03T11:33:32.521+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.214.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:32.532+05:30,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-03T11:33:32.532+05:30,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-03T11:33:32.532+05:30,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-03T11:33:32.532+05:30,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-03-03T11:33:32.552+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.224.0>,menelaus_web}
             started: [{pid,<0.225.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,
                                 248,182,7,124,8,59,248,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,50,97,98,
                                 102,50,53,101,101,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,50,
                                 97,98,102,50,53,101,101,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,
                                 60,60,86,193,26,44,44,119,65,5,91,147,59,14,
                                 232,249,145,134,45,60,199,98,60,96,32,53,13,
                                 176,19,215,53,68,36,137,4,27,91,189,212,37,
                                 131,237,224,33,90,192,131,221,53,176,4,86,
                                 95,112,68,94,227,42,177,182,222,115,166,195,
                                 124,224,180,23,145,221,124,49,103,133,97,
                                 230,66,30,86,238,151,131,123,94,27,56,61,
                                 171,209,206,53,165,109,57,60,66,30,45,76,
                                 210,27,138,129,108,228,155,143,205,230,58,
                                 114,198,150,68,170,210,222,61,249,145,141,
                                 251,150,25,143,207,193,14,128,174,48,57,221,
                                 69,163,187,221,180,65,120,194,21,213,252,
                                 165,191,127,194,73,189,142,183,240,180,156,
                                 43,97,132,100,172,245,16,255,71,110,103,220,
                                 9,135,254,252,221,45,169,65,241,218,72,51,
                                 247,194,65,155,116,191,232,65,27,197,107,
                                 213,172,80,40,176,72,79,108,13,208,111,177,
                                 148,188,139,143,17,168,71,64,58,161,47,154,
                                 81,195,225,255,41,211,168,44,88,2,177,165,
                                 152,147,201,226,1,159,67,57,50,235,12,36,8,
                                 36,65,51,19,63,177,155,8,245,159,208,23,2,3,
                                 1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 162,140,210,87,119,97,248,23,114,150,69,187,
                                 249,72,156,70,55,139,123,172,82,139,226,69,
                                 218,245,228,109,173,246,181,9,49,203,83,240,
                                 183,251,106,41,26,19,240,211,126,120,73,53,
                                 230,19,140,218,5,236,233,231,195,56,53,104,
                                 213,129,90,62,185,251,98,143,77,57,199,76,
                                 191,117,164,20,183,76,201,95,172,148,121,94,
                                 120,237,28,101,169,175,226,195,107,186,72,
                                 229,145,50,31,155,120,222,47,94,33,119,79,
                                 118,95,33,102,114,199,76,233,27,210,39,208,
                                 73,174,8,72,186,22,5,105,195,119,145,168,53,
                                 48,123,157,157,15,29,13,23,53,103,173,149,
                                 77,40,77,218,26,83,222,38,29,85,38,30,152,
                                 178,132,3,177,58,244,159,153,3,114,62,197,
                                 223,4,148,157,75,214,79,25,128,121,122,61,
                                 32,116,135,0,172,225,170,9,103,193,164,52,
                                 121,181,151,55,240,180,221,156,134,79,19,
                                 141,156,195,178,163,120,64,156,142,88,132,
                                 178,20,2,92,191,156,40,102,105,148,195,189,
                                 224,203,229,50,219,137,240,63,92,189,212,41,
                                 239,149,130,27,100,76,202,197,74,244,38,188,
                                 204,52,63,3,57>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:32.554+05:30,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-03T11:33:32.554+05:30,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-03T11:33:32.554+05:30,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-03T11:33:32.554+05:30,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2020-03-03T11:33:32.555+05:30,ns_1@cb.local:<0.223.0>:restartable:start_child:98]Started child process <0.224.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2020-03-03T11:33:32.555+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.224.0>,menelaus_web}
             started: [{pid,<0.243.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,
                                 248,182,7,124,8,59,248,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,50,97,98,
                                 102,50,53,101,101,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,50,
                                 97,98,102,50,53,101,101,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,
                                 60,60,86,193,26,44,44,119,65,5,91,147,59,14,
                                 232,249,145,134,45,60,199,98,60,96,32,53,13,
                                 176,19,215,53,68,36,137,4,27,91,189,212,37,
                                 131,237,224,33,90,192,131,221,53,176,4,86,
                                 95,112,68,94,227,42,177,182,222,115,166,195,
                                 124,224,180,23,145,221,124,49,103,133,97,
                                 230,66,30,86,238,151,131,123,94,27,56,61,
                                 171,209,206,53,165,109,57,60,66,30,45,76,
                                 210,27,138,129,108,228,155,143,205,230,58,
                                 114,198,150,68,170,210,222,61,249,145,141,
                                 251,150,25,143,207,193,14,128,174,48,57,221,
                                 69,163,187,221,180,65,120,194,21,213,252,
                                 165,191,127,194,73,189,142,183,240,180,156,
                                 43,97,132,100,172,245,16,255,71,110,103,220,
                                 9,135,254,252,221,45,169,65,241,218,72,51,
                                 247,194,65,155,116,191,232,65,27,197,107,
                                 213,172,80,40,176,72,79,108,13,208,111,177,
                                 148,188,139,143,17,168,71,64,58,161,47,154,
                                 81,195,225,255,41,211,168,44,88,2,177,165,
                                 152,147,201,226,1,159,67,57,50,235,12,36,8,
                                 36,65,51,19,63,177,155,8,245,159,208,23,2,3,
                                 1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 162,140,210,87,119,97,248,23,114,150,69,187,
                                 249,72,156,70,55,139,123,172,82,139,226,69,
                                 218,245,228,109,173,246,181,9,49,203,83,240,
                                 183,251,106,41,26,19,240,211,126,120,73,53,
                                 230,19,140,218,5,236,233,231,195,56,53,104,
                                 213,129,90,62,185,251,98,143,77,57,199,76,
                                 191,117,164,20,183,76,201,95,172,148,121,94,
                                 120,237,28,101,169,175,226,195,107,186,72,
                                 229,145,50,31,155,120,222,47,94,33,119,79,
                                 118,95,33,102,114,199,76,233,27,210,39,208,
                                 73,174,8,72,186,22,5,105,195,119,145,168,53,
                                 48,123,157,157,15,29,13,23,53,103,173,149,
                                 77,40,77,218,26,83,222,38,29,85,38,30,152,
                                 178,132,3,177,58,244,159,153,3,114,62,197,
                                 223,4,148,157,75,214,79,25,128,121,122,61,
                                 32,116,135,0,172,225,170,9,103,193,164,52,
                                 121,181,151,55,240,180,221,156,134,79,19,
                                 141,156,195,178,163,120,64,156,142,88,132,
                                 178,20,2,92,191,156,40,102,105,148,195,189,
                                 224,203,229,50,219,137,240,63,92,189,212,41,
                                 239,149,130,27,100,76,202,197,74,244,38,188,
                                 204,52,63,3,57>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:32.556+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.223.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:32.556+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:32.563+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.261.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:32.564+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.264.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:32.573+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.266.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:32.574+05:30,ns_1@cb.local:users_replicator<0.266.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-03-03T11:33:32.576+05:30,ns_1@cb.local:users_storage<0.267.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.266.0>
[ns_server:debug,2020-03-03T11:33:32.576+05:30,ns_1@cb.local:users_replicator<0.266.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.267.0>
[ns_server:debug,2020-03-03T11:33:32.577+05:30,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-03-03T11:33:32.577+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.267.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:32.577+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.265.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:33:32.578+05:30,ns_1@cb.local:compiled_roles_cache<0.269.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-03-03T11:33:32.578+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.269.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:32.585+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.272.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:32.585+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.263.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:32.588+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.276.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:32.588+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.277.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:32.602+05:30,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-03-03T11:33:32.602+05:30,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 24ms
[error_logger:info,2020-03-03T11:33:32.606+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.275.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:32.606+05:30,ns_1@cb.local:wait_link_to_couchdb_node<0.280.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[error_logger:info,2020-03-03T11:33:32.606+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:32.606+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:33:32.606+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1044975193.1441792004.213829>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:32.607+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1044975193.1441792004.213829>,
                                  inet_tcp_dist,<0.283.0>,
                                  #Ref<0.1044975193.1441792004.213833>}
[ns_server:debug,2020-03-03T11:33:32.607+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1044975193.1441792004.213829>,
                               inet_tcp_dist,<0.283.0>,
                               #Ref<0.1044975193.1441792004.213833>}
[error_logger:info,2020-03-03T11:33:32.607+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.283.0>,shutdown}}
[error_logger:info,2020-03-03T11:33:32.607+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:32.607+05:30,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-03-03T11:33:32.608+05:30,ns_1@cb.local:users_replicator<0.266.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2020-03-03T11:33:32.810+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:32.810+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:33:32.810+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1044975193.1441792004.213844>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:32.810+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1044975193.1441792004.213844>,
                                  inet_tcp_dist,<0.286.0>,
                                  #Ref<0.1044975193.1441792004.213846>}
[ns_server:debug,2020-03-03T11:33:32.812+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1044975193.1441792004.213844>,
                               inet_tcp_dist,<0.286.0>,
                               #Ref<0.1044975193.1441792004.213846>}
[error_logger:info,2020-03-03T11:33:32.812+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.286.0>,shutdown}}
[ns_server:debug,2020-03-03T11:33:32.812+05:30,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:33:32.812+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:33.013+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:33:33.013+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1044975193.1441792003.213786>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:33.013+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1044975193.1441792003.213786>,
                                  inet_tcp_dist,<0.289.0>,
                                  #Ref<0.1044975193.1441792003.213790>}
[error_logger:info,2020-03-03T11:33:33.013+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:33.048+05:30,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-03T11:33:33.250+05:30,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-03T11:33:33.450+05:30,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-03-03T11:33:33.659+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.294.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:33.738+05:30,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-03T11:33:33.750+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1044975193.1441792003.213786>,
                               inet_tcp_dist,<0.289.0>,
                               #Ref<0.1044975193.1441792003.213790>}
[error_logger:info,2020-03-03T11:33:33.750+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.289.0>,connection_closed}}
[ns_server:info,2020-03-03T11:33:33.859+05:30,ns_1@cb.local:ns_couchdb_port<0.275.0>:ns_port_server:log:224]ns_couchdb<0.275.0>: Apache CouchDB  (LogLevel=info) is starting.
ns_couchdb<0.275.0>: Failure to start Mochiweb: eaddrinuse
ns_couchdb<0.275.0>: 3969: Booted. Waiting for shutdown request
ns_couchdb<0.275.0>: [os_mon] memory supervisor port (memsup): Erlang has closed
ns_couchdb<0.275.0>: [os_mon] cpu supervisor port (cpu_sup): Erlang has closed

[ns_server:debug,2020-03-03T11:33:33.939+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-03T11:33:33.939+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:33.939+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1044975193.1441792004.213864>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:33.939+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1044975193.1441792004.213864>,
                                  inet_tcp_dist,<0.296.0>,
                                  #Ref<0.1044975193.1441792003.213794>}
[ns_server:debug,2020-03-03T11:33:33.940+05:30,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-03-03T11:33:33.940+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1044975193.1441792004.213864>,
                               inet_tcp_dist,<0.296.0>,
                               #Ref<0.1044975193.1441792003.213794>}
[error_logger:info,2020-03-03T11:33:33.940+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.296.0>,shutdown}}
[error_logger:info,2020-03-03T11:33:33.940+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-03-03T11:33:34.140+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:34.140+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:33:34.140+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1044975193.1441792004.213880>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:34.140+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1044975193.1441792004.213880>,
                                  inet_tcp_dist,<0.299.0>,
                                  #Ref<0.1044975193.1441792004.213882>}
[ns_server:debug,2020-03-03T11:33:34.141+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1044975193.1441792004.213880>,
                               inet_tcp_dist,<0.299.0>,
                               #Ref<0.1044975193.1441792004.213882>}
[error_logger:info,2020-03-03T11:33:34.141+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.299.0>,shutdown}}
[ns_server:debug,2020-03-03T11:33:34.141+05:30,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:33:34.141+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-03-03T11:33:34.341+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:34.341+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:33:34.341+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1044975193.1441792003.213808>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:34.341+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1044975193.1441792003.213808>,
                                  inet_tcp_dist,<0.302.0>,
                                  #Ref<0.1044975193.1441792004.213885>}
[ns_server:debug,2020-03-03T11:33:34.342+05:30,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-03-03T11:33:34.342+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1044975193.1441792003.213808>,
                               inet_tcp_dist,<0.302.0>,
                               #Ref<0.1044975193.1441792004.213885>}
[error_logger:info,2020-03-03T11:33:34.342+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.302.0>,shutdown}}
[error_logger:info,2020-03-03T11:33:34.342+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:info,2020-03-03T11:33:34.383+05:30,ns_1@cb.local:ns_couchdb_port<0.275.0>:ns_port_server:log:224]ns_couchdb<0.275.0>: {"Kernel pid terminated",application_controller,"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\"},{line,102}]},{supervisor,do_start_child,2,[{file,\"supervisor.erl\"},{line,365}]},{supervisor,start_children,3,[{file,\"supervisor.erl\"},{line,348}]},{supervisor,init_children,2,[{file,\"supervisor.erl\"},{line,314}]},{gen_server,init_it,2,[{file,\"gen_server.erl\"},{line,365}]},{gen_server,init_it,6,[{file,\"gen_server.erl\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}"}
ns_couchdb<0.275.0>: Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro
ns_couchdb<0.275.0>: 
ns_couchdb<0.275.0>: Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done

[error_logger:error,2020-03-03T11:33:34.383+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]** Generic server ns_couchdb_port terminating 
** Last message in was {#Port<0.5119>,{exit_status,1}}
** When Server state == {state,#Port<0.5119>,
                            {ns_couchdb,"/opt/couchbase/lib/erlang/bin/erl",
                                ["-pa",
                                 "/opt/couchbase/lib/erlang/lib/asn1-5.0.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/compiler-7.1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEvent-2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEventDomain-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosFileTransfer-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosNotification-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosProperty-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTime-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTransactions-1.3.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/crypto-4.2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/dialyzer-3.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/diameter-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/edoc-0.9.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eldap-1.2.3.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_docgen-0.7.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_interface-3.10.2.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erts-9.3.3.9/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eunit-2.3.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/hipe-3.17.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ic-4.4.4.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/inets-6.5.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/mnesia-4.15.3.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/orber-3.8.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/os_mon-2.4.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/otp_mibs-1.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/parsetools-2.1.6/ebin",
                                 "/opt/couchbase/lib/erlang/lib/public_key-1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/reltool-0.7.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/runtime_tools-1.12.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/sasl-3.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/snmp-5.2.11/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssh-4.6.9.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssl-8.2.6.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/syntax_tools-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/tools-2.11.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/xmerl-1.3.16.1/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/gc-couchbase-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/vtree-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/wkb-1.2.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch-1.2.0a-961ad59-git/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_audit-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_dcp-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_index_merger-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_set_view-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_view_parser-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/ejson-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/erlang-oauth/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/etap/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/lhttpc-1.3/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mapreduce-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mochiweb-1.4.1/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/snappy-1.0.4/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ale/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/gen_smtp/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_babysitter/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_couchdb/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/ebin",
                                 "/opt/couchbase/lib/erlang/lib/stdlib-3.4.5.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/kernel-5.4.3.2/ebin",
                                 ".","-couch_ini",
                                 "/opt/couchbase/etc/couchdb/default.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/capi.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/geocouch.ini",
                                 "/opt/couchbase/etc/couchdb/local.ini",
                                 "-kernel","error_logger","false","-kernel",
                                 "error_logger","false","inetrc",
                                 "\"/opt/couchbase/etc/couchbase/hosts.cfg\"",
                                 "dist_config_file",
                                 "\"/opt/couchbase/var/lib/couchbase/config/dist_cfg\"",
                                 "-ssl_dist_optfile",
                                 "/opt/couchbase/etc/couchbase/ssl_dist_opts",
                                 "-setcookie",
                                 "dce5392bcba7669ee9f057b78581e574cccf9efc1961f2376ff32b0a61220948",
                                 "-name","couchdb_ns_1@cb.local","-smp",
                                 "enable","+P","327680","+K","true","-kernel",
                                 "error_logger","false","-sasl",
                                 "sasl_error_logger","false","-nouser",
                                 "-hidden","-proto_dist","cb","-epmd_module",
                                 "cb_epmd","-start_epmd","false","-run",
                                 "child_erlang","child_start","ns_couchdb"],
                                [use_stdio,
                                 {env,
                                     [{"NS_COUCHDB_ENV_ARGS",
                                       "[{ns_server_node,'ns_1@cb.local'},\n {path_config_tmpdir,\"/opt/couchbase/var/lib/couchbase/tmp\"},\n {net_kernel_verbosity,10},\n {loglevel_error_logger,debug},\n {path_config_libdir,\"/opt/couchbase/lib\"},\n {loglevel_stats,debug},\n {loglevel_menelaus,debug},\n {path_config_secdir,\"/opt/couchbase/etc/security\"},\n {loglevel_user,debug},\n {path_config_etcdir,\"/opt/couchbase/etc/couchbase\"},\n {loglevel_ns_server,debug},\n {loglevel_mapreduce_errors,debug},\n {loglevel_rebalance,debug},\n {loglevel_default,debug},\n {disk_sink_opts,[{rotation,[{compress,true},\n                             {size,41943040},\n                             {num_files,10},\n                             {buffer_size_max,52428800}]}]},\n {loglevel_cbas,debug},\n {loglevel_xdcr,debug},\n {loglevel_ns_doctor,debug},\n {loglevel_access,info},\n {error_logger_mf_dir,\"/opt/couchbase/var/lib/couchbase/logs\"},\n {path_config_datadir,\"/opt/couchbase/var/lib/couchbase\"},\n {loglevel_cluster,debug},\n {loglevel_couchdb,info},\n {loglevel_views,debug},\n {path_config_bindir,\"/opt/couchbase/bin\"}]"},
                                      {"ERL_CRASH_DUMP",
                                       "erl_crash.dump.1583215409.3666.ns_couchdb"}]}]},
                            {ringbuffer,1190,1024,
                                {[{<<"Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done">>,
                                   80},
                                  {<<>>,0},
                                  {<<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                                   200}],
                                 [{<<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>,
                                   910}]}},
                            undefined,
                            {ok,{-576460748704,
                                 #Ref<0.1044975193.1441792004.213884>}},
                            [<<"Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done">>,
                             <<>>,
                             <<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                             <<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>],
                            0}
** Reason for termination == 
** {abnormal,1}

[ns_server:error,2020-03-03T11:33:34.386+05:30,ns_1@cb.local:wait_link_to_couchdb_node<0.280.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:189]ns_couchdb_port(<0.275.0>) died with reason {abnormal,1}
[ns_server:debug,2020-03-03T11:33:34.387+05:30,ns_1@cb.local:<0.274.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.272.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:33:34.387+05:30,ns_1@cb.local:<0.273.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.272.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:33:34.387+05:30,ns_1@cb.local:<0.271.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.269.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:33:34.387+05:30,ns_1@cb.local:<0.270.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.269.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:33:34.387+05:30,ns_1@cb.local:<0.262.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.261.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:33:34.387+05:30,ns_1@cb.local:<0.223.0>:restartable:shutdown_child:120]Successfully terminated process <0.224.0>
[ns_server:debug,2020-03-03T11:33:34.387+05:30,ns_1@cb.local:<0.215.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.214.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:33:34.388+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:wait_saver:866]Done waiting for saver.
[ns_server:debug,2020-03-03T11:33:34.388+05:30,ns_1@cb.local:<0.201.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.200.0>} exited with reason shutdown
[error_logger:error,2020-03-03T11:33:34.391+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_port_server:init/1
    pid: <0.275.0>
    registered_name: ns_couchdb_port
    exception exit: {abnormal,1}
      in function  gen_server:handle_common_reply/8 (gen_server.erl, line 726)
    ancestors: [ns_server_nodes_sup,<0.206.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 1
    messages: [{'EXIT',#Port<0.5119>,normal}]
    links: [<0.207.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 11872
  neighbours:

[error_logger:error,2020-03-03T11:33:34.391+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: erlang:apply/2
    pid: <0.280.0>
    registered_name: wait_link_to_couchdb_node
    exception exit: {abnormal,1}
      in function  ns_server_nodes_sup:do_wait_link_to_couchdb_node/1 (src/ns_server_nodes_sup.erl, line 190)
    ancestors: [ns_server_nodes_sup,<0.206.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.207.0>,<0.281.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 987
    stack_size: 27
    reductions: 3382
  neighbours:
    neighbour:
      pid: <0.281.0>
      registered_name: []
      initial call: ns_server_nodes_sup:'-do_wait_link_to_couchdb_node/1-fun-2-'/0
      current_function: {timer,sleep,1}
      ancestors: [wait_link_to_couchdb_node,ns_server_nodes_sup,<0.206.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
      message_queue_len: 0
      links: [<0.280.0>]
      trap_exit: false
      status: waiting
      heap_size: 2586
      stack_size: 12
      reductions: 13904
      current_stacktrace: [{timer,sleep,1,[{file,"timer.erl"},{line,153}]},
                  {misc,poll_for_condition_rec,3,
                      [{file,"src/misc.erl"},{line,508}]},
                  {ns_server_nodes_sup,
                      '-do_wait_link_to_couchdb_node/1-fun-2-',2,
                      [{file,"src/ns_server_nodes_sup.erl"},{line,159}]},
                  {proc_lib,init_p,3,[{file,"proc_lib.erl"},{line,232}]}]

[error_logger:error,2020-03-03T11:33:34.391+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    start_error
     Reason:     {abnormal,1}
     Offender:   [{pid,undefined},
                  {name,wait_for_couchdb_node},
                  {mfargs,{erlang,apply,
                                  [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:error,2020-03-03T11:33:34.392+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    shutdown_error
     Reason:     {abnormal,1}
     Offender:   [{pid,<0.275.0>},
                  {name,start_couchdb_node},
                  {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                  {restart_type,{permanent,5}},
                  {shutdown,86400000},
                  {child_type,worker}]


[error_logger:error,2020-03-03T11:33:34.392+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_cluster_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,wait_for_couchdb_node,
                         {abnormal,1}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_nodes_sup},
                  {mfargs,
                      {restartable,start_link,
                          [{ns_server_nodes_sup,start_link,[]},infinity]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-03-03T11:33:34.392+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,root_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                             {failed_to_start_child,wait_for_couchdb_node,
                                 {abnormal,1}}}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_cluster_sup},
                  {mfargs,{ns_server_cluster_sup,start_link,[]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-03-03T11:33:34.392+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: application_master:init/4
    pid: <0.117.0>
    registered_name: []
    exception exit: {{shutdown,
                      {failed_to_start_child,ns_server_cluster_sup,
                       {shutdown,
                        {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                          {failed_to_start_child,wait_for_couchdb_node,
                           {abnormal,1}}}}}}},
                     {ns_server,start,[normal,[]]}}
      in function  application_master:init/4 (application_master.erl, line 134)
    ancestors: [<0.116.0>]
    message_queue_len: 1
    messages: [{'EXIT',<0.118.0>,normal}]
    links: [<0.116.0>,<0.33.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 274
  neighbours:

[error_logger:info,2020-03-03T11:33:34.392+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
         application: ns_server
              exited: {{shutdown,
                        {failed_to_start_child,ns_server_cluster_sup,
                         {shutdown,
                          {failed_to_start_child,ns_server_nodes_sup,
                           {shutdown,
                            {failed_to_start_child,wait_for_couchdb_node,
                             {abnormal,1}}}}}}},
                       {ns_server,start,[normal,[]]}}
                type: permanent

[error_logger:info,2020-03-03T11:33:34.392+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/8689"}}

[error_logger:info,2020-03-03T11:33:34.393+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/4486"}}

[error_logger:info,2020-03-03T11:33:34.393+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-calculator/154"}}

[error_logger:info,2020-03-03T11:33:34.393+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-logs/25"}}

[error_logger:info,2020-03-03T11:33:34.393+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/59"}}

[error_logger:info,2020-03-03T11:33:34.393+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gnome-system-monitor/36"}}

[error_logger:info,2020-03-03T11:33:34.393+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/98"}}

[error_logger:info,2020-03-03T11:33:34.393+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-characters/69"}}

[ns_server:info,2020-03-03T11:33:41.696+05:30,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-03-03T11:33:41.710+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-03-03T11:33:41.710+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.710+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.710+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.710+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.710+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.710+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.710+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.710+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.710+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.710+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.710+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-03-03T11:33:41.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-03-03T11:33:41.716+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-03-03T11:33:41.716+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-03T11:33:41.718+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2020-03-03T11:33:41.726+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.726+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.726+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.169.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:41.727+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:41.729+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2020-03-03T11:33:41.729+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.174.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:41.729+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.175.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:41.729+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:41.729+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.168.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:41.730+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.176.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.730+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.177.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.731+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:41.732+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-03-03T11:33:41.732+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-03-03T11:33:41.732+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2020-03-03T11:33:41.733+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-03-03T11:33:41.733+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.733+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.167.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-03-03T11:33:41.734+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-03-03T11:33:41.738+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-03-03T11:33:41.738+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2020-03-03T11:33:41.738+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:41.738+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:33:41.738+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2425924955.3858497538.193092>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:41.738+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2425924955.3858497538.193092>,
                                  inet_tcp_dist,<0.183.0>,
                                  #Ref<0.2425924955.3858497538.193097>}
[ns_server:debug,2020-03-03T11:33:41.740+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-03-03T11:33:41.740+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2020-03-03T11:33:41.740+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-03T11:33:41.742+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-03-03T11:33:41.742+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.746+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.186.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:41.748+05:30,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,3,3},{11,33,41}}},
               {memory,
                   [{total,26966128},
                    {processes,10176992},
                    {processes_used,10171536},
                    {system,16789136},
                    {atom,388625},
                    {atom_used,364408},
                    {binary,112616},
                    {code,8250921},
                    {ets,1504296}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    error_handler,application_controller,error_logger,
                    code_server,file_server,file,file_io_server,heart,
                    erl_parse,supervisor,ets,gen_event,application,proc_lib,
                    application_master,erl_eval,gen,gen_server,code,filename,
                    kernel,lists,erl_lint,erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{os_mon,"CPO  CXC 138 46","2.4.4"},
                    {sasl,"SASL  CXC 138 11","3.1.2"},
                    {ns_server,"Couchbase server","6.5.0-4960-enterprise"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ale,"Another Logger for Erlang","0.0.0"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,129},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,auth,httpd_sup,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    release_handler,lhttpc_sup,httpc_sup,lhttpc_manager,
                    alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,kernel_safe_sup,
                    'sink-ns_log',local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    'sink-disk_xdcr',timer_server,'sink-disk_debug',ale_sup,
                    'sink-disk_error',inet_db,'sink-disk_default',
                    ssl_pem_cache_dist,ale_dynamic_sup,rex,global_group,
                    net_sup,kernel_sup,ssl_connection_sup,global_name_server,
                    ssl_admin_sup,tftp_sup,ssl_sup,root_sup,erts_code_purger,
                    os_mon_sup,file_server_2,error_logger,cpu_sup,erl_epmd,
                    init,memsup,erl_signal_server,net_kernel,disksup,ale,
                    dist_manager,ssl_pem_cache,ssl_manager,ssl_dist_admin_sup,
                    ssl_dist_connection_sup,ssl_dist_sup,user,
                    ssl_tls_dist_proxy,ssl_manager_dist,sasl_safe_sup,
                    ssl_listen_tracker_sup,code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,0}]
[ns_server:info,2020-03-03T11:33:41.751+05:30,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"cfa0f75f28402d2e1aa254b2a374bead19433526\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" upstream=\"master\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"f2a16b53bb74146f20d18ba2c0443d5f10a9a550\" upstream=\"master\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4960\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" upstream=\"master\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" upstream=\"mad-hatter\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" upstream=\"master\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" upstream=\"master\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" upstream=\"master\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" upstream=\"mad-hatter\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" upstream=\"cb-master\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" upstream=\"master\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" upstream=\"master\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" upstream=\"mad-hatter\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" upstream=\"mad-hatter\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" upstream=\"mad-hatter\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" upstream=\"master\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" upstream=\"master\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" upstream=\"mad-hatter\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" upstream=\"mad-hatter\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" upstream=\"mad-hatter\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" upstream=\"mad-hatter\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" upstream=\"master\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" upstream=\"master\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" upstream=\"master\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" upstream=\"mad-hatter\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" upstream=\"master\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"7a3aa547b3f5eb3ea28d279a08384609cd2cea7c\" upstream=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" upstream=\"mad-hatter\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" upstream=\"master\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" upstream=\"master\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" upstream=\"master\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" upstream=\"v1.1.0-couchbase\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"ee64d41320d14fabe814a241a5cf4f6a6f6e827a\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" upstream=\"master\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" upstream=\"master\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-03-03T11:33:41.753+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.187.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.754+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.754+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:41.755+05:30,ns_1@cb.local:ns_config_sup<0.190.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-03-03T11:33:41.755+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.755+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:41.773+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-03-03T11:33:41.774+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2020-03-03T11:33:41.782+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[{cert_and_pkey,
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFfi2B3wIO/gwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAyYWJmMjVlZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMmFiZjI1\nZWUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDI7xEpYzw8VsEaLCx3\nQQVbkzsO6PmRhi08x2I8YCA1DbAT1zVEJIkEG1u91CWD7eAhWsCD3TWwBFZfcERe\n4yqxtt5zpsN84LQXkd18MWeFYeZCHlbul4N7Xhs4PavRzjWlbTk8Qh4tTNIbioFs\n5JuPzeY6csaWRKrS3j35kY37lhmPz8EOgK4wOd1Fo7vdtEF4whXV/KW/f8JJvY63\n8LScK2GEZKz1EP9HbmfcCYf+/N0tqUHx2kgz98JBm3S/6EEbxWvVrFAosEhPbA3Q\nb7GUvIuPEahHQDqhL5pRw+H/KdOoLFgCsaWYk8niAZ9DOTLrDCQIJEEzEz+xmwj1\nn9AXAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCijNJXd2H4F3KW\nRbv5SJxGN4t7rFKL4kXa9eRtrfa1CTHLU/C3+2opGhPw0354STXmE4zaBezp58M4\nNWjVgVo+uftij005x0y/daQUt0zJX6yUeV547Rxlqa/iw2u6SOWRMh+beN4vXiF3\nT3ZfIWZyx0zpG9In0EmuCEi6FgVpw3eRqDUwe52dDx0NFzVnrZVNKE3aGlPeJh1V\nJh6YsoQDsTr0n5kDcj7F3wSUnUvWTxmAeXo9IHSHAKzhqglnwaQ0ebWXN/C03ZyG\nTxONnMOyo3hAnI5YhLIUAly/nChmaZTDveDL5TLbifA/XL3UKe+VghtkTMrFSvQm\nvMw0PwM5\n-----END CERTIFICATE-----\n">>,
     <<"*****">>}]},
  {{node,'ns_1@cb.local',erl_external_listeners},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {inet,false},
    {inet6,false}]},
  {{node,'ns_1@cb.local',node_encryption},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    false]},
  {{node,'ns_1@cb.local',address_family},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    inet]},
  {alert_limits,
   [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
  {audit,
   [{auditd_enabled,false},
    {rotate_interval,86400},
    {rotate_size,20971520},
    {disabled,[]},
    {sync,[]},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
  {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
  {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
  {autocompaction,
   [{database_fragmentation_threshold,{30,undefined}},
    {view_fragmentation_threshold,{30,undefined}}]},
  {buckets,[{configs,[]}]},
  {cbas_memory_quota,2174},
  {drop_request_memory_threshold_mib,undefined},
  {email_alerts,
   [{recipients,["root@localhost"]},
    {sender,"couchbase@localhost"},
    {enabled,false},
    {email_server,
     [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
    {alerts,
     [auto_failover_node,auto_failover_maximum_reached,
      auto_failover_other_nodes_down,auto_failover_cluster_too_small,
      auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
      ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
      ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
  {fts_memory_quota,512},
  {index_aware_rebalance_disabled,false},
  {log_redaction_default_cfg,[{redact_level,none}]},
  {max_bucket_count,30},
  {memcached,[]},
  {memory_quota,8886},
  {nodes_wanted,['ns_1@cb.local']},
  {password_policy,[{min_length,6},{must_present,[]}]},
  {quorum_nodes,['ns_1@cb.local']},
  {remote_clusters,[]},
  {replication,[{enabled,true}]},
  {rest,[{port,8091}]},
  {rest_creds,null},
  {secure_headers,[]},
  {server_groups,
   [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
  {set_view_update_daemon,
   [{update_interval,5000},
    {update_min_changes,5000},
    {replica_update_min_changes,5000}]},
  {{couchdb,max_parallel_indexers},4},
  {{couchdb,max_parallel_replica_indexers},2},
  {{metakv,<<"/indexing/settings/config">>},
   <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
  {{request_limit,capi},undefined},
  {{request_limit,rest},undefined},
  {{node,'ns_1@cb.local',audit},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
  {{node,'ns_1@cb.local',capi_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8092]},
  {{node,'ns_1@cb.local',cbas_admin_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9110]},
  {{node,'ns_1@cb.local',cbas_cc_client_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9113]},
  {{node,'ns_1@cb.local',cbas_cc_cluster_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9112]},
  {{node,'ns_1@cb.local',cbas_cc_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9111]},
  {{node,'ns_1@cb.local',cbas_cluster_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9115]},
  {{node,'ns_1@cb.local',cbas_console_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9114]},
  {{node,'ns_1@cb.local',cbas_data_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9116]},
  {{node,'ns_1@cb.local',cbas_debug_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    -1]},
  {{node,'ns_1@cb.local',cbas_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8095]},
  {{node,'ns_1@cb.local',cbas_messaging_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9118]},
  {{node,'ns_1@cb.local',cbas_metadata_callback_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9119]},
  {{node,'ns_1@cb.local',cbas_metadata_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9121]},
  {{node,'ns_1@cb.local',cbas_parent_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9122]},
  {{node,'ns_1@cb.local',cbas_replication_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9120]},
  {{node,'ns_1@cb.local',cbas_result_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9117]},
  {{node,'ns_1@cb.local',cbas_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18095]},
  {{node,'ns_1@cb.local',compaction_daemon},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@cb.local',config_version},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {6,5}]},
  {{node,'ns_1@cb.local',eventing_debug_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9140]},
  {{node,'ns_1@cb.local',eventing_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8096]},
  {{node,'ns_1@cb.local',eventing_https_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18096]},
  {{node,'ns_1@cb.local',fts_grpc_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9130]},
  {{node,'ns_1@cb.local',fts_grpc_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    19130]},
  {{node,'ns_1@cb.local',fts_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8094]},
  {{node,'ns_1@cb.local',fts_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18094]},
  {{node,'ns_1@cb.local',indexer_admin_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9100]},
  {{node,'ns_1@cb.local',indexer_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9102]},
  {{node,'ns_1@cb.local',indexer_https_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    19102]},
  {{node,'ns_1@cb.local',indexer_scan_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9101]},
  {{node,'ns_1@cb.local',indexer_stcatchup_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9104]},
  {{node,'ns_1@cb.local',indexer_stinit_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9103]},
  {{node,'ns_1@cb.local',indexer_stmaint_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9105]},
  {{node,'ns_1@cb.local',is_enterprise},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    true]},
  {{node,'ns_1@cb.local',isasl},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@cb.local',membership},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    active]},
  {{node,'ns_1@cb.local',memcached},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,11210},
    {dedicated_port,11209},
    {dedicated_ssl_port,11206},
    {ssl_port,11207},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_sleeptime,19},
    {log_rotation_period,39003}]},
  {{node,'ns_1@cb.local',memcached_config},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {[{interfaces,
       {memcached_config_mgr,omit_missing_mcd_ports,
        [{[{host,<<"*">>},
           {port,port},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_port},
           {system,true},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,ssl_port},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_ssl_port},
           {system,true},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
      {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
      {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {opentracing,
       {[{enabled,opentracing_enabled},
         {module,{"~s",[opentracing_module]}},
         {config,{"~s",[opentracing_config]}}]}},
      {admin,{"~s",[admin_user]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize},
         {sleeptime,log_sleeptime}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
  {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    11206]},
  {{node,'ns_1@cb.local',memcached_defaults},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {max_connections,65000},
    {system_connections,5000},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {opentracing_enabled,false},
    {opentracing_module,[]},
    {opentracing_config,[]},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {tracing_enabled,true},
    {datatype_snappy,true},
    {num_reader_threads,<<"default">>},
    {num_writer_threads,<<"default">>}]},
  {{node,'ns_1@cb.local',moxi},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,0}]},
  {{node,'ns_1@cb.local',ns_log},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@cb.local',port_servers},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
  {{node,'ns_1@cb.local',projector_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9999]},
  {{node,'ns_1@cb.local',projector_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9999]},
  {{node,'ns_1@cb.local',query_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8093]},
  {{node,'ns_1@cb.local',rest},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@cb.local',saslauthd_enabled},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    true]},
  {{node,'ns_1@cb.local',ssl_capi_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18092]},
  {{node,'ns_1@cb.local',ssl_query_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18093]},
  {{node,'ns_1@cb.local',ssl_rest_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18091]},
  {{node,'ns_1@cb.local',uuid},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    <<"e32a4d3bd8aa759a4b96cd6ac25889ee">>]},
  {{node,'ns_1@cb.local',xdcr_rest_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9998]},
  {{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    false]},
  {{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{2,63750434612}}]}]}]]
[ns_server:info,2020-03-03T11:33:41.787+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{2,63750434612}}]}]},
 {{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   false]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9998]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   <<"e32a4d3bd8aa759a4b96cd6ac25889ee">>]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18091]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18093]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18092]},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   true]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9999]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',moxi},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,0}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   11206]},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',membership},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   active]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   true]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9100]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8094]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9130]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8096]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9140]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {6,5}]},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8095]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9110]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8092]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
 {{request_limit,rest},undefined},
 {{request_limit,capi},undefined},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
 {secure_headers,[]},
 {rest_creds,null},
 {rest,[{port,8091}]},
 {replication,[{enabled,true}]},
 {remote_clusters,[]},
 {quorum_nodes,['ns_1@cb.local']},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {nodes_wanted,['ns_1@cb.local']},
 {memory_quota,8886},
 {memcached,[]},
 {max_bucket_count,30},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {index_aware_rebalance_disabled,false},
 {fts_memory_quota,512},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {drop_request_memory_threshold_mib,undefined},
 {cbas_memory_quota,2174},
 {buckets,[{configs,[]}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {{node,'ns_1@cb.local',address_family},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   inet]},
 {{node,'ns_1@cb.local',node_encryption},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   false]},
 {{node,'ns_1@cb.local',erl_external_listeners},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {inet,false},
   {inet6,false}]},
 {cert_and_pkey,
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFfi2B3wIO/gwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAyYWJmMjVlZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMmFiZjI1\nZWUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDI7xEpYzw8VsEaLCx3\nQQVbkzsO6PmRhi08x2I8YCA1DbAT1zVEJIkEG1u91CWD7eAhWsCD3TWwBFZfcERe\n4yqxtt5zpsN84LQXkd18MWeFYeZCHlbul4N7Xhs4PavRzjWlbTk8Qh4tTNIbioFs\n5JuPzeY6csaWRKrS3j35kY37lhmPz8EOgK4wOd1Fo7vdtEF4whXV/KW/f8JJvY63\n8LScK2GEZKz1EP9HbmfcCYf+/N0tqUHx2kgz98JBm3S/6EEbxWvVrFAosEhPbA3Q\nb7GUvIuPEahHQDqhL5pRw+H/KdOoLFgCsaWYk8niAZ9DOTLrDCQIJEEzEz+xmwj1\nn9AXAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCijNJXd2H4F3KW\nRbv5SJxGN4t7rFKL4kXa9eRtrfa1CTHLU/C3+2opGhPw0354STXmE4zaBezp58M4\nNWjVgVo+uftij005x0y/daQUt0zJX6yUeV547Rxlqa/iw2u6SOWRMh+beN4vXiF3\nT3ZfIWZyx0zpG9In0EmuCEi6FgVpw3eRqDUwe52dDx0NFzVnrZVNKE3aGlPeJh1V\nJh6YsoQDsTr0n5kDcj7F3wSUnUvWTxmAeXo9IHSHAKzhqglnwaQ0ebWXN/C03ZyG\nTxONnMOyo3hAnI5YhLIUAly/nChmaZTDveDL5TLbifA/XL3UKe+VghtkTMrFSvQm\nvMw0PwM5\n-----END CERTIFICATE-----\n">>,
    <<"*****">>}]}]
[error_logger:info,2020-03-03T11:33:41.790+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.794+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.199.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.797+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.200.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.797+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:33:41.799+05:30,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>} ->
[{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{3,63750434621}}]}]
[error_logger:info,2020-03-03T11:33:41.799+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.202.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:41.799+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.205.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:41.804+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.208.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:41.805+05:30,ns_1@cb.local:menelaus_barrier<0.209.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-03-03T11:33:41.805+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.209.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.805+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.811+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.812+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.213.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:41.824+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2020-03-03T11:33:41.824+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2020-03-03T11:33:41.824+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2020-03-03T11:33:41.836+05:30,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:init:462]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,248,182,7,124,8,59,
             248,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
             6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
             118,101,114,32,50,97,98,102,50,53,101,101,48,30,23,13,49,51,48,
             49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
             57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
             104,98,97,115,101,32,83,101,114,118,101,114,32,50,97,98,102,50,
             53,101,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,
             3,130,1,15,0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,60,60,86,
             193,26,44,44,119,65,5,91,147,59,14,232,249,145,134,45,60,199,98,
             60,96,32,53,13,176,19,215,53,68,36,137,4,27,91,189,212,37,131,
             237,224,33,90,192,131,221,53,176,4,86,95,112,68,94,227,42,177,
             182,222,115,166,195,124,224,180,23,145,221,124,49,103,133,97,
             230,66,30,86,238,151,131,123,94,27,56,61,171,209,206,53,165,109,
             57,60,66,30,45,76,210,27,138,129,108,228,155,143,205,230,58,114,
             198,150,68,170,210,222,61,249,145,141,251,150,25,143,207,193,14,
             128,174,48,57,221,69,163,187,221,180,65,120,194,21,213,252,165,
             191,127,194,73,189,142,183,240,180,156,43,97,132,100,172,245,16,
             255,71,110,103,220,9,135,254,252,221,45,169,65,241,218,72,51,
             247,194,65,155,116,191,232,65,27,197,107,213,172,80,40,176,72,
             79,108,13,208,111,177,148,188,139,143,17,168,71,64,58,161,47,
             154,81,195,225,255,41,211,168,44,88,2,177,165,152,147,201,226,1,
             159,67,57,50,235,12,36,8,36,65,51,19,63,177,155,8,245,159,208,
             23,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,
             164,48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,
             3,85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,134,72,134,247,
             13,1,1,11,5,0,3,130,1,1,0,162,140,210,87,119,97,248,23,114,150,
             69,187,249,72,156,70,55,139,123,172,82,139,226,69,218,245,228,
             109,173,246,181,9,49,203,83,240,183,251,106,41,26,19,240,211,
             126,120,73,53,230,19,140,218,5,236,233,231,195,56,53,104,213,
             129,90,62,185,251,98,143,77,57,199,76,191,117,164,20,183,76,201,
             95,172,148,121,94,120,237,28,101,169,175,226,195,107,186,72,229,
             145,50,31,155,120,222,47,94,33,119,79,118,95,33,102,114,199,76,
             233,27,210,39,208,73,174,8,72,186,22,5,105,195,119,145,168,53,
             48,123,157,157,15,29,13,23,53,103,173,149,77,40,77,218,26,83,
             222,38,29,85,38,30,152,178,132,3,177,58,244,159,153,3,114,62,
             197,223,4,148,157,75,214,79,25,128,121,122,61,32,116,135,0,172,
             225,170,9,103,193,164,52,121,181,151,55,240,180,221,156,134,79,
             19,141,156,195,178,163,120,64,156,142,88,132,178,20,2,92,191,
             156,40,102,105,148,195,189,224,203,229,50,219,137,240,63,92,189,
             212,41,239,149,130,27,100,76,202,197,74,244,38,188,204,52,63,3,
             57>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2020-03-03T11:33:41.837+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.214.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:41.843+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-03T11:33:41.843+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-03T11:33:41.844+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-03T11:33:41.844+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:info,2020-03-03T11:33:41.852+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-03T11:33:41.852+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-03T11:33:41.852+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-03T11:33:41.852+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-03-03T11:33:41.852+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.217.0>,menelaus_web}
             started: [{pid,<0.218.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,
                                 248,182,7,124,8,59,248,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,50,97,98,
                                 102,50,53,101,101,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,50,
                                 97,98,102,50,53,101,101,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,
                                 60,60,86,193,26,44,44,119,65,5,91,147,59,14,
                                 232,249,145,134,45,60,199,98,60,96,32,53,13,
                                 176,19,215,53,68,36,137,4,27,91,189,212,37,
                                 131,237,224,33,90,192,131,221,53,176,4,86,
                                 95,112,68,94,227,42,177,182,222,115,166,195,
                                 124,224,180,23,145,221,124,49,103,133,97,
                                 230,66,30,86,238,151,131,123,94,27,56,61,
                                 171,209,206,53,165,109,57,60,66,30,45,76,
                                 210,27,138,129,108,228,155,143,205,230,58,
                                 114,198,150,68,170,210,222,61,249,145,141,
                                 251,150,25,143,207,193,14,128,174,48,57,221,
                                 69,163,187,221,180,65,120,194,21,213,252,
                                 165,191,127,194,73,189,142,183,240,180,156,
                                 43,97,132,100,172,245,16,255,71,110,103,220,
                                 9,135,254,252,221,45,169,65,241,218,72,51,
                                 247,194,65,155,116,191,232,65,27,197,107,
                                 213,172,80,40,176,72,79,108,13,208,111,177,
                                 148,188,139,143,17,168,71,64,58,161,47,154,
                                 81,195,225,255,41,211,168,44,88,2,177,165,
                                 152,147,201,226,1,159,67,57,50,235,12,36,8,
                                 36,65,51,19,63,177,155,8,245,159,208,23,2,3,
                                 1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 162,140,210,87,119,97,248,23,114,150,69,187,
                                 249,72,156,70,55,139,123,172,82,139,226,69,
                                 218,245,228,109,173,246,181,9,49,203,83,240,
                                 183,251,106,41,26,19,240,211,126,120,73,53,
                                 230,19,140,218,5,236,233,231,195,56,53,104,
                                 213,129,90,62,185,251,98,143,77,57,199,76,
                                 191,117,164,20,183,76,201,95,172,148,121,94,
                                 120,237,28,101,169,175,226,195,107,186,72,
                                 229,145,50,31,155,120,222,47,94,33,119,79,
                                 118,95,33,102,114,199,76,233,27,210,39,208,
                                 73,174,8,72,186,22,5,105,195,119,145,168,53,
                                 48,123,157,157,15,29,13,23,53,103,173,149,
                                 77,40,77,218,26,83,222,38,29,85,38,30,152,
                                 178,132,3,177,58,244,159,153,3,114,62,197,
                                 223,4,148,157,75,214,79,25,128,121,122,61,
                                 32,116,135,0,172,225,170,9,103,193,164,52,
                                 121,181,151,55,240,180,221,156,134,79,19,
                                 141,156,195,178,163,120,64,156,142,88,132,
                                 178,20,2,92,191,156,40,102,105,148,195,189,
                                 224,203,229,50,219,137,240,63,92,189,212,41,
                                 239,149,130,27,100,76,202,197,74,244,38,188,
                                 204,52,63,3,57>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:41.853+05:30,ns_1@cb.local:<0.216.0>:restartable:start_child:98]Started child process <0.217.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2020-03-03T11:33:41.853+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.217.0>,menelaus_web}
             started: [{pid,<0.236.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,
                                 248,182,7,124,8,59,248,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,50,97,98,
                                 102,50,53,101,101,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,50,
                                 97,98,102,50,53,101,101,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,
                                 60,60,86,193,26,44,44,119,65,5,91,147,59,14,
                                 232,249,145,134,45,60,199,98,60,96,32,53,13,
                                 176,19,215,53,68,36,137,4,27,91,189,212,37,
                                 131,237,224,33,90,192,131,221,53,176,4,86,
                                 95,112,68,94,227,42,177,182,222,115,166,195,
                                 124,224,180,23,145,221,124,49,103,133,97,
                                 230,66,30,86,238,151,131,123,94,27,56,61,
                                 171,209,206,53,165,109,57,60,66,30,45,76,
                                 210,27,138,129,108,228,155,143,205,230,58,
                                 114,198,150,68,170,210,222,61,249,145,141,
                                 251,150,25,143,207,193,14,128,174,48,57,221,
                                 69,163,187,221,180,65,120,194,21,213,252,
                                 165,191,127,194,73,189,142,183,240,180,156,
                                 43,97,132,100,172,245,16,255,71,110,103,220,
                                 9,135,254,252,221,45,169,65,241,218,72,51,
                                 247,194,65,155,116,191,232,65,27,197,107,
                                 213,172,80,40,176,72,79,108,13,208,111,177,
                                 148,188,139,143,17,168,71,64,58,161,47,154,
                                 81,195,225,255,41,211,168,44,88,2,177,165,
                                 152,147,201,226,1,159,67,57,50,235,12,36,8,
                                 36,65,51,19,63,177,155,8,245,159,208,23,2,3,
                                 1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 162,140,210,87,119,97,248,23,114,150,69,187,
                                 249,72,156,70,55,139,123,172,82,139,226,69,
                                 218,245,228,109,173,246,181,9,49,203,83,240,
                                 183,251,106,41,26,19,240,211,126,120,73,53,
                                 230,19,140,218,5,236,233,231,195,56,53,104,
                                 213,129,90,62,185,251,98,143,77,57,199,76,
                                 191,117,164,20,183,76,201,95,172,148,121,94,
                                 120,237,28,101,169,175,226,195,107,186,72,
                                 229,145,50,31,155,120,222,47,94,33,119,79,
                                 118,95,33,102,114,199,76,233,27,210,39,208,
                                 73,174,8,72,186,22,5,105,195,119,145,168,53,
                                 48,123,157,157,15,29,13,23,53,103,173,149,
                                 77,40,77,218,26,83,222,38,29,85,38,30,152,
                                 178,132,3,177,58,244,159,153,3,114,62,197,
                                 223,4,148,157,75,214,79,25,128,121,122,61,
                                 32,116,135,0,172,225,170,9,103,193,164,52,
                                 121,181,151,55,240,180,221,156,134,79,19,
                                 141,156,195,178,163,120,64,156,142,88,132,
                                 178,20,2,92,191,156,40,102,105,148,195,189,
                                 224,203,229,50,219,137,240,63,92,189,212,41,
                                 239,149,130,27,100,76,202,197,74,244,38,188,
                                 204,52,63,3,57>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.854+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.216.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.854+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:41.859+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.254.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.860+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.257.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.865+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.259.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:41.867+05:30,ns_1@cb.local:users_replicator<0.259.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-03-03T11:33:41.870+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.259.0>
[ns_server:debug,2020-03-03T11:33:41.870+05:30,ns_1@cb.local:users_replicator<0.259.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.260.0>
[ns_server:debug,2020-03-03T11:33:41.871+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-03-03T11:33:41.871+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.260.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.872+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.258.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:33:41.877+05:30,ns_1@cb.local:compiled_roles_cache<0.262.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-03-03T11:33:41.878+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.262.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.880+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.265.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.880+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.256.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:41.880+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.268.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:41.880+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.269.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:41.888+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-03-03T11:33:41.889+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 17ms
[ns_server:debug,2020-03-03T11:33:41.892+05:30,ns_1@cb.local:users_replicator<0.259.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[ns_server:debug,2020-03-03T11:33:41.899+05:30,ns_1@cb.local:wait_link_to_couchdb_node<0.273.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[error_logger:info,2020-03-03T11:33:41.899+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.272.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:41.899+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:41.899+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:33:41.899+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2425924955.3858497538.193382>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:41.899+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2425924955.3858497538.193382>,
                                  inet_tcp_dist,<0.276.0>,
                                  #Ref<0.2425924955.3858497538.193386>}
[ns_server:debug,2020-03-03T11:33:41.899+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2425924955.3858497538.193382>,
                               inet_tcp_dist,<0.276.0>,
                               #Ref<0.2425924955.3858497538.193386>}
[ns_server:debug,2020-03-03T11:33:41.899+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:33:41.899+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.276.0>,shutdown}}
[error_logger:info,2020-03-03T11:33:41.899+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-03-03T11:33:42.099+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:42.100+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:33:42.100+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2425924955.3858497538.193397>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:42.100+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2425924955.3858497538.193397>,
                                  inet_tcp_dist,<0.279.0>,
                                  #Ref<0.2425924955.3858497538.193401>}
[ns_server:debug,2020-03-03T11:33:42.138+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-03T11:33:42.340+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-03T11:33:42.542+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-03-03T11:33:42.737+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.283.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:42.781+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-03T11:33:42.792+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2425924955.3858497538.193397>,
                               inet_tcp_dist,<0.279.0>,
                               #Ref<0.2425924955.3858497538.193401>}
[error_logger:info,2020-03-03T11:33:42.792+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.279.0>,connection_closed}}
[ns_server:info,2020-03-03T11:33:42.938+05:30,ns_1@cb.local:ns_couchdb_port<0.272.0>:ns_port_server:log:224]ns_couchdb<0.272.0>: Apache CouchDB  (LogLevel=info) is starting.
ns_couchdb<0.272.0>: Failure to start Mochiweb: eaddrinuse
ns_couchdb<0.272.0>: 4075: Booted. Waiting for shutdown request
ns_couchdb<0.272.0>: [os_mon] memory supervisor port (memsup): Erlang has closed
ns_couchdb<0.272.0>: [os_mon] cpu supervisor port (cpu_sup): Erlang has closed

[error_logger:info,2020-03-03T11:33:42.982+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:42.982+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:33:42.982+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2425924955.3858497538.193436>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:42.982+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2425924955.3858497538.193436>,
                                  inet_tcp_dist,<0.286.0>,
                                  #Ref<0.2425924955.3858497538.193438>}
[ns_server:debug,2020-03-03T11:33:42.982+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2425924955.3858497538.193436>,
                               inet_tcp_dist,<0.286.0>,
                               #Ref<0.2425924955.3858497538.193438>}
[error_logger:info,2020-03-03T11:33:42.982+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.286.0>,shutdown}}
[ns_server:debug,2020-03-03T11:33:42.982+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:33:42.982+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:43.183+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-03T11:33:43.183+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:43.183+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2425924955.3858497537.193482>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:43.183+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2425924955.3858497537.193482>,
                                  inet_tcp_dist,<0.289.0>,
                                  #Ref<0.2425924955.3858497537.193484>}
[ns_server:debug,2020-03-03T11:33:43.183+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-03-03T11:33:43.183+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2425924955.3858497537.193482>,
                               inet_tcp_dist,<0.289.0>,
                               #Ref<0.2425924955.3858497537.193484>}
[error_logger:info,2020-03-03T11:33:43.183+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.289.0>,shutdown}}
[error_logger:info,2020-03-03T11:33:43.183+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-03-03T11:33:43.384+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:43.384+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:33:43.384+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2425924955.3858497540.193451>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:43.384+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2425924955.3858497540.193451>,
                                  inet_tcp_dist,<0.292.0>,
                                  #Ref<0.2425924955.3858497540.193455>}
[ns_server:debug,2020-03-03T11:33:43.384+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-03-03T11:33:43.384+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2425924955.3858497540.193451>,
                               inet_tcp_dist,<0.292.0>,
                               #Ref<0.2425924955.3858497540.193455>}
[error_logger:info,2020-03-03T11:33:43.384+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.292.0>,shutdown}}
[error_logger:info,2020-03-03T11:33:43.384+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:info,2020-03-03T11:33:43.409+05:30,ns_1@cb.local:ns_couchdb_port<0.272.0>:ns_port_server:log:224]ns_couchdb<0.272.0>: {"Kernel pid terminated",application_controller,"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\"},{line,102}]},{supervisor,do_start_child,2,[{file,\"supervisor.erl\"},{line,365}]},{supervisor,start_children,3,[{file,\"supervisor.erl\"},{line,348}]},{supervisor,init_children,2,[{file,\"supervisor.erl\"},{line,314}]},{gen_server,init_it,2,[{file,\"gen_server.erl\"},{line,365}]},{gen_server,init_it,6,[{file,\"gen_server.erl\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}"}
ns_couchdb<0.272.0>: Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro
ns_couchdb<0.272.0>: 
ns_couchdb<0.272.0>: Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done

[ns_server:error,2020-03-03T11:33:43.410+05:30,ns_1@cb.local:wait_link_to_couchdb_node<0.273.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:189]ns_couchdb_port(<0.272.0>) died with reason {abnormal,1}
[error_logger:error,2020-03-03T11:33:43.409+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]** Generic server ns_couchdb_port terminating 
** Last message in was {#Port<0.5096>,{exit_status,1}}
** When Server state == {state,#Port<0.5096>,
                            {ns_couchdb,"/opt/couchbase/lib/erlang/bin/erl",
                                ["-pa",
                                 "/opt/couchbase/lib/erlang/lib/asn1-5.0.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/compiler-7.1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEvent-2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEventDomain-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosFileTransfer-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosNotification-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosProperty-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTime-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTransactions-1.3.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/crypto-4.2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/dialyzer-3.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/diameter-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/edoc-0.9.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eldap-1.2.3.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_docgen-0.7.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_interface-3.10.2.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erts-9.3.3.9/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eunit-2.3.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/hipe-3.17.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ic-4.4.4.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/inets-6.5.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/mnesia-4.15.3.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/orber-3.8.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/os_mon-2.4.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/otp_mibs-1.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/parsetools-2.1.6/ebin",
                                 "/opt/couchbase/lib/erlang/lib/public_key-1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/reltool-0.7.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/runtime_tools-1.12.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/sasl-3.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/snmp-5.2.11/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssh-4.6.9.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssl-8.2.6.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/syntax_tools-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/tools-2.11.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/xmerl-1.3.16.1/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/gc-couchbase-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/vtree-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/wkb-1.2.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch-1.2.0a-961ad59-git/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_audit-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_dcp-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_index_merger-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_set_view-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_view_parser-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/ejson-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/erlang-oauth/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/etap/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/lhttpc-1.3/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mapreduce-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mochiweb-1.4.1/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/snappy-1.0.4/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ale/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/gen_smtp/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_babysitter/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_couchdb/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/ebin",
                                 "/opt/couchbase/lib/erlang/lib/stdlib-3.4.5.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/kernel-5.4.3.2/ebin",
                                 ".","-couch_ini",
                                 "/opt/couchbase/etc/couchdb/default.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/capi.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/geocouch.ini",
                                 "/opt/couchbase/etc/couchdb/local.ini",
                                 "-kernel","error_logger","false","-kernel",
                                 "error_logger","false","inetrc",
                                 "\"/opt/couchbase/etc/couchbase/hosts.cfg\"",
                                 "dist_config_file",
                                 "\"/opt/couchbase/var/lib/couchbase/config/dist_cfg\"",
                                 "-ssl_dist_optfile",
                                 "/opt/couchbase/etc/couchbase/ssl_dist_opts",
                                 "-setcookie",
                                 "dce5392bcba7669ee9f057b78581e574cccf9efc1961f2376ff32b0a61220948",
                                 "-name","couchdb_ns_1@cb.local","-smp",
                                 "enable","+P","327680","+K","true","-kernel",
                                 "error_logger","false","-sasl",
                                 "sasl_error_logger","false","-nouser",
                                 "-hidden","-proto_dist","cb","-epmd_module",
                                 "cb_epmd","-start_epmd","false","-run",
                                 "child_erlang","child_start","ns_couchdb"],
                                [use_stdio,
                                 {env,
                                     [{"NS_COUCHDB_ENV_ARGS",
                                       "[{ns_server_node,'ns_1@cb.local'},\n {path_config_tmpdir,\"/opt/couchbase/var/lib/couchbase/tmp\"},\n {net_kernel_verbosity,10},\n {loglevel_error_logger,debug},\n {path_config_libdir,\"/opt/couchbase/lib\"},\n {loglevel_stats,debug},\n {loglevel_menelaus,debug},\n {path_config_secdir,\"/opt/couchbase/etc/security\"},\n {loglevel_user,debug},\n {path_config_etcdir,\"/opt/couchbase/etc/couchbase\"},\n {loglevel_ns_server,debug},\n {loglevel_mapreduce_errors,debug},\n {loglevel_rebalance,debug},\n {loglevel_default,debug},\n {disk_sink_opts,[{rotation,[{compress,true},\n                             {size,41943040},\n                             {num_files,10},\n                             {buffer_size_max,52428800}]}]},\n {loglevel_cbas,debug},\n {loglevel_xdcr,debug},\n {loglevel_ns_doctor,debug},\n {loglevel_access,info},\n {error_logger_mf_dir,\"/opt/couchbase/var/lib/couchbase/logs\"},\n {path_config_datadir,\"/opt/couchbase/var/lib/couchbase\"},\n {loglevel_cluster,debug},\n {loglevel_couchdb,info},\n {loglevel_views,debug},\n {path_config_bindir,\"/opt/couchbase/bin\"}]"},
                                      {"ERL_CRASH_DUMP",
                                       "erl_crash.dump.1583215409.3666.ns_couchdb"}]}]},
                            {ringbuffer,1190,1024,
                                {[{<<"Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done">>,
                                   80},
                                  {<<>>,0},
                                  {<<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                                   200}],
                                 [{<<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>,
                                   910}]}},
                            undefined,
                            {ok,{-576460749407,
                                 #Ref<0.2425924955.3858497538.193440>}},
                            [<<"Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done">>,
                             <<>>,
                             <<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                             <<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>],
                            0}
** Reason for termination == 
** {abnormal,1}

[ns_server:debug,2020-03-03T11:33:43.411+05:30,ns_1@cb.local:<0.266.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.265.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:33:43.411+05:30,ns_1@cb.local:<0.267.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.265.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:33:43.411+05:30,ns_1@cb.local:<0.264.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.262.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:33:43.411+05:30,ns_1@cb.local:<0.263.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.262.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:33:43.411+05:30,ns_1@cb.local:<0.255.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.254.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:33:43.411+05:30,ns_1@cb.local:<0.216.0>:restartable:shutdown_child:120]Successfully terminated process <0.217.0>
[ns_server:debug,2020-03-03T11:33:43.411+05:30,ns_1@cb.local:<0.215.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.214.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:33:43.411+05:30,ns_1@cb.local:<0.201.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.200.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:33:43.411+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:wait_saver:866]Done waiting for saver.
[error_logger:error,2020-03-03T11:33:43.413+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_port_server:init/1
    pid: <0.272.0>
    registered_name: ns_couchdb_port
    exception exit: {abnormal,1}
      in function  gen_server:handle_common_reply/8 (gen_server.erl, line 726)
    ancestors: [ns_server_nodes_sup,<0.206.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 1
    messages: [{'EXIT',#Port<0.5096>,normal}]
    links: [<0.207.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 6772
    stack_size: 27
    reductions: 11885
  neighbours:

[error_logger:error,2020-03-03T11:33:43.413+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: erlang:apply/2
    pid: <0.273.0>
    registered_name: wait_link_to_couchdb_node
    exception exit: {abnormal,1}
      in function  ns_server_nodes_sup:do_wait_link_to_couchdb_node/1 (src/ns_server_nodes_sup.erl, line 190)
    ancestors: [ns_server_nodes_sup,<0.206.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.207.0>,<0.274.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 987
    stack_size: 27
    reductions: 3382
  neighbours:
    neighbour:
      pid: <0.274.0>
      registered_name: []
      initial call: ns_server_nodes_sup:'-do_wait_link_to_couchdb_node/1-fun-2-'/0
      current_function: {timer,sleep,1}
      ancestors: [wait_link_to_couchdb_node,ns_server_nodes_sup,<0.206.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
      message_queue_len: 0
      links: [<0.273.0>]
      trap_exit: false
      status: waiting
      heap_size: 2586
      stack_size: 12
      reductions: 12343
      current_stacktrace: [{timer,sleep,1,[{file,"timer.erl"},{line,153}]},
                  {misc,poll_for_condition_rec,3,
                      [{file,"src/misc.erl"},{line,508}]},
                  {ns_server_nodes_sup,
                      '-do_wait_link_to_couchdb_node/1-fun-2-',2,
                      [{file,"src/ns_server_nodes_sup.erl"},{line,159}]},
                  {proc_lib,init_p,3,[{file,"proc_lib.erl"},{line,232}]}]

[error_logger:error,2020-03-03T11:33:43.413+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    start_error
     Reason:     {abnormal,1}
     Offender:   [{pid,undefined},
                  {name,wait_for_couchdb_node},
                  {mfargs,{erlang,apply,
                                  [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:error,2020-03-03T11:33:43.413+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    shutdown_error
     Reason:     {abnormal,1}
     Offender:   [{pid,<0.272.0>},
                  {name,start_couchdb_node},
                  {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                  {restart_type,{permanent,5}},
                  {shutdown,86400000},
                  {child_type,worker}]


[error_logger:error,2020-03-03T11:33:43.413+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_cluster_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,wait_for_couchdb_node,
                         {abnormal,1}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_nodes_sup},
                  {mfargs,
                      {restartable,start_link,
                          [{ns_server_nodes_sup,start_link,[]},infinity]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-03-03T11:33:43.414+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,root_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                             {failed_to_start_child,wait_for_couchdb_node,
                                 {abnormal,1}}}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_cluster_sup},
                  {mfargs,{ns_server_cluster_sup,start_link,[]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-03-03T11:33:43.414+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: application_master:init/4
    pid: <0.117.0>
    registered_name: []
    exception exit: {{shutdown,
                      {failed_to_start_child,ns_server_cluster_sup,
                       {shutdown,
                        {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                          {failed_to_start_child,wait_for_couchdb_node,
                           {abnormal,1}}}}}}},
                     {ns_server,start,[normal,[]]}}
      in function  application_master:init/4 (application_master.erl, line 134)
    ancestors: [<0.116.0>]
    message_queue_len: 1
    messages: [{'EXIT',<0.118.0>,normal}]
    links: [<0.116.0>,<0.33.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 274
  neighbours:

[error_logger:info,2020-03-03T11:33:43.414+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
         application: ns_server
              exited: {{shutdown,
                        {failed_to_start_child,ns_server_cluster_sup,
                         {shutdown,
                          {failed_to_start_child,ns_server_nodes_sup,
                           {shutdown,
                            {failed_to_start_child,wait_for_couchdb_node,
                             {abnormal,1}}}}}}},
                       {ns_server,start,[normal,[]]}}
                type: permanent

[error_logger:info,2020-03-03T11:33:43.414+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/8689"}}

[error_logger:info,2020-03-03T11:33:43.414+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/4486"}}

[error_logger:info,2020-03-03T11:33:43.414+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-calculator/154"}}

[error_logger:info,2020-03-03T11:33:43.414+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-logs/25"}}

[error_logger:info,2020-03-03T11:33:43.414+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/59"}}

[error_logger:info,2020-03-03T11:33:43.414+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gnome-system-monitor/36"}}

[error_logger:info,2020-03-03T11:33:43.415+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/98"}}

[error_logger:info,2020-03-03T11:33:43.415+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-characters/69"}}

[ns_server:info,2020-03-03T11:33:50.700+05:30,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.711+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.712+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-03-03T11:33:50.712+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-03-03T11:33:50.716+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-03-03T11:33:50.716+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-03T11:33:50.717+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2020-03-03T11:33:50.724+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.724+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.724+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.169.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:50.726+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.727+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.174.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:50.727+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.175.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:33:50.727+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2020-03-03T11:33:50.727+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:50.727+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.168.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:50.728+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.176.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.728+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.177.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.729+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:50.731+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-03-03T11:33:50.731+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-03-03T11:33:50.731+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2020-03-03T11:33:50.732+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-03-03T11:33:50.732+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.733+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.167.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-03-03T11:33:50.733+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-03-03T11:33:50.735+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-03-03T11:33:50.735+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2020-03-03T11:33:50.735+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:50.735+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:33:50.735+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3829445903.1979449346.240133>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:50.735+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3829445903.1979449346.240133>,
                                  inet_tcp_dist,<0.183.0>,
                                  #Ref<0.3829445903.1979449346.240138>}
[ns_server:debug,2020-03-03T11:33:50.738+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-03-03T11:33:50.738+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2020-03-03T11:33:50.738+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-03T11:33:50.739+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-03-03T11:33:50.739+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.744+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.186.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:50.745+05:30,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,3,3},{11,33,50}}},
               {memory,
                   [{total,26329048},
                    {processes,9557328},
                    {processes_used,9537216},
                    {system,16771720},
                    {atom,388625},
                    {atom_used,364408},
                    {binary,96496},
                    {code,8250921},
                    {ets,1504288}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    error_handler,application,kernel,heart,file_server,
                    application_master,code,code_server,file,proc_lib,
                    application_controller,file_io_server,error_logger,gen,
                    filename,lists,supervisor,gen_event,erl_parse,erl_eval,
                    gen_server,ets,erl_lint,erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{os_mon,"CPO  CXC 138 46","2.4.4"},
                    {sasl,"SASL  CXC 138 11","3.1.2"},
                    {ns_server,"Couchbase server","6.5.0-4960-enterprise"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ale,"Another Logger for Erlang","0.0.0"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,129},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,auth,httpd_sup,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    release_handler,lhttpc_sup,httpc_sup,lhttpc_manager,
                    alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    'sink-disk_xdcr',timer_server,'sink-disk_debug',ale_sup,
                    'sink-disk_error',inet_db,'sink-disk_default',
                    ssl_pem_cache_dist,ale_dynamic_sup,rex,global_group,
                    net_sup,kernel_sup,ssl_connection_sup,global_name_server,
                    ssl_admin_sup,tftp_sup,ssl_sup,root_sup,erts_code_purger,
                    os_mon_sup,file_server_2,error_logger,cpu_sup,erl_epmd,
                    init,memsup,kernel_safe_sup,erl_signal_server,net_kernel,
                    disksup,ale,dist_manager,ssl_pem_cache,ssl_manager,
                    ssl_dist_admin_sup,ssl_dist_connection_sup,ssl_dist_sup,
                    user,ssl_tls_dist_proxy,ssl_manager_dist,sasl_safe_sup,
                    ssl_listen_tracker_sup,code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,0}]
[ns_server:info,2020-03-03T11:33:50.749+05:30,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"cfa0f75f28402d2e1aa254b2a374bead19433526\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" upstream=\"master\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"f2a16b53bb74146f20d18ba2c0443d5f10a9a550\" upstream=\"master\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4960\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" upstream=\"master\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" upstream=\"mad-hatter\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" upstream=\"master\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" upstream=\"master\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" upstream=\"master\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" upstream=\"mad-hatter\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" upstream=\"cb-master\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" upstream=\"master\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" upstream=\"master\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" upstream=\"mad-hatter\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" upstream=\"mad-hatter\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" upstream=\"mad-hatter\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" upstream=\"master\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" upstream=\"master\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" upstream=\"mad-hatter\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" upstream=\"mad-hatter\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" upstream=\"mad-hatter\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" upstream=\"mad-hatter\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" upstream=\"master\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" upstream=\"master\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" upstream=\"master\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" upstream=\"mad-hatter\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" upstream=\"master\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"7a3aa547b3f5eb3ea28d279a08384609cd2cea7c\" upstream=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" upstream=\"mad-hatter\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" upstream=\"master\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" upstream=\"master\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" upstream=\"master\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" upstream=\"v1.1.0-couchbase\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"ee64d41320d14fabe814a241a5cf4f6a6f6e827a\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" upstream=\"master\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" upstream=\"master\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-03-03T11:33:50.752+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.187.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.753+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.753+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:50.754+05:30,ns_1@cb.local:ns_config_sup<0.190.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-03-03T11:33:50.754+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.754+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:50.778+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-03-03T11:33:50.779+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2020-03-03T11:33:50.787+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[{alert_limits,
   [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
  {audit,
   [{auditd_enabled,false},
    {rotate_interval,86400},
    {rotate_size,20971520},
    {disabled,[]},
    {sync,[]},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
  {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
  {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
  {autocompaction,
   [{database_fragmentation_threshold,{30,undefined}},
    {view_fragmentation_threshold,{30,undefined}}]},
  {buckets,[{configs,[]}]},
  {cbas_memory_quota,2174},
  {cert_and_pkey,
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFfi2B3wIO/gwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAyYWJmMjVlZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMmFiZjI1\nZWUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDI7xEpYzw8VsEaLCx3\nQQVbkzsO6PmRhi08x2I8YCA1DbAT1zVEJIkEG1u91CWD7eAhWsCD3TWwBFZfcERe\n4yqxtt5zpsN84LQXkd18MWeFYeZCHlbul4N7Xhs4PavRzjWlbTk8Qh4tTNIbioFs\n5JuPzeY6csaWRKrS3j35kY37lhmPz8EOgK4wOd1Fo7vdtEF4whXV/KW/f8JJvY63\n8LScK2GEZKz1EP9HbmfcCYf+/N0tqUHx2kgz98JBm3S/6EEbxWvVrFAosEhPbA3Q\nb7GUvIuPEahHQDqhL5pRw+H/KdOoLFgCsaWYk8niAZ9DOTLrDCQIJEEzEz+xmwj1\nn9AXAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCijNJXd2H4F3KW\nRbv5SJxGN4t7rFKL4kXa9eRtrfa1CTHLU/C3+2opGhPw0354STXmE4zaBezp58M4\nNWjVgVo+uftij005x0y/daQUt0zJX6yUeV547Rxlqa/iw2u6SOWRMh+beN4vXiF3\nT3ZfIWZyx0zpG9In0EmuCEi6FgVpw3eRqDUwe52dDx0NFzVnrZVNKE3aGlPeJh1V\nJh6YsoQDsTr0n5kDcj7F3wSUnUvWTxmAeXo9IHSHAKzhqglnwaQ0ebWXN/C03ZyG\nTxONnMOyo3hAnI5YhLIUAly/nChmaZTDveDL5TLbifA/XL3UKe+VghtkTMrFSvQm\nvMw0PwM5\n-----END CERTIFICATE-----\n">>,
     <<"*****">>}]},
  {drop_request_memory_threshold_mib,undefined},
  {email_alerts,
   [{recipients,["root@localhost"]},
    {sender,"couchbase@localhost"},
    {enabled,false},
    {email_server,
     [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
    {alerts,
     [auto_failover_node,auto_failover_maximum_reached,
      auto_failover_other_nodes_down,auto_failover_cluster_too_small,
      auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
      ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
      ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
  {fts_memory_quota,512},
  {index_aware_rebalance_disabled,false},
  {log_redaction_default_cfg,[{redact_level,none}]},
  {max_bucket_count,30},
  {memcached,[]},
  {memory_quota,8886},
  {nodes_wanted,['ns_1@cb.local']},
  {password_policy,[{min_length,6},{must_present,[]}]},
  {quorum_nodes,['ns_1@cb.local']},
  {remote_clusters,[]},
  {replication,[{enabled,true}]},
  {rest,[{port,8091}]},
  {rest_creds,null},
  {secure_headers,[]},
  {server_groups,
   [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
  {set_view_update_daemon,
   [{update_interval,5000},
    {update_min_changes,5000},
    {replica_update_min_changes,5000}]},
  {{couchdb,max_parallel_indexers},4},
  {{couchdb,max_parallel_replica_indexers},2},
  {{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{3,63750434621}}]}]},
  {{metakv,<<"/indexing/settings/config">>},
   <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
  {{request_limit,capi},undefined},
  {{request_limit,rest},undefined},
  {{node,'ns_1@cb.local',address_family},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    inet]},
  {{node,'ns_1@cb.local',audit},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
  {{node,'ns_1@cb.local',capi_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8092]},
  {{node,'ns_1@cb.local',cbas_admin_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9110]},
  {{node,'ns_1@cb.local',cbas_cc_client_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9113]},
  {{node,'ns_1@cb.local',cbas_cc_cluster_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9112]},
  {{node,'ns_1@cb.local',cbas_cc_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9111]},
  {{node,'ns_1@cb.local',cbas_cluster_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9115]},
  {{node,'ns_1@cb.local',cbas_console_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9114]},
  {{node,'ns_1@cb.local',cbas_data_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9116]},
  {{node,'ns_1@cb.local',cbas_debug_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    -1]},
  {{node,'ns_1@cb.local',cbas_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8095]},
  {{node,'ns_1@cb.local',cbas_messaging_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9118]},
  {{node,'ns_1@cb.local',cbas_metadata_callback_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9119]},
  {{node,'ns_1@cb.local',cbas_metadata_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9121]},
  {{node,'ns_1@cb.local',cbas_parent_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9122]},
  {{node,'ns_1@cb.local',cbas_replication_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9120]},
  {{node,'ns_1@cb.local',cbas_result_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9117]},
  {{node,'ns_1@cb.local',cbas_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18095]},
  {{node,'ns_1@cb.local',compaction_daemon},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@cb.local',config_version},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {6,5}]},
  {{node,'ns_1@cb.local',erl_external_listeners},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {inet,false},
    {inet6,false}]},
  {{node,'ns_1@cb.local',eventing_debug_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9140]},
  {{node,'ns_1@cb.local',eventing_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8096]},
  {{node,'ns_1@cb.local',eventing_https_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18096]},
  {{node,'ns_1@cb.local',fts_grpc_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9130]},
  {{node,'ns_1@cb.local',fts_grpc_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    19130]},
  {{node,'ns_1@cb.local',fts_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8094]},
  {{node,'ns_1@cb.local',fts_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18094]},
  {{node,'ns_1@cb.local',indexer_admin_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9100]},
  {{node,'ns_1@cb.local',indexer_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9102]},
  {{node,'ns_1@cb.local',indexer_https_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    19102]},
  {{node,'ns_1@cb.local',indexer_scan_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9101]},
  {{node,'ns_1@cb.local',indexer_stcatchup_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9104]},
  {{node,'ns_1@cb.local',indexer_stinit_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9103]},
  {{node,'ns_1@cb.local',indexer_stmaint_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9105]},
  {{node,'ns_1@cb.local',is_enterprise},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    true]},
  {{node,'ns_1@cb.local',isasl},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@cb.local',membership},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    active]},
  {{node,'ns_1@cb.local',memcached},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,11210},
    {dedicated_port,11209},
    {dedicated_ssl_port,11206},
    {ssl_port,11207},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_sleeptime,19},
    {log_rotation_period,39003}]},
  {{node,'ns_1@cb.local',memcached_config},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {[{interfaces,
       {memcached_config_mgr,omit_missing_mcd_ports,
        [{[{host,<<"*">>},
           {port,port},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_port},
           {system,true},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,ssl_port},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_ssl_port},
           {system,true},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
      {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
      {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {opentracing,
       {[{enabled,opentracing_enabled},
         {module,{"~s",[opentracing_module]}},
         {config,{"~s",[opentracing_config]}}]}},
      {admin,{"~s",[admin_user]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize},
         {sleeptime,log_sleeptime}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
  {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    11206]},
  {{node,'ns_1@cb.local',memcached_defaults},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {max_connections,65000},
    {system_connections,5000},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {opentracing_enabled,false},
    {opentracing_module,[]},
    {opentracing_config,[]},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {tracing_enabled,true},
    {datatype_snappy,true},
    {num_reader_threads,<<"default">>},
    {num_writer_threads,<<"default">>}]},
  {{node,'ns_1@cb.local',moxi},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,0}]},
  {{node,'ns_1@cb.local',node_encryption},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    false]},
  {{node,'ns_1@cb.local',ns_log},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@cb.local',port_servers},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
  {{node,'ns_1@cb.local',projector_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9999]},
  {{node,'ns_1@cb.local',projector_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9999]},
  {{node,'ns_1@cb.local',query_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8093]},
  {{node,'ns_1@cb.local',rest},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@cb.local',saslauthd_enabled},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    true]},
  {{node,'ns_1@cb.local',ssl_capi_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18092]},
  {{node,'ns_1@cb.local',ssl_query_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18093]},
  {{node,'ns_1@cb.local',ssl_rest_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18091]},
  {{node,'ns_1@cb.local',uuid},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    <<"e32a4d3bd8aa759a4b96cd6ac25889ee">>]},
  {{node,'ns_1@cb.local',xdcr_rest_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9998]},
  {{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    false]}]]
[ns_server:info,2020-03-03T11:33:50.790+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   false]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9998]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   <<"e32a4d3bd8aa759a4b96cd6ac25889ee">>]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18091]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18093]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18092]},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   true]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9999]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',node_encryption},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   false]},
 {{node,'ns_1@cb.local',moxi},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,0}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   11206]},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',membership},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   active]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   true]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9100]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8094]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9130]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8096]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9140]},
 {{node,'ns_1@cb.local',erl_external_listeners},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {inet,false},
   {inet6,false}]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {6,5}]},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8095]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9110]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8092]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
 {{node,'ns_1@cb.local',address_family},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   inet]},
 {{request_limit,rest},undefined},
 {{request_limit,capi},undefined},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{3,63750434621}}]}]},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
 {secure_headers,[]},
 {rest_creds,null},
 {rest,[{port,8091}]},
 {replication,[{enabled,true}]},
 {remote_clusters,[]},
 {quorum_nodes,['ns_1@cb.local']},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {nodes_wanted,['ns_1@cb.local']},
 {memory_quota,8886},
 {memcached,[]},
 {max_bucket_count,30},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {index_aware_rebalance_disabled,false},
 {fts_memory_quota,512},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {drop_request_memory_threshold_mib,undefined},
 {cert_and_pkey,
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFfi2B3wIO/gwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAyYWJmMjVlZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMmFiZjI1\nZWUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDI7xEpYzw8VsEaLCx3\nQQVbkzsO6PmRhi08x2I8YCA1DbAT1zVEJIkEG1u91CWD7eAhWsCD3TWwBFZfcERe\n4yqxtt5zpsN84LQXkd18MWeFYeZCHlbul4N7Xhs4PavRzjWlbTk8Qh4tTNIbioFs\n5JuPzeY6csaWRKrS3j35kY37lhmPz8EOgK4wOd1Fo7vdtEF4whXV/KW/f8JJvY63\n8LScK2GEZKz1EP9HbmfcCYf+/N0tqUHx2kgz98JBm3S/6EEbxWvVrFAosEhPbA3Q\nb7GUvIuPEahHQDqhL5pRw+H/KdOoLFgCsaWYk8niAZ9DOTLrDCQIJEEzEz+xmwj1\nn9AXAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCijNJXd2H4F3KW\nRbv5SJxGN4t7rFKL4kXa9eRtrfa1CTHLU/C3+2opGhPw0354STXmE4zaBezp58M4\nNWjVgVo+uftij005x0y/daQUt0zJX6yUeV547Rxlqa/iw2u6SOWRMh+beN4vXiF3\nT3ZfIWZyx0zpG9In0EmuCEi6FgVpw3eRqDUwe52dDx0NFzVnrZVNKE3aGlPeJh1V\nJh6YsoQDsTr0n5kDcj7F3wSUnUvWTxmAeXo9IHSHAKzhqglnwaQ0ebWXN/C03ZyG\nTxONnMOyo3hAnI5YhLIUAly/nChmaZTDveDL5TLbifA/XL3UKe+VghtkTMrFSvQm\nvMw0PwM5\n-----END CERTIFICATE-----\n">>,
    <<"*****">>}]},
 {cbas_memory_quota,2174},
 {buckets,[{configs,[]}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]}]
[error_logger:info,2020-03-03T11:33:50.795+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.796+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.199.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.797+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.200.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.797+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:33:50.799+05:30,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>} ->
[{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{4,63750434630}}]}]
[error_logger:info,2020-03-03T11:33:50.799+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.202.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:50.800+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.205.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:50.804+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.208.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:50.805+05:30,ns_1@cb.local:menelaus_barrier<0.209.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-03-03T11:33:50.805+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.209.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.805+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.806+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.807+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.213.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:50.816+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2020-03-03T11:33:50.816+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2020-03-03T11:33:50.816+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2020-03-03T11:33:50.827+05:30,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:init:462]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,248,182,7,124,8,59,
             248,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
             6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
             118,101,114,32,50,97,98,102,50,53,101,101,48,30,23,13,49,51,48,
             49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
             57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
             104,98,97,115,101,32,83,101,114,118,101,114,32,50,97,98,102,50,
             53,101,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,
             3,130,1,15,0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,60,60,86,
             193,26,44,44,119,65,5,91,147,59,14,232,249,145,134,45,60,199,98,
             60,96,32,53,13,176,19,215,53,68,36,137,4,27,91,189,212,37,131,
             237,224,33,90,192,131,221,53,176,4,86,95,112,68,94,227,42,177,
             182,222,115,166,195,124,224,180,23,145,221,124,49,103,133,97,
             230,66,30,86,238,151,131,123,94,27,56,61,171,209,206,53,165,109,
             57,60,66,30,45,76,210,27,138,129,108,228,155,143,205,230,58,114,
             198,150,68,170,210,222,61,249,145,141,251,150,25,143,207,193,14,
             128,174,48,57,221,69,163,187,221,180,65,120,194,21,213,252,165,
             191,127,194,73,189,142,183,240,180,156,43,97,132,100,172,245,16,
             255,71,110,103,220,9,135,254,252,221,45,169,65,241,218,72,51,
             247,194,65,155,116,191,232,65,27,197,107,213,172,80,40,176,72,
             79,108,13,208,111,177,148,188,139,143,17,168,71,64,58,161,47,
             154,81,195,225,255,41,211,168,44,88,2,177,165,152,147,201,226,1,
             159,67,57,50,235,12,36,8,36,65,51,19,63,177,155,8,245,159,208,
             23,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,
             164,48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,
             3,85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,134,72,134,247,
             13,1,1,11,5,0,3,130,1,1,0,162,140,210,87,119,97,248,23,114,150,
             69,187,249,72,156,70,55,139,123,172,82,139,226,69,218,245,228,
             109,173,246,181,9,49,203,83,240,183,251,106,41,26,19,240,211,
             126,120,73,53,230,19,140,218,5,236,233,231,195,56,53,104,213,
             129,90,62,185,251,98,143,77,57,199,76,191,117,164,20,183,76,201,
             95,172,148,121,94,120,237,28,101,169,175,226,195,107,186,72,229,
             145,50,31,155,120,222,47,94,33,119,79,118,95,33,102,114,199,76,
             233,27,210,39,208,73,174,8,72,186,22,5,105,195,119,145,168,53,
             48,123,157,157,15,29,13,23,53,103,173,149,77,40,77,218,26,83,
             222,38,29,85,38,30,152,178,132,3,177,58,244,159,153,3,114,62,
             197,223,4,148,157,75,214,79,25,128,121,122,61,32,116,135,0,172,
             225,170,9,103,193,164,52,121,181,151,55,240,180,221,156,134,79,
             19,141,156,195,178,163,120,64,156,142,88,132,178,20,2,92,191,
             156,40,102,105,148,195,189,224,203,229,50,219,137,240,63,92,189,
             212,41,239,149,130,27,100,76,202,197,74,244,38,188,204,52,63,3,
             57>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2020-03-03T11:33:50.828+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.214.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:50.842+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-03T11:33:50.842+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-03T11:33:50.842+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-03T11:33:50.842+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:info,2020-03-03T11:33:50.853+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-03T11:33:50.853+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-03T11:33:50.854+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[error_logger:info,2020-03-03T11:33:50.853+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.217.0>,menelaus_web}
             started: [{pid,<0.218.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,
                                 248,182,7,124,8,59,248,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,50,97,98,
                                 102,50,53,101,101,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,50,
                                 97,98,102,50,53,101,101,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,
                                 60,60,86,193,26,44,44,119,65,5,91,147,59,14,
                                 232,249,145,134,45,60,199,98,60,96,32,53,13,
                                 176,19,215,53,68,36,137,4,27,91,189,212,37,
                                 131,237,224,33,90,192,131,221,53,176,4,86,
                                 95,112,68,94,227,42,177,182,222,115,166,195,
                                 124,224,180,23,145,221,124,49,103,133,97,
                                 230,66,30,86,238,151,131,123,94,27,56,61,
                                 171,209,206,53,165,109,57,60,66,30,45,76,
                                 210,27,138,129,108,228,155,143,205,230,58,
                                 114,198,150,68,170,210,222,61,249,145,141,
                                 251,150,25,143,207,193,14,128,174,48,57,221,
                                 69,163,187,221,180,65,120,194,21,213,252,
                                 165,191,127,194,73,189,142,183,240,180,156,
                                 43,97,132,100,172,245,16,255,71,110,103,220,
                                 9,135,254,252,221,45,169,65,241,218,72,51,
                                 247,194,65,155,116,191,232,65,27,197,107,
                                 213,172,80,40,176,72,79,108,13,208,111,177,
                                 148,188,139,143,17,168,71,64,58,161,47,154,
                                 81,195,225,255,41,211,168,44,88,2,177,165,
                                 152,147,201,226,1,159,67,57,50,235,12,36,8,
                                 36,65,51,19,63,177,155,8,245,159,208,23,2,3,
                                 1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 162,140,210,87,119,97,248,23,114,150,69,187,
                                 249,72,156,70,55,139,123,172,82,139,226,69,
                                 218,245,228,109,173,246,181,9,49,203,83,240,
                                 183,251,106,41,26,19,240,211,126,120,73,53,
                                 230,19,140,218,5,236,233,231,195,56,53,104,
                                 213,129,90,62,185,251,98,143,77,57,199,76,
                                 191,117,164,20,183,76,201,95,172,148,121,94,
                                 120,237,28,101,169,175,226,195,107,186,72,
                                 229,145,50,31,155,120,222,47,94,33,119,79,
                                 118,95,33,102,114,199,76,233,27,210,39,208,
                                 73,174,8,72,186,22,5,105,195,119,145,168,53,
                                 48,123,157,157,15,29,13,23,53,103,173,149,
                                 77,40,77,218,26,83,222,38,29,85,38,30,152,
                                 178,132,3,177,58,244,159,153,3,114,62,197,
                                 223,4,148,157,75,214,79,25,128,121,122,61,
                                 32,116,135,0,172,225,170,9,103,193,164,52,
                                 121,181,151,55,240,180,221,156,134,79,19,
                                 141,156,195,178,163,120,64,156,142,88,132,
                                 178,20,2,92,191,156,40,102,105,148,195,189,
                                 224,203,229,50,219,137,240,63,92,189,212,41,
                                 239,149,130,27,100,76,202,197,74,244,38,188,
                                 204,52,63,3,57>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:50.854+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2020-03-03T11:33:50.855+05:30,ns_1@cb.local:<0.216.0>:restartable:start_child:98]Started child process <0.217.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2020-03-03T11:33:50.855+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.217.0>,menelaus_web}
             started: [{pid,<0.236.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,
                                 248,182,7,124,8,59,248,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,50,97,98,
                                 102,50,53,101,101,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,50,
                                 97,98,102,50,53,101,101,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,
                                 60,60,86,193,26,44,44,119,65,5,91,147,59,14,
                                 232,249,145,134,45,60,199,98,60,96,32,53,13,
                                 176,19,215,53,68,36,137,4,27,91,189,212,37,
                                 131,237,224,33,90,192,131,221,53,176,4,86,
                                 95,112,68,94,227,42,177,182,222,115,166,195,
                                 124,224,180,23,145,221,124,49,103,133,97,
                                 230,66,30,86,238,151,131,123,94,27,56,61,
                                 171,209,206,53,165,109,57,60,66,30,45,76,
                                 210,27,138,129,108,228,155,143,205,230,58,
                                 114,198,150,68,170,210,222,61,249,145,141,
                                 251,150,25,143,207,193,14,128,174,48,57,221,
                                 69,163,187,221,180,65,120,194,21,213,252,
                                 165,191,127,194,73,189,142,183,240,180,156,
                                 43,97,132,100,172,245,16,255,71,110,103,220,
                                 9,135,254,252,221,45,169,65,241,218,72,51,
                                 247,194,65,155,116,191,232,65,27,197,107,
                                 213,172,80,40,176,72,79,108,13,208,111,177,
                                 148,188,139,143,17,168,71,64,58,161,47,154,
                                 81,195,225,255,41,211,168,44,88,2,177,165,
                                 152,147,201,226,1,159,67,57,50,235,12,36,8,
                                 36,65,51,19,63,177,155,8,245,159,208,23,2,3,
                                 1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 162,140,210,87,119,97,248,23,114,150,69,187,
                                 249,72,156,70,55,139,123,172,82,139,226,69,
                                 218,245,228,109,173,246,181,9,49,203,83,240,
                                 183,251,106,41,26,19,240,211,126,120,73,53,
                                 230,19,140,218,5,236,233,231,195,56,53,104,
                                 213,129,90,62,185,251,98,143,77,57,199,76,
                                 191,117,164,20,183,76,201,95,172,148,121,94,
                                 120,237,28,101,169,175,226,195,107,186,72,
                                 229,145,50,31,155,120,222,47,94,33,119,79,
                                 118,95,33,102,114,199,76,233,27,210,39,208,
                                 73,174,8,72,186,22,5,105,195,119,145,168,53,
                                 48,123,157,157,15,29,13,23,53,103,173,149,
                                 77,40,77,218,26,83,222,38,29,85,38,30,152,
                                 178,132,3,177,58,244,159,153,3,114,62,197,
                                 223,4,148,157,75,214,79,25,128,121,122,61,
                                 32,116,135,0,172,225,170,9,103,193,164,52,
                                 121,181,151,55,240,180,221,156,134,79,19,
                                 141,156,195,178,163,120,64,156,142,88,132,
                                 178,20,2,92,191,156,40,102,105,148,195,189,
                                 224,203,229,50,219,137,240,63,92,189,212,41,
                                 239,149,130,27,100,76,202,197,74,244,38,188,
                                 204,52,63,3,57>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.855+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.216.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.856+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:50.861+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.254.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.862+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.257.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.865+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.259.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:50.866+05:30,ns_1@cb.local:users_replicator<0.259.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-03-03T11:33:50.867+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.259.0>
[ns_server:debug,2020-03-03T11:33:50.867+05:30,ns_1@cb.local:users_replicator<0.259.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.260.0>
[ns_server:debug,2020-03-03T11:33:50.868+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-03-03T11:33:50.868+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.260.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.868+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.258.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:33:50.875+05:30,ns_1@cb.local:compiled_roles_cache<0.262.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-03-03T11:33:50.875+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.262.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.877+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.265.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:50.877+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.256.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:50.877+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.268.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:50.877+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.269.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:50.885+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-03-03T11:33:50.885+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 16ms
[ns_server:debug,2020-03-03T11:33:50.887+05:30,ns_1@cb.local:users_replicator<0.259.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[ns_server:debug,2020-03-03T11:33:50.887+05:30,ns_1@cb.local:wait_link_to_couchdb_node<0.273.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[ns_server:debug,2020-03-03T11:33:50.888+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-03T11:33:50.888+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.272.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:50.888+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3829445903.1979449346.240220>,
                               inet_tcp_dist,undefined,undefined}
[error_logger:info,2020-03-03T11:33:50.888+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:50.888+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3829445903.1979449346.240220>,
                                  inet_tcp_dist,<0.276.0>,
                                  #Ref<0.3829445903.1979449346.240224>}
[ns_server:debug,2020-03-03T11:33:50.888+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3829445903.1979449346.240220>,
                               inet_tcp_dist,<0.276.0>,
                               #Ref<0.3829445903.1979449346.240224>}
[ns_server:debug,2020-03-03T11:33:50.888+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:33:50.888+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.276.0>,shutdown}}
[error_logger:info,2020-03-03T11:33:50.888+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-03-03T11:33:51.088+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:51.088+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:33:51.089+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3829445903.1979449346.240234>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:51.089+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3829445903.1979449346.240234>,
                                  inet_tcp_dist,<0.279.0>,
                                  #Ref<0.3829445903.1979449346.240238>}
[ns_server:debug,2020-03-03T11:33:51.115+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-03T11:33:51.316+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-03-03T11:33:51.547+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.283.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:51.602+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-03-03T11:33:51.612+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.279.0>,connection_closed}}
[ns_server:debug,2020-03-03T11:33:51.612+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3829445903.1979449346.240234>,
                               inet_tcp_dist,<0.279.0>,
                               #Ref<0.3829445903.1979449346.240238>}
[ns_server:info,2020-03-03T11:33:51.747+05:30,ns_1@cb.local:ns_couchdb_port<0.272.0>:ns_port_server:log:224]ns_couchdb<0.272.0>: Apache CouchDB  (LogLevel=info) is starting.
ns_couchdb<0.272.0>: Failure to start Mochiweb: eaddrinuse
ns_couchdb<0.272.0>: 4207: Booted. Waiting for shutdown request
ns_couchdb<0.272.0>: [os_mon] memory supervisor port (memsup): Erlang has closed
ns_couchdb<0.272.0>: [os_mon] cpu supervisor port (cpu_sup): Erlang has closed

[ns_server:debug,2020-03-03T11:33:51.802+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-03T11:33:51.802+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:51.802+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3829445903.1979449347.240805>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:51.802+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3829445903.1979449347.240805>,
                                  inet_tcp_dist,<0.285.0>,
                                  #Ref<0.3829445903.1979449346.240258>}
[ns_server:debug,2020-03-03T11:33:51.803+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3829445903.1979449347.240805>,
                               inet_tcp_dist,<0.285.0>,
                               #Ref<0.3829445903.1979449346.240258>}
[ns_server:debug,2020-03-03T11:33:51.803+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:33:51.803+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.285.0>,shutdown}}
[error_logger:info,2020-03-03T11:33:51.803+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:52.003+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-03T11:33:52.003+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:52.003+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3829445903.1979449347.240819>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:52.003+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3829445903.1979449347.240819>,
                                  inet_tcp_dist,<0.288.0>,
                                  #Ref<0.3829445903.1979449347.240823>}
[ns_server:debug,2020-03-03T11:33:52.004+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-03-03T11:33:52.004+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3829445903.1979449347.240819>,
                               inet_tcp_dist,<0.288.0>,
                               #Ref<0.3829445903.1979449347.240823>}
[error_logger:info,2020-03-03T11:33:52.004+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.288.0>,shutdown}}
[error_logger:info,2020-03-03T11:33:52.004+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-03-03T11:33:52.204+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:52.204+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:33:52.204+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3829445903.1979449346.240265>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:52.205+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3829445903.1979449346.240265>,
                                  inet_tcp_dist,<0.291.0>,
                                  #Ref<0.3829445903.1979449347.240830>}
[ns_server:debug,2020-03-03T11:33:52.205+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3829445903.1979449346.240265>,
                               inet_tcp_dist,<0.291.0>,
                               #Ref<0.3829445903.1979449347.240830>}
[error_logger:info,2020-03-03T11:33:52.205+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.291.0>,shutdown}}
[ns_server:debug,2020-03-03T11:33:52.205+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:33:52.205+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:info,2020-03-03T11:33:52.231+05:30,ns_1@cb.local:ns_couchdb_port<0.272.0>:ns_port_server:log:224]ns_couchdb<0.272.0>: {"Kernel pid terminated",application_controller,"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\"},{line,102}]},{supervisor,do_start_child,2,[{file,\"supervisor.erl\"},{line,365}]},{supervisor,start_children,3,[{file,\"supervisor.erl\"},{line,348}]},{supervisor,init_children,2,[{file,\"supervisor.erl\"},{line,314}]},{gen_server,init_it,2,[{file,\"gen_server.erl\"},{line,365}]},{gen_server,init_it,6,[{file,\"gen_server.erl\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}"}
ns_couchdb<0.272.0>: Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro
ns_couchdb<0.272.0>: 
ns_couchdb<0.272.0>: Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done

[ns_server:error,2020-03-03T11:33:52.233+05:30,ns_1@cb.local:wait_link_to_couchdb_node<0.273.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:189]ns_couchdb_port(<0.272.0>) died with reason {abnormal,1}
[ns_server:debug,2020-03-03T11:33:52.233+05:30,ns_1@cb.local:<0.267.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.265.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:33:52.233+05:30,ns_1@cb.local:<0.266.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.265.0>} exited with reason shutdown
[error_logger:error,2020-03-03T11:33:52.231+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]** Generic server ns_couchdb_port terminating 
** Last message in was {#Port<0.5096>,{exit_status,1}}
** When Server state == {state,#Port<0.5096>,
                            {ns_couchdb,"/opt/couchbase/lib/erlang/bin/erl",
                                ["-pa",
                                 "/opt/couchbase/lib/erlang/lib/asn1-5.0.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/compiler-7.1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEvent-2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEventDomain-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosFileTransfer-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosNotification-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosProperty-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTime-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTransactions-1.3.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/crypto-4.2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/dialyzer-3.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/diameter-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/edoc-0.9.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eldap-1.2.3.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_docgen-0.7.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_interface-3.10.2.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erts-9.3.3.9/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eunit-2.3.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/hipe-3.17.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ic-4.4.4.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/inets-6.5.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/mnesia-4.15.3.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/orber-3.8.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/os_mon-2.4.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/otp_mibs-1.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/parsetools-2.1.6/ebin",
                                 "/opt/couchbase/lib/erlang/lib/public_key-1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/reltool-0.7.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/runtime_tools-1.12.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/sasl-3.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/snmp-5.2.11/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssh-4.6.9.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssl-8.2.6.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/syntax_tools-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/tools-2.11.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/xmerl-1.3.16.1/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/gc-couchbase-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/vtree-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/wkb-1.2.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch-1.2.0a-961ad59-git/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_audit-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_dcp-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_index_merger-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_set_view-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_view_parser-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/ejson-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/erlang-oauth/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/etap/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/lhttpc-1.3/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mapreduce-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mochiweb-1.4.1/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/snappy-1.0.4/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ale/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/gen_smtp/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_babysitter/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_couchdb/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/ebin",
                                 "/opt/couchbase/lib/erlang/lib/stdlib-3.4.5.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/kernel-5.4.3.2/ebin",
                                 ".","-couch_ini",
                                 "/opt/couchbase/etc/couchdb/default.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/capi.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/geocouch.ini",
                                 "/opt/couchbase/etc/couchdb/local.ini",
                                 "-kernel","error_logger","false","-kernel",
                                 "error_logger","false","inetrc",
                                 "\"/opt/couchbase/etc/couchbase/hosts.cfg\"",
                                 "dist_config_file",
                                 "\"/opt/couchbase/var/lib/couchbase/config/dist_cfg\"",
                                 "-ssl_dist_optfile",
                                 "/opt/couchbase/etc/couchbase/ssl_dist_opts",
                                 "-setcookie",
                                 "dce5392bcba7669ee9f057b78581e574cccf9efc1961f2376ff32b0a61220948",
                                 "-name","couchdb_ns_1@cb.local","-smp",
                                 "enable","+P","327680","+K","true","-kernel",
                                 "error_logger","false","-sasl",
                                 "sasl_error_logger","false","-nouser",
                                 "-hidden","-proto_dist","cb","-epmd_module",
                                 "cb_epmd","-start_epmd","false","-run",
                                 "child_erlang","child_start","ns_couchdb"],
                                [use_stdio,
                                 {env,
                                     [{"NS_COUCHDB_ENV_ARGS",
                                       "[{ns_server_node,'ns_1@cb.local'},\n {path_config_tmpdir,\"/opt/couchbase/var/lib/couchbase/tmp\"},\n {net_kernel_verbosity,10},\n {loglevel_error_logger,debug},\n {path_config_libdir,\"/opt/couchbase/lib\"},\n {loglevel_stats,debug},\n {loglevel_menelaus,debug},\n {path_config_secdir,\"/opt/couchbase/etc/security\"},\n {loglevel_user,debug},\n {path_config_etcdir,\"/opt/couchbase/etc/couchbase\"},\n {loglevel_ns_server,debug},\n {loglevel_mapreduce_errors,debug},\n {loglevel_rebalance,debug},\n {loglevel_default,debug},\n {disk_sink_opts,[{rotation,[{compress,true},\n                             {size,41943040},\n                             {num_files,10},\n                             {buffer_size_max,52428800}]}]},\n {loglevel_cbas,debug},\n {loglevel_xdcr,debug},\n {loglevel_ns_doctor,debug},\n {loglevel_access,info},\n {error_logger_mf_dir,\"/opt/couchbase/var/lib/couchbase/logs\"},\n {path_config_datadir,\"/opt/couchbase/var/lib/couchbase\"},\n {loglevel_cluster,debug},\n {loglevel_couchdb,info},\n {loglevel_views,debug},\n {path_config_bindir,\"/opt/couchbase/bin\"}]"},
                                      {"ERL_CRASH_DUMP",
                                       "erl_crash.dump.1583215409.3666.ns_couchdb"}]}]},
                            {ringbuffer,1190,1024,
                                {[{<<"Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done">>,
                                   80},
                                  {<<>>,0},
                                  {<<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                                   200}],
                                 [{<<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>,
                                   910}]}},
                            undefined,
                            {ok,{-576460749610,
                                 #Ref<0.3829445903.1979449346.240260>}},
                            [<<"Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done">>,
                             <<>>,
                             <<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                             <<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>],
                            0}
** Reason for termination == 
** {abnormal,1}

[ns_server:debug,2020-03-03T11:33:52.233+05:30,ns_1@cb.local:<0.264.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.262.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:33:52.233+05:30,ns_1@cb.local:<0.263.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.262.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:33:52.233+05:30,ns_1@cb.local:<0.255.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.254.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:33:52.233+05:30,ns_1@cb.local:<0.216.0>:restartable:shutdown_child:120]Successfully terminated process <0.217.0>
[ns_server:debug,2020-03-03T11:33:52.233+05:30,ns_1@cb.local:<0.215.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.214.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:33:52.234+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:wait_saver:866]Done waiting for saver.
[ns_server:debug,2020-03-03T11:33:52.233+05:30,ns_1@cb.local:<0.201.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.200.0>} exited with reason shutdown
[error_logger:error,2020-03-03T11:33:52.235+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_port_server:init/1
    pid: <0.272.0>
    registered_name: ns_couchdb_port
    exception exit: {abnormal,1}
      in function  gen_server:handle_common_reply/8 (gen_server.erl, line 726)
    ancestors: [ns_server_nodes_sup,<0.206.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 1
    messages: [{'EXIT',#Port<0.5096>,normal}]
    links: [<0.207.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 6772
    stack_size: 27
    reductions: 11885
  neighbours:

[error_logger:error,2020-03-03T11:33:52.235+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: erlang:apply/2
    pid: <0.273.0>
    registered_name: wait_link_to_couchdb_node
    exception exit: {abnormal,1}
      in function  ns_server_nodes_sup:do_wait_link_to_couchdb_node/1 (src/ns_server_nodes_sup.erl, line 190)
    ancestors: [ns_server_nodes_sup,<0.206.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.207.0>,<0.274.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 987
    stack_size: 27
    reductions: 3382
  neighbours:
    neighbour:
      pid: <0.274.0>
      registered_name: []
      initial call: ns_server_nodes_sup:'-do_wait_link_to_couchdb_node/1-fun-2-'/0
      current_function: {timer,sleep,1}
      ancestors: [wait_link_to_couchdb_node,ns_server_nodes_sup,<0.206.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
      message_queue_len: 0
      links: [<0.273.0>]
      trap_exit: false
      status: waiting
      heap_size: 2586
      stack_size: 12
      reductions: 10907
      current_stacktrace: [{timer,sleep,1,[{file,"timer.erl"},{line,153}]},
                  {misc,poll_for_condition_rec,3,
                      [{file,"src/misc.erl"},{line,508}]},
                  {ns_server_nodes_sup,
                      '-do_wait_link_to_couchdb_node/1-fun-2-',2,
                      [{file,"src/ns_server_nodes_sup.erl"},{line,159}]},
                  {proc_lib,init_p,3,[{file,"proc_lib.erl"},{line,232}]}]

[error_logger:error,2020-03-03T11:33:52.235+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    start_error
     Reason:     {abnormal,1}
     Offender:   [{pid,undefined},
                  {name,wait_for_couchdb_node},
                  {mfargs,{erlang,apply,
                                  [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:error,2020-03-03T11:33:52.236+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    shutdown_error
     Reason:     {abnormal,1}
     Offender:   [{pid,<0.272.0>},
                  {name,start_couchdb_node},
                  {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                  {restart_type,{permanent,5}},
                  {shutdown,86400000},
                  {child_type,worker}]


[error_logger:error,2020-03-03T11:33:52.236+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_cluster_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,wait_for_couchdb_node,
                         {abnormal,1}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_nodes_sup},
                  {mfargs,
                      {restartable,start_link,
                          [{ns_server_nodes_sup,start_link,[]},infinity]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-03-03T11:33:52.236+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,root_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                             {failed_to_start_child,wait_for_couchdb_node,
                                 {abnormal,1}}}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_cluster_sup},
                  {mfargs,{ns_server_cluster_sup,start_link,[]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-03-03T11:33:52.236+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: application_master:init/4
    pid: <0.117.0>
    registered_name: []
    exception exit: {{shutdown,
                      {failed_to_start_child,ns_server_cluster_sup,
                       {shutdown,
                        {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                          {failed_to_start_child,wait_for_couchdb_node,
                           {abnormal,1}}}}}}},
                     {ns_server,start,[normal,[]]}}
      in function  application_master:init/4 (application_master.erl, line 134)
    ancestors: [<0.116.0>]
    message_queue_len: 1
    messages: [{'EXIT',<0.118.0>,normal}]
    links: [<0.116.0>,<0.33.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 274
  neighbours:

[error_logger:info,2020-03-03T11:33:52.236+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
         application: ns_server
              exited: {{shutdown,
                        {failed_to_start_child,ns_server_cluster_sup,
                         {shutdown,
                          {failed_to_start_child,ns_server_nodes_sup,
                           {shutdown,
                            {failed_to_start_child,wait_for_couchdb_node,
                             {abnormal,1}}}}}}},
                       {ns_server,start,[normal,[]]}}
                type: permanent

[error_logger:info,2020-03-03T11:33:52.236+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/8689"}}

[error_logger:info,2020-03-03T11:33:52.236+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/4486"}}

[error_logger:info,2020-03-03T11:33:52.236+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-calculator/154"}}

[error_logger:info,2020-03-03T11:33:52.236+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-logs/25"}}

[error_logger:info,2020-03-03T11:33:52.236+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/59"}}

[error_logger:info,2020-03-03T11:33:52.236+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gnome-system-monitor/36"}}

[error_logger:info,2020-03-03T11:33:52.237+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/98"}}

[error_logger:info,2020-03-03T11:33:52.237+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-characters/69"}}

[ns_server:info,2020-03-03T11:33:59.414+05:30,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-03-03T11:33:59.425+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-03-03T11:33:59.425+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.425+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.425+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.425+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.425+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.425+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.425+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.425+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.425+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.425+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.425+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.425+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.425+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.425+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.425+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.425+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.425+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.426+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.426+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.426+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.426+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.426+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.426+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.426+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.426+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-03-03T11:33:59.426+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-03-03T11:33:59.429+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-03-03T11:33:59.429+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-03T11:33:59.430+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2020-03-03T11:33:59.438+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.439+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.439+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.169.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:59.440+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.441+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.174.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:59.441+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.175.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:59.441+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:33:59.441+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2020-03-03T11:33:59.441+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.168.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:59.443+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.176.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.443+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.177.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.444+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:59.445+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-03-03T11:33:59.446+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-03-03T11:33:59.446+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2020-03-03T11:33:59.448+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-03-03T11:33:59.448+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.448+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.167.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-03-03T11:33:59.449+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-03-03T11:33:59.450+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-03-03T11:33:59.450+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2020-03-03T11:33:59.450+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:59.451+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:33:59.451+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3638564551.100401155.85395>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:59.451+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3638564551.100401155.85395>,
                                  inet_tcp_dist,<0.183.0>,
                                  #Ref<0.3638564551.100401154.85916>}
[ns_server:debug,2020-03-03T11:33:59.453+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-03-03T11:33:59.453+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2020-03-03T11:33:59.453+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-03T11:33:59.454+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-03-03T11:33:59.454+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.458+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.186.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:59.459+05:30,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,3,3},{11,33,59}}},
               {memory,
                   [{total,26446520},
                    {processes,9662080},
                    {processes_used,9640576},
                    {system,16784440},
                    {atom,388625},
                    {atom_used,364408},
                    {binary,106968},
                    {code,8250921},
                    {ets,1504264}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    error_handler,application,error_logger,code,
                    application_master,file,file_server,file_io_server,
                    code_server,heart,kernel,gen,proc_lib,filename,ets,
                    application_controller,lists,gen_event,supervisor,
                    gen_server,erl_parse,erl_eval,erl_lint,
                    erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{os_mon,"CPO  CXC 138 46","2.4.4"},
                    {sasl,"SASL  CXC 138 11","3.1.2"},
                    {ns_server,"Couchbase server","6.5.0-4960-enterprise"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ale,"Another Logger for Erlang","0.0.0"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,129},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,auth,httpd_sup,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    release_handler,lhttpc_sup,httpc_sup,lhttpc_manager,
                    alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,kernel_safe_sup,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    'sink-disk_xdcr',timer_server,'sink-disk_debug',ale_sup,
                    'sink-disk_error',inet_db,'sink-disk_default',
                    ssl_pem_cache_dist,ale_dynamic_sup,rex,global_group,
                    net_sup,kernel_sup,ssl_connection_sup,global_name_server,
                    ssl_admin_sup,tftp_sup,ssl_sup,root_sup,erts_code_purger,
                    os_mon_sup,file_server_2,error_logger,cpu_sup,erl_epmd,
                    init,memsup,erl_signal_server,net_kernel,disksup,ale,
                    dist_manager,ssl_pem_cache,ssl_manager,ssl_dist_admin_sup,
                    ssl_dist_connection_sup,ssl_dist_sup,user,
                    ssl_tls_dist_proxy,ssl_manager_dist,sasl_safe_sup,
                    ssl_listen_tracker_sup,code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,0}]
[ns_server:info,2020-03-03T11:33:59.462+05:30,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"cfa0f75f28402d2e1aa254b2a374bead19433526\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" upstream=\"master\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"f2a16b53bb74146f20d18ba2c0443d5f10a9a550\" upstream=\"master\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4960\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" upstream=\"master\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" upstream=\"mad-hatter\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" upstream=\"master\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" upstream=\"master\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" upstream=\"master\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" upstream=\"mad-hatter\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" upstream=\"cb-master\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" upstream=\"master\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" upstream=\"master\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" upstream=\"mad-hatter\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" upstream=\"mad-hatter\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" upstream=\"mad-hatter\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" upstream=\"master\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" upstream=\"master\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" upstream=\"mad-hatter\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" upstream=\"mad-hatter\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" upstream=\"mad-hatter\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" upstream=\"mad-hatter\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" upstream=\"master\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" upstream=\"master\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" upstream=\"master\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" upstream=\"mad-hatter\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" upstream=\"master\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"7a3aa547b3f5eb3ea28d279a08384609cd2cea7c\" upstream=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" upstream=\"mad-hatter\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" upstream=\"master\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" upstream=\"master\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" upstream=\"master\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" upstream=\"v1.1.0-couchbase\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"ee64d41320d14fabe814a241a5cf4f6a6f6e827a\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" upstream=\"master\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" upstream=\"master\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-03-03T11:33:59.464+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.187.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.465+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.465+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:59.465+05:30,ns_1@cb.local:ns_config_sup<0.190.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-03-03T11:33:59.466+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.466+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:59.480+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-03-03T11:33:59.481+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2020-03-03T11:33:59.489+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[{alert_limits,
   [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
  {audit,
   [{auditd_enabled,false},
    {rotate_interval,86400},
    {rotate_size,20971520},
    {disabled,[]},
    {sync,[]},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
  {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
  {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
  {autocompaction,
   [{database_fragmentation_threshold,{30,undefined}},
    {view_fragmentation_threshold,{30,undefined}}]},
  {buckets,[{configs,[]}]},
  {cbas_memory_quota,2174},
  {cert_and_pkey,
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFfi2B3wIO/gwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAyYWJmMjVlZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMmFiZjI1\nZWUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDI7xEpYzw8VsEaLCx3\nQQVbkzsO6PmRhi08x2I8YCA1DbAT1zVEJIkEG1u91CWD7eAhWsCD3TWwBFZfcERe\n4yqxtt5zpsN84LQXkd18MWeFYeZCHlbul4N7Xhs4PavRzjWlbTk8Qh4tTNIbioFs\n5JuPzeY6csaWRKrS3j35kY37lhmPz8EOgK4wOd1Fo7vdtEF4whXV/KW/f8JJvY63\n8LScK2GEZKz1EP9HbmfcCYf+/N0tqUHx2kgz98JBm3S/6EEbxWvVrFAosEhPbA3Q\nb7GUvIuPEahHQDqhL5pRw+H/KdOoLFgCsaWYk8niAZ9DOTLrDCQIJEEzEz+xmwj1\nn9AXAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCijNJXd2H4F3KW\nRbv5SJxGN4t7rFKL4kXa9eRtrfa1CTHLU/C3+2opGhPw0354STXmE4zaBezp58M4\nNWjVgVo+uftij005x0y/daQUt0zJX6yUeV547Rxlqa/iw2u6SOWRMh+beN4vXiF3\nT3ZfIWZyx0zpG9In0EmuCEi6FgVpw3eRqDUwe52dDx0NFzVnrZVNKE3aGlPeJh1V\nJh6YsoQDsTr0n5kDcj7F3wSUnUvWTxmAeXo9IHSHAKzhqglnwaQ0ebWXN/C03ZyG\nTxONnMOyo3hAnI5YhLIUAly/nChmaZTDveDL5TLbifA/XL3UKe+VghtkTMrFSvQm\nvMw0PwM5\n-----END CERTIFICATE-----\n">>,
     <<"*****">>}]},
  {drop_request_memory_threshold_mib,undefined},
  {email_alerts,
   [{recipients,["root@localhost"]},
    {sender,"couchbase@localhost"},
    {enabled,false},
    {email_server,
     [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
    {alerts,
     [auto_failover_node,auto_failover_maximum_reached,
      auto_failover_other_nodes_down,auto_failover_cluster_too_small,
      auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
      ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
      ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
  {fts_memory_quota,512},
  {index_aware_rebalance_disabled,false},
  {log_redaction_default_cfg,[{redact_level,none}]},
  {max_bucket_count,30},
  {memcached,[]},
  {memory_quota,8886},
  {nodes_wanted,['ns_1@cb.local']},
  {password_policy,[{min_length,6},{must_present,[]}]},
  {quorum_nodes,['ns_1@cb.local']},
  {remote_clusters,[]},
  {replication,[{enabled,true}]},
  {rest,[{port,8091}]},
  {rest_creds,null},
  {secure_headers,[]},
  {server_groups,
   [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
  {set_view_update_daemon,
   [{update_interval,5000},
    {update_min_changes,5000},
    {replica_update_min_changes,5000}]},
  {{couchdb,max_parallel_indexers},4},
  {{couchdb,max_parallel_replica_indexers},2},
  {{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{4,63750434630}}]}]},
  {{metakv,<<"/indexing/settings/config">>},
   <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
  {{request_limit,capi},undefined},
  {{request_limit,rest},undefined},
  {{node,'ns_1@cb.local',address_family},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    inet]},
  {{node,'ns_1@cb.local',audit},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
  {{node,'ns_1@cb.local',capi_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8092]},
  {{node,'ns_1@cb.local',cbas_admin_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9110]},
  {{node,'ns_1@cb.local',cbas_cc_client_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9113]},
  {{node,'ns_1@cb.local',cbas_cc_cluster_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9112]},
  {{node,'ns_1@cb.local',cbas_cc_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9111]},
  {{node,'ns_1@cb.local',cbas_cluster_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9115]},
  {{node,'ns_1@cb.local',cbas_console_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9114]},
  {{node,'ns_1@cb.local',cbas_data_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9116]},
  {{node,'ns_1@cb.local',cbas_debug_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    -1]},
  {{node,'ns_1@cb.local',cbas_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8095]},
  {{node,'ns_1@cb.local',cbas_messaging_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9118]},
  {{node,'ns_1@cb.local',cbas_metadata_callback_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9119]},
  {{node,'ns_1@cb.local',cbas_metadata_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9121]},
  {{node,'ns_1@cb.local',cbas_parent_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9122]},
  {{node,'ns_1@cb.local',cbas_replication_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9120]},
  {{node,'ns_1@cb.local',cbas_result_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9117]},
  {{node,'ns_1@cb.local',cbas_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18095]},
  {{node,'ns_1@cb.local',compaction_daemon},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@cb.local',config_version},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {6,5}]},
  {{node,'ns_1@cb.local',erl_external_listeners},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {inet,false},
    {inet6,false}]},
  {{node,'ns_1@cb.local',eventing_debug_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9140]},
  {{node,'ns_1@cb.local',eventing_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8096]},
  {{node,'ns_1@cb.local',eventing_https_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18096]},
  {{node,'ns_1@cb.local',fts_grpc_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9130]},
  {{node,'ns_1@cb.local',fts_grpc_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    19130]},
  {{node,'ns_1@cb.local',fts_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8094]},
  {{node,'ns_1@cb.local',fts_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18094]},
  {{node,'ns_1@cb.local',indexer_admin_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9100]},
  {{node,'ns_1@cb.local',indexer_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9102]},
  {{node,'ns_1@cb.local',indexer_https_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    19102]},
  {{node,'ns_1@cb.local',indexer_scan_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9101]},
  {{node,'ns_1@cb.local',indexer_stcatchup_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9104]},
  {{node,'ns_1@cb.local',indexer_stinit_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9103]},
  {{node,'ns_1@cb.local',indexer_stmaint_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9105]},
  {{node,'ns_1@cb.local',is_enterprise},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    true]},
  {{node,'ns_1@cb.local',isasl},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@cb.local',membership},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    active]},
  {{node,'ns_1@cb.local',memcached},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,11210},
    {dedicated_port,11209},
    {dedicated_ssl_port,11206},
    {ssl_port,11207},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_sleeptime,19},
    {log_rotation_period,39003}]},
  {{node,'ns_1@cb.local',memcached_config},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {[{interfaces,
       {memcached_config_mgr,omit_missing_mcd_ports,
        [{[{host,<<"*">>},
           {port,port},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_port},
           {system,true},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,ssl_port},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_ssl_port},
           {system,true},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
      {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
      {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {opentracing,
       {[{enabled,opentracing_enabled},
         {module,{"~s",[opentracing_module]}},
         {config,{"~s",[opentracing_config]}}]}},
      {admin,{"~s",[admin_user]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize},
         {sleeptime,log_sleeptime}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
  {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    11206]},
  {{node,'ns_1@cb.local',memcached_defaults},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {max_connections,65000},
    {system_connections,5000},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {opentracing_enabled,false},
    {opentracing_module,[]},
    {opentracing_config,[]},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {tracing_enabled,true},
    {datatype_snappy,true},
    {num_reader_threads,<<"default">>},
    {num_writer_threads,<<"default">>}]},
  {{node,'ns_1@cb.local',moxi},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,0}]},
  {{node,'ns_1@cb.local',node_encryption},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    false]},
  {{node,'ns_1@cb.local',ns_log},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@cb.local',port_servers},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
  {{node,'ns_1@cb.local',projector_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9999]},
  {{node,'ns_1@cb.local',projector_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9999]},
  {{node,'ns_1@cb.local',query_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8093]},
  {{node,'ns_1@cb.local',rest},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@cb.local',saslauthd_enabled},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    true]},
  {{node,'ns_1@cb.local',ssl_capi_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18092]},
  {{node,'ns_1@cb.local',ssl_query_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18093]},
  {{node,'ns_1@cb.local',ssl_rest_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18091]},
  {{node,'ns_1@cb.local',uuid},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    <<"e32a4d3bd8aa759a4b96cd6ac25889ee">>]},
  {{node,'ns_1@cb.local',xdcr_rest_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9998]},
  {{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    false]}]]
[ns_server:info,2020-03-03T11:33:59.494+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   false]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9998]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   <<"e32a4d3bd8aa759a4b96cd6ac25889ee">>]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18091]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18093]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18092]},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   true]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9999]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',node_encryption},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   false]},
 {{node,'ns_1@cb.local',moxi},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,0}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   11206]},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',membership},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   active]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   true]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9100]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8094]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9130]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8096]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9140]},
 {{node,'ns_1@cb.local',erl_external_listeners},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {inet,false},
   {inet6,false}]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {6,5}]},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8095]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9110]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8092]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
 {{node,'ns_1@cb.local',address_family},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   inet]},
 {{request_limit,rest},undefined},
 {{request_limit,capi},undefined},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{4,63750434630}}]}]},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
 {secure_headers,[]},
 {rest_creds,null},
 {rest,[{port,8091}]},
 {replication,[{enabled,true}]},
 {remote_clusters,[]},
 {quorum_nodes,['ns_1@cb.local']},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {nodes_wanted,['ns_1@cb.local']},
 {memory_quota,8886},
 {memcached,[]},
 {max_bucket_count,30},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {index_aware_rebalance_disabled,false},
 {fts_memory_quota,512},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {drop_request_memory_threshold_mib,undefined},
 {cert_and_pkey,
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFfi2B3wIO/gwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAyYWJmMjVlZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMmFiZjI1\nZWUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDI7xEpYzw8VsEaLCx3\nQQVbkzsO6PmRhi08x2I8YCA1DbAT1zVEJIkEG1u91CWD7eAhWsCD3TWwBFZfcERe\n4yqxtt5zpsN84LQXkd18MWeFYeZCHlbul4N7Xhs4PavRzjWlbTk8Qh4tTNIbioFs\n5JuPzeY6csaWRKrS3j35kY37lhmPz8EOgK4wOd1Fo7vdtEF4whXV/KW/f8JJvY63\n8LScK2GEZKz1EP9HbmfcCYf+/N0tqUHx2kgz98JBm3S/6EEbxWvVrFAosEhPbA3Q\nb7GUvIuPEahHQDqhL5pRw+H/KdOoLFgCsaWYk8niAZ9DOTLrDCQIJEEzEz+xmwj1\nn9AXAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCijNJXd2H4F3KW\nRbv5SJxGN4t7rFKL4kXa9eRtrfa1CTHLU/C3+2opGhPw0354STXmE4zaBezp58M4\nNWjVgVo+uftij005x0y/daQUt0zJX6yUeV547Rxlqa/iw2u6SOWRMh+beN4vXiF3\nT3ZfIWZyx0zpG9In0EmuCEi6FgVpw3eRqDUwe52dDx0NFzVnrZVNKE3aGlPeJh1V\nJh6YsoQDsTr0n5kDcj7F3wSUnUvWTxmAeXo9IHSHAKzhqglnwaQ0ebWXN/C03ZyG\nTxONnMOyo3hAnI5YhLIUAly/nChmaZTDveDL5TLbifA/XL3UKe+VghtkTMrFSvQm\nvMw0PwM5\n-----END CERTIFICATE-----\n">>,
    <<"*****">>}]},
 {cbas_memory_quota,2174},
 {buckets,[{configs,[]}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]}]
[error_logger:info,2020-03-03T11:33:59.497+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.497+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.199.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.498+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.200.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.498+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:33:59.499+05:30,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>} ->
[{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{5,63750434639}}]}]
[error_logger:info,2020-03-03T11:33:59.499+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.202.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:59.500+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.205.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:59.506+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.208.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:59.507+05:30,ns_1@cb.local:menelaus_barrier<0.209.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-03-03T11:33:59.507+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.209.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.507+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.508+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.509+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.213.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:59.518+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2020-03-03T11:33:59.518+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2020-03-03T11:33:59.518+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2020-03-03T11:33:59.540+05:30,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:init:462]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,248,182,7,124,8,59,
             248,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
             6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
             118,101,114,32,50,97,98,102,50,53,101,101,48,30,23,13,49,51,48,
             49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
             57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
             104,98,97,115,101,32,83,101,114,118,101,114,32,50,97,98,102,50,
             53,101,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,
             3,130,1,15,0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,60,60,86,
             193,26,44,44,119,65,5,91,147,59,14,232,249,145,134,45,60,199,98,
             60,96,32,53,13,176,19,215,53,68,36,137,4,27,91,189,212,37,131,
             237,224,33,90,192,131,221,53,176,4,86,95,112,68,94,227,42,177,
             182,222,115,166,195,124,224,180,23,145,221,124,49,103,133,97,
             230,66,30,86,238,151,131,123,94,27,56,61,171,209,206,53,165,109,
             57,60,66,30,45,76,210,27,138,129,108,228,155,143,205,230,58,114,
             198,150,68,170,210,222,61,249,145,141,251,150,25,143,207,193,14,
             128,174,48,57,221,69,163,187,221,180,65,120,194,21,213,252,165,
             191,127,194,73,189,142,183,240,180,156,43,97,132,100,172,245,16,
             255,71,110,103,220,9,135,254,252,221,45,169,65,241,218,72,51,
             247,194,65,155,116,191,232,65,27,197,107,213,172,80,40,176,72,
             79,108,13,208,111,177,148,188,139,143,17,168,71,64,58,161,47,
             154,81,195,225,255,41,211,168,44,88,2,177,165,152,147,201,226,1,
             159,67,57,50,235,12,36,8,36,65,51,19,63,177,155,8,245,159,208,
             23,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,
             164,48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,
             3,85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,134,72,134,247,
             13,1,1,11,5,0,3,130,1,1,0,162,140,210,87,119,97,248,23,114,150,
             69,187,249,72,156,70,55,139,123,172,82,139,226,69,218,245,228,
             109,173,246,181,9,49,203,83,240,183,251,106,41,26,19,240,211,
             126,120,73,53,230,19,140,218,5,236,233,231,195,56,53,104,213,
             129,90,62,185,251,98,143,77,57,199,76,191,117,164,20,183,76,201,
             95,172,148,121,94,120,237,28,101,169,175,226,195,107,186,72,229,
             145,50,31,155,120,222,47,94,33,119,79,118,95,33,102,114,199,76,
             233,27,210,39,208,73,174,8,72,186,22,5,105,195,119,145,168,53,
             48,123,157,157,15,29,13,23,53,103,173,149,77,40,77,218,26,83,
             222,38,29,85,38,30,152,178,132,3,177,58,244,159,153,3,114,62,
             197,223,4,148,157,75,214,79,25,128,121,122,61,32,116,135,0,172,
             225,170,9,103,193,164,52,121,181,151,55,240,180,221,156,134,79,
             19,141,156,195,178,163,120,64,156,142,88,132,178,20,2,92,191,
             156,40,102,105,148,195,189,224,203,229,50,219,137,240,63,92,189,
             212,41,239,149,130,27,100,76,202,197,74,244,38,188,204,52,63,3,
             57>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2020-03-03T11:33:59.541+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.214.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:59.548+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-03T11:33:59.549+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-03T11:33:59.549+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-03T11:33:59.549+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:info,2020-03-03T11:33:59.558+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-03T11:33:59.558+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-03T11:33:59.558+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-03T11:33:59.558+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2020-03-03T11:33:59.559+05:30,ns_1@cb.local:<0.216.0>:restartable:start_child:98]Started child process <0.217.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2020-03-03T11:33:59.558+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.217.0>,menelaus_web}
             started: [{pid,<0.218.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,
                                 248,182,7,124,8,59,248,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,50,97,98,
                                 102,50,53,101,101,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,50,
                                 97,98,102,50,53,101,101,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,
                                 60,60,86,193,26,44,44,119,65,5,91,147,59,14,
                                 232,249,145,134,45,60,199,98,60,96,32,53,13,
                                 176,19,215,53,68,36,137,4,27,91,189,212,37,
                                 131,237,224,33,90,192,131,221,53,176,4,86,
                                 95,112,68,94,227,42,177,182,222,115,166,195,
                                 124,224,180,23,145,221,124,49,103,133,97,
                                 230,66,30,86,238,151,131,123,94,27,56,61,
                                 171,209,206,53,165,109,57,60,66,30,45,76,
                                 210,27,138,129,108,228,155,143,205,230,58,
                                 114,198,150,68,170,210,222,61,249,145,141,
                                 251,150,25,143,207,193,14,128,174,48,57,221,
                                 69,163,187,221,180,65,120,194,21,213,252,
                                 165,191,127,194,73,189,142,183,240,180,156,
                                 43,97,132,100,172,245,16,255,71,110,103,220,
                                 9,135,254,252,221,45,169,65,241,218,72,51,
                                 247,194,65,155,116,191,232,65,27,197,107,
                                 213,172,80,40,176,72,79,108,13,208,111,177,
                                 148,188,139,143,17,168,71,64,58,161,47,154,
                                 81,195,225,255,41,211,168,44,88,2,177,165,
                                 152,147,201,226,1,159,67,57,50,235,12,36,8,
                                 36,65,51,19,63,177,155,8,245,159,208,23,2,3,
                                 1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 162,140,210,87,119,97,248,23,114,150,69,187,
                                 249,72,156,70,55,139,123,172,82,139,226,69,
                                 218,245,228,109,173,246,181,9,49,203,83,240,
                                 183,251,106,41,26,19,240,211,126,120,73,53,
                                 230,19,140,218,5,236,233,231,195,56,53,104,
                                 213,129,90,62,185,251,98,143,77,57,199,76,
                                 191,117,164,20,183,76,201,95,172,148,121,94,
                                 120,237,28,101,169,175,226,195,107,186,72,
                                 229,145,50,31,155,120,222,47,94,33,119,79,
                                 118,95,33,102,114,199,76,233,27,210,39,208,
                                 73,174,8,72,186,22,5,105,195,119,145,168,53,
                                 48,123,157,157,15,29,13,23,53,103,173,149,
                                 77,40,77,218,26,83,222,38,29,85,38,30,152,
                                 178,132,3,177,58,244,159,153,3,114,62,197,
                                 223,4,148,157,75,214,79,25,128,121,122,61,
                                 32,116,135,0,172,225,170,9,103,193,164,52,
                                 121,181,151,55,240,180,221,156,134,79,19,
                                 141,156,195,178,163,120,64,156,142,88,132,
                                 178,20,2,92,191,156,40,102,105,148,195,189,
                                 224,203,229,50,219,137,240,63,92,189,212,41,
                                 239,149,130,27,100,76,202,197,74,244,38,188,
                                 204,52,63,3,57>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.559+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.217.0>,menelaus_web}
             started: [{pid,<0.236.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,
                                 248,182,7,124,8,59,248,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,50,97,98,
                                 102,50,53,101,101,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,50,
                                 97,98,102,50,53,101,101,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,
                                 60,60,86,193,26,44,44,119,65,5,91,147,59,14,
                                 232,249,145,134,45,60,199,98,60,96,32,53,13,
                                 176,19,215,53,68,36,137,4,27,91,189,212,37,
                                 131,237,224,33,90,192,131,221,53,176,4,86,
                                 95,112,68,94,227,42,177,182,222,115,166,195,
                                 124,224,180,23,145,221,124,49,103,133,97,
                                 230,66,30,86,238,151,131,123,94,27,56,61,
                                 171,209,206,53,165,109,57,60,66,30,45,76,
                                 210,27,138,129,108,228,155,143,205,230,58,
                                 114,198,150,68,170,210,222,61,249,145,141,
                                 251,150,25,143,207,193,14,128,174,48,57,221,
                                 69,163,187,221,180,65,120,194,21,213,252,
                                 165,191,127,194,73,189,142,183,240,180,156,
                                 43,97,132,100,172,245,16,255,71,110,103,220,
                                 9,135,254,252,221,45,169,65,241,218,72,51,
                                 247,194,65,155,116,191,232,65,27,197,107,
                                 213,172,80,40,176,72,79,108,13,208,111,177,
                                 148,188,139,143,17,168,71,64,58,161,47,154,
                                 81,195,225,255,41,211,168,44,88,2,177,165,
                                 152,147,201,226,1,159,67,57,50,235,12,36,8,
                                 36,65,51,19,63,177,155,8,245,159,208,23,2,3,
                                 1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 162,140,210,87,119,97,248,23,114,150,69,187,
                                 249,72,156,70,55,139,123,172,82,139,226,69,
                                 218,245,228,109,173,246,181,9,49,203,83,240,
                                 183,251,106,41,26,19,240,211,126,120,73,53,
                                 230,19,140,218,5,236,233,231,195,56,53,104,
                                 213,129,90,62,185,251,98,143,77,57,199,76,
                                 191,117,164,20,183,76,201,95,172,148,121,94,
                                 120,237,28,101,169,175,226,195,107,186,72,
                                 229,145,50,31,155,120,222,47,94,33,119,79,
                                 118,95,33,102,114,199,76,233,27,210,39,208,
                                 73,174,8,72,186,22,5,105,195,119,145,168,53,
                                 48,123,157,157,15,29,13,23,53,103,173,149,
                                 77,40,77,218,26,83,222,38,29,85,38,30,152,
                                 178,132,3,177,58,244,159,153,3,114,62,197,
                                 223,4,148,157,75,214,79,25,128,121,122,61,
                                 32,116,135,0,172,225,170,9,103,193,164,52,
                                 121,181,151,55,240,180,221,156,134,79,19,
                                 141,156,195,178,163,120,64,156,142,88,132,
                                 178,20,2,92,191,156,40,102,105,148,195,189,
                                 224,203,229,50,219,137,240,63,92,189,212,41,
                                 239,149,130,27,100,76,202,197,74,244,38,188,
                                 204,52,63,3,57>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.560+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.216.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.560+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:59.565+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.254.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.566+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.257.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.569+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.259.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:33:59.571+05:30,ns_1@cb.local:users_replicator<0.259.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-03-03T11:33:59.572+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.259.0>
[ns_server:debug,2020-03-03T11:33:59.572+05:30,ns_1@cb.local:users_replicator<0.259.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.260.0>
[ns_server:debug,2020-03-03T11:33:59.573+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-03-03T11:33:59.574+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.260.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.574+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.258.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:33:59.577+05:30,ns_1@cb.local:compiled_roles_cache<0.262.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-03-03T11:33:59.577+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.262.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.579+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.265.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.579+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.256.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:59.580+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.268.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:33:59.580+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.269.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:33:59.588+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-03-03T11:33:59.588+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 14ms
[ns_server:debug,2020-03-03T11:33:59.590+05:30,ns_1@cb.local:users_replicator<0.259.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[ns_server:debug,2020-03-03T11:33:59.590+05:30,ns_1@cb.local:wait_link_to_couchdb_node<0.273.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[error_logger:info,2020-03-03T11:33:59.590+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.272.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:33:59.591+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:59.591+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:33:59.591+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3638564551.100401153.85726>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:59.591+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3638564551.100401153.85726>,
                                  inet_tcp_dist,<0.276.0>,
                                  #Ref<0.3638564551.100401153.85730>}
[ns_server:debug,2020-03-03T11:33:59.591+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3638564551.100401153.85726>,
                               inet_tcp_dist,<0.276.0>,
                               #Ref<0.3638564551.100401153.85730>}
[error_logger:info,2020-03-03T11:33:59.591+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.276.0>,shutdown}}
[ns_server:debug,2020-03-03T11:33:59.591+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:33:59.591+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:59.791+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-03T11:33:59.791+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:33:59.792+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3638564551.100401155.85473>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:33:59.792+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3638564551.100401155.85473>,
                                  inet_tcp_dist,<0.279.0>,
                                  #Ref<0.3638564551.100401155.85477>}
[ns_server:debug,2020-03-03T11:33:59.826+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-03T11:34:00.028+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-03-03T11:34:00.274+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.283.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:00.318+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-03T11:34:00.328+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3638564551.100401155.85473>,
                               inet_tcp_dist,<0.279.0>,
                               #Ref<0.3638564551.100401155.85477>}
[error_logger:info,2020-03-03T11:34:00.328+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.279.0>,connection_closed}}
[ns_server:info,2020-03-03T11:34:00.475+05:30,ns_1@cb.local:ns_couchdb_port<0.272.0>:ns_port_server:log:224]ns_couchdb<0.272.0>: Apache CouchDB  (LogLevel=info) is starting.
ns_couchdb<0.272.0>: Failure to start Mochiweb: eaddrinuse
ns_couchdb<0.272.0>: 4311: Booted. Waiting for shutdown request
ns_couchdb<0.272.0>: [os_mon] memory supervisor port (memsup): Erlang has closed
ns_couchdb<0.272.0>: [os_mon] cpu supervisor port (cpu_sup): Erlang has closed

[error_logger:info,2020-03-03T11:34:00.519+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:00.519+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:34:00.519+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3638564551.100401155.85485>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:00.519+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3638564551.100401155.85485>,
                                  inet_tcp_dist,<0.285.0>,
                                  #Ref<0.3638564551.100401154.86169>}
[ns_server:debug,2020-03-03T11:34:00.519+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3638564551.100401155.85485>,
                               inet_tcp_dist,<0.285.0>,
                               #Ref<0.3638564551.100401154.86169>}
[error_logger:info,2020-03-03T11:34:00.519+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.285.0>,shutdown}}
[ns_server:debug,2020-03-03T11:34:00.519+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:34:00.519+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:00.720+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-03T11:34:00.720+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:00.720+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3638564551.100401155.85499>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:00.720+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3638564551.100401155.85499>,
                                  inet_tcp_dist,<0.288.0>,
                                  #Ref<0.3638564551.100401155.85501>}
[error_logger:info,2020-03-03T11:34:00.720+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.288.0>,shutdown}}
[ns_server:debug,2020-03-03T11:34:00.720+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3638564551.100401155.85499>,
                               inet_tcp_dist,<0.288.0>,
                               #Ref<0.3638564551.100401155.85501>}
[error_logger:info,2020-03-03T11:34:00.720+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:00.720+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-03-03T11:34:00.921+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-03T11:34:00.921+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:00.921+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3638564551.100401154.86175>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:00.921+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3638564551.100401154.86175>,
                                  inet_tcp_dist,<0.291.0>,
                                  #Ref<0.3638564551.100401154.86179>}
[ns_server:debug,2020-03-03T11:34:00.921+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:34:00.921+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.291.0>,shutdown}}
[ns_server:debug,2020-03-03T11:34:00.921+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3638564551.100401154.86175>,
                               inet_tcp_dist,<0.291.0>,
                               #Ref<0.3638564551.100401154.86179>}
[error_logger:info,2020-03-03T11:34:00.921+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:info,2020-03-03T11:34:00.947+05:30,ns_1@cb.local:ns_couchdb_port<0.272.0>:ns_port_server:log:224]ns_couchdb<0.272.0>: {"Kernel pid terminated",application_controller,"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\"},{line,102}]},{supervisor,do_start_child,2,[{file,\"supervisor.erl\"},{line,365}]},{supervisor,start_children,3,[{file,\"supervisor.erl\"},{line,348}]},{supervisor,init_children,2,[{file,\"supervisor.erl\"},{line,314}]},{gen_server,init_it,2,[{file,\"gen_server.erl\"},{line,365}]},{gen_server,init_it,6,[{file,\"gen_server.erl\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}"}
ns_couchdb<0.272.0>: Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro
ns_couchdb<0.272.0>: 
ns_couchdb<0.272.0>: Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done

[error_logger:error,2020-03-03T11:34:00.947+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]** Generic server ns_couchdb_port terminating 
** Last message in was {#Port<0.5096>,{exit_status,1}}
** When Server state == {state,#Port<0.5096>,
                            {ns_couchdb,"/opt/couchbase/lib/erlang/bin/erl",
                                ["-pa",
                                 "/opt/couchbase/lib/erlang/lib/asn1-5.0.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/compiler-7.1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEvent-2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEventDomain-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosFileTransfer-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosNotification-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosProperty-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTime-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTransactions-1.3.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/crypto-4.2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/dialyzer-3.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/diameter-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/edoc-0.9.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eldap-1.2.3.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_docgen-0.7.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_interface-3.10.2.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erts-9.3.3.9/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eunit-2.3.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/hipe-3.17.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ic-4.4.4.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/inets-6.5.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/mnesia-4.15.3.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/orber-3.8.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/os_mon-2.4.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/otp_mibs-1.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/parsetools-2.1.6/ebin",
                                 "/opt/couchbase/lib/erlang/lib/public_key-1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/reltool-0.7.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/runtime_tools-1.12.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/sasl-3.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/snmp-5.2.11/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssh-4.6.9.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssl-8.2.6.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/syntax_tools-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/tools-2.11.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/xmerl-1.3.16.1/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/gc-couchbase-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/vtree-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/wkb-1.2.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch-1.2.0a-961ad59-git/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_audit-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_dcp-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_index_merger-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_set_view-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_view_parser-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/ejson-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/erlang-oauth/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/etap/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/lhttpc-1.3/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mapreduce-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mochiweb-1.4.1/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/snappy-1.0.4/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ale/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/gen_smtp/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_babysitter/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_couchdb/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/ebin",
                                 "/opt/couchbase/lib/erlang/lib/stdlib-3.4.5.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/kernel-5.4.3.2/ebin",
                                 ".","-couch_ini",
                                 "/opt/couchbase/etc/couchdb/default.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/capi.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/geocouch.ini",
                                 "/opt/couchbase/etc/couchdb/local.ini",
                                 "-kernel","error_logger","false","-kernel",
                                 "error_logger","false","inetrc",
                                 "\"/opt/couchbase/etc/couchbase/hosts.cfg\"",
                                 "dist_config_file",
                                 "\"/opt/couchbase/var/lib/couchbase/config/dist_cfg\"",
                                 "-ssl_dist_optfile",
                                 "/opt/couchbase/etc/couchbase/ssl_dist_opts",
                                 "-setcookie",
                                 "dce5392bcba7669ee9f057b78581e574cccf9efc1961f2376ff32b0a61220948",
                                 "-name","couchdb_ns_1@cb.local","-smp",
                                 "enable","+P","327680","+K","true","-kernel",
                                 "error_logger","false","-sasl",
                                 "sasl_error_logger","false","-nouser",
                                 "-hidden","-proto_dist","cb","-epmd_module",
                                 "cb_epmd","-start_epmd","false","-run",
                                 "child_erlang","child_start","ns_couchdb"],
                                [use_stdio,
                                 {env,
                                     [{"NS_COUCHDB_ENV_ARGS",
                                       "[{ns_server_node,'ns_1@cb.local'},\n {path_config_tmpdir,\"/opt/couchbase/var/lib/couchbase/tmp\"},\n {net_kernel_verbosity,10},\n {loglevel_error_logger,debug},\n {path_config_libdir,\"/opt/couchbase/lib\"},\n {loglevel_stats,debug},\n {loglevel_menelaus,debug},\n {path_config_secdir,\"/opt/couchbase/etc/security\"},\n {loglevel_user,debug},\n {path_config_etcdir,\"/opt/couchbase/etc/couchbase\"},\n {loglevel_ns_server,debug},\n {loglevel_mapreduce_errors,debug},\n {loglevel_rebalance,debug},\n {loglevel_default,debug},\n {disk_sink_opts,[{rotation,[{compress,true},\n                             {size,41943040},\n                             {num_files,10},\n                             {buffer_size_max,52428800}]}]},\n {loglevel_cbas,debug},\n {loglevel_xdcr,debug},\n {loglevel_ns_doctor,debug},\n {loglevel_access,info},\n {error_logger_mf_dir,\"/opt/couchbase/var/lib/couchbase/logs\"},\n {path_config_datadir,\"/opt/couchbase/var/lib/couchbase\"},\n {loglevel_cluster,debug},\n {loglevel_couchdb,info},\n {loglevel_views,debug},\n {path_config_bindir,\"/opt/couchbase/bin\"}]"},
                                      {"ERL_CRASH_DUMP",
                                       "erl_crash.dump.1583215409.3666.ns_couchdb"}]}]},
                            {ringbuffer,1190,1024,
                                {[{<<"Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done">>,
                                   80},
                                  {<<>>,0},
                                  {<<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                                   200}],
                                 [{<<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>,
                                   910}]}},
                            undefined,
                            {ok,{-576460749714,
                                 #Ref<0.3638564551.100401154.86174>}},
                            [<<"Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done">>,
                             <<>>,
                             <<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                             <<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>],
                            0}
** Reason for termination == 
** {abnormal,1}

[ns_server:error,2020-03-03T11:34:00.949+05:30,ns_1@cb.local:wait_link_to_couchdb_node<0.273.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:189]ns_couchdb_port(<0.272.0>) died with reason {abnormal,1}
[ns_server:debug,2020-03-03T11:34:00.949+05:30,ns_1@cb.local:<0.267.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.265.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:00.949+05:30,ns_1@cb.local:<0.264.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.262.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:00.949+05:30,ns_1@cb.local:<0.263.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.262.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:00.949+05:30,ns_1@cb.local:<0.255.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.254.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:00.949+05:30,ns_1@cb.local:<0.216.0>:restartable:shutdown_child:120]Successfully terminated process <0.217.0>
[ns_server:debug,2020-03-03T11:34:00.950+05:30,ns_1@cb.local:<0.215.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.214.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:00.950+05:30,ns_1@cb.local:<0.266.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.265.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:00.950+05:30,ns_1@cb.local:<0.201.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.200.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:00.950+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:wait_saver:866]Done waiting for saver.
[error_logger:error,2020-03-03T11:34:00.951+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_port_server:init/1
    pid: <0.272.0>
    registered_name: ns_couchdb_port
    exception exit: {abnormal,1}
      in function  gen_server:handle_common_reply/8 (gen_server.erl, line 726)
    ancestors: [ns_server_nodes_sup,<0.206.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 1
    messages: [{'EXIT',#Port<0.5096>,normal}]
    links: [<0.207.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 6772
    stack_size: 27
    reductions: 11884
  neighbours:

[error_logger:error,2020-03-03T11:34:00.952+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: erlang:apply/2
    pid: <0.273.0>
    registered_name: wait_link_to_couchdb_node
    exception exit: {abnormal,1}
      in function  ns_server_nodes_sup:do_wait_link_to_couchdb_node/1 (src/ns_server_nodes_sup.erl, line 190)
    ancestors: [ns_server_nodes_sup,<0.206.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.207.0>,<0.274.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 987
    stack_size: 27
    reductions: 3381
  neighbours:
    neighbour:
      pid: <0.274.0>
      registered_name: []
      initial call: ns_server_nodes_sup:'-do_wait_link_to_couchdb_node/1-fun-2-'/0
      current_function: {timer,sleep,1}
      ancestors: [wait_link_to_couchdb_node,ns_server_nodes_sup,<0.206.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
      message_queue_len: 0
      links: [<0.273.0>]
      trap_exit: false
      status: waiting
      heap_size: 2586
      stack_size: 12
      reductions: 10885
      current_stacktrace: [{timer,sleep,1,[{file,"timer.erl"},{line,153}]},
                  {misc,poll_for_condition_rec,3,
                      [{file,"src/misc.erl"},{line,508}]},
                  {ns_server_nodes_sup,
                      '-do_wait_link_to_couchdb_node/1-fun-2-',2,
                      [{file,"src/ns_server_nodes_sup.erl"},{line,159}]},
                  {proc_lib,init_p,3,[{file,"proc_lib.erl"},{line,232}]}]

[error_logger:error,2020-03-03T11:34:00.952+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    start_error
     Reason:     {abnormal,1}
     Offender:   [{pid,undefined},
                  {name,wait_for_couchdb_node},
                  {mfargs,{erlang,apply,
                                  [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:error,2020-03-03T11:34:00.952+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    shutdown_error
     Reason:     {abnormal,1}
     Offender:   [{pid,<0.272.0>},
                  {name,start_couchdb_node},
                  {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                  {restart_type,{permanent,5}},
                  {shutdown,86400000},
                  {child_type,worker}]


[error_logger:error,2020-03-03T11:34:00.952+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_cluster_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,wait_for_couchdb_node,
                         {abnormal,1}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_nodes_sup},
                  {mfargs,
                      {restartable,start_link,
                          [{ns_server_nodes_sup,start_link,[]},infinity]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-03-03T11:34:00.952+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,root_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                             {failed_to_start_child,wait_for_couchdb_node,
                                 {abnormal,1}}}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_cluster_sup},
                  {mfargs,{ns_server_cluster_sup,start_link,[]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-03-03T11:34:00.952+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: application_master:init/4
    pid: <0.117.0>
    registered_name: []
    exception exit: {{shutdown,
                      {failed_to_start_child,ns_server_cluster_sup,
                       {shutdown,
                        {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                          {failed_to_start_child,wait_for_couchdb_node,
                           {abnormal,1}}}}}}},
                     {ns_server,start,[normal,[]]}}
      in function  application_master:init/4 (application_master.erl, line 134)
    ancestors: [<0.116.0>]
    message_queue_len: 1
    messages: [{'EXIT',<0.118.0>,normal}]
    links: [<0.116.0>,<0.33.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 274
  neighbours:

[error_logger:info,2020-03-03T11:34:00.952+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
         application: ns_server
              exited: {{shutdown,
                        {failed_to_start_child,ns_server_cluster_sup,
                         {shutdown,
                          {failed_to_start_child,ns_server_nodes_sup,
                           {shutdown,
                            {failed_to_start_child,wait_for_couchdb_node,
                             {abnormal,1}}}}}}},
                       {ns_server,start,[normal,[]]}}
                type: permanent

[error_logger:info,2020-03-03T11:34:00.953+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/8689"}}

[error_logger:info,2020-03-03T11:34:00.953+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/4486"}}

[error_logger:info,2020-03-03T11:34:00.953+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-calculator/154"}}

[error_logger:info,2020-03-03T11:34:00.953+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-logs/25"}}

[error_logger:info,2020-03-03T11:34:00.953+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/59"}}

[error_logger:info,2020-03-03T11:34:00.953+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gnome-system-monitor/36"}}

[error_logger:info,2020-03-03T11:34:00.953+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/98"}}

[error_logger:info,2020-03-03T11:34:00.953+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-characters/69"}}

[ns_server:info,2020-03-03T11:34:08.203+05:30,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-03-03T11:34:08.222+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-03-03T11:34:08.222+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.222+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.222+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.222+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.222+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.222+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.222+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.222+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.222+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.222+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.222+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.222+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.222+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.222+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.222+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.222+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.223+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.223+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.223+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.223+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.223+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.223+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.223+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.223+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.223+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-03-03T11:34:08.223+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-03-03T11:34:08.227+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-03-03T11:34:08.227+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-03T11:34:08.229+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2020-03-03T11:34:08.236+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.236+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.236+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.169.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:08.237+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.239+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.174.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:08.239+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.175.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:34:08.239+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2020-03-03T11:34:08.239+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:08.239+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.168.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:08.240+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.176.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.240+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.177.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.241+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:08.242+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-03-03T11:34:08.242+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-03-03T11:34:08.242+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2020-03-03T11:34:08.243+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-03-03T11:34:08.243+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.243+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.167.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-03-03T11:34:08.244+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-03-03T11:34:08.247+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-03-03T11:34:08.247+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[ns_server:debug,2020-03-03T11:34:08.247+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-03T11:34:08.247+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:08.247+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.4239454410.2516058115.83805>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:08.247+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.4239454410.2516058115.83805>,
                                  inet_tcp_dist,<0.183.0>,
                                  #Ref<0.4239454410.2516058115.83810>}
[ns_server:debug,2020-03-03T11:34:08.249+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-03-03T11:34:08.249+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2020-03-03T11:34:08.249+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-03T11:34:08.251+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-03-03T11:34:08.251+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.254+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.186.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:08.255+05:30,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,3,3},{11,34,8}}},
               {memory,
                   [{total,26393240},
                    {processes,9608904},
                    {processes_used,9604056},
                    {system,16784336},
                    {atom,388625},
                    {atom_used,364408},
                    {binary,108432},
                    {code,8250921},
                    {ets,1504264}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    error_handler,application,code,application_master,heart,
                    error_logger,file,application_controller,code_server,
                    file_server,gen,gen_event,erl_eval,proc_lib,lists,ets,
                    file_io_server,kernel,supervisor,gen_server,erl_lint,
                    erl_parse,filename,erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{os_mon,"CPO  CXC 138 46","2.4.4"},
                    {sasl,"SASL  CXC 138 11","3.1.2"},
                    {ns_server,"Couchbase server","6.5.0-4960-enterprise"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ale,"Another Logger for Erlang","0.0.0"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,129},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,auth,httpd_sup,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    release_handler,lhttpc_sup,httpc_sup,lhttpc_manager,
                    alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,kernel_safe_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    'sink-disk_xdcr',timer_server,'sink-disk_debug',ale_sup,
                    'sink-disk_error',inet_db,'sink-disk_default',
                    ssl_pem_cache_dist,ale_dynamic_sup,rex,global_group,
                    net_sup,kernel_sup,ssl_connection_sup,global_name_server,
                    ssl_admin_sup,tftp_sup,ssl_sup,root_sup,erts_code_purger,
                    os_mon_sup,file_server_2,error_logger,cpu_sup,erl_epmd,
                    init,memsup,erl_signal_server,net_kernel,disksup,ale,
                    dist_manager,ssl_pem_cache,ssl_manager,ssl_dist_admin_sup,
                    ssl_dist_connection_sup,ssl_dist_sup,user,
                    ssl_tls_dist_proxy,ssl_manager_dist,sasl_safe_sup,
                    ssl_listen_tracker_sup,code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,0}]
[ns_server:info,2020-03-03T11:34:08.258+05:30,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"cfa0f75f28402d2e1aa254b2a374bead19433526\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" upstream=\"master\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"f2a16b53bb74146f20d18ba2c0443d5f10a9a550\" upstream=\"master\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4960\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" upstream=\"master\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" upstream=\"mad-hatter\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" upstream=\"master\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" upstream=\"master\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" upstream=\"master\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" upstream=\"mad-hatter\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" upstream=\"cb-master\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" upstream=\"master\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" upstream=\"master\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" upstream=\"mad-hatter\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" upstream=\"mad-hatter\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" upstream=\"mad-hatter\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" upstream=\"master\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" upstream=\"master\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" upstream=\"mad-hatter\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" upstream=\"mad-hatter\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" upstream=\"mad-hatter\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" upstream=\"mad-hatter\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" upstream=\"master\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" upstream=\"master\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" upstream=\"master\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" upstream=\"mad-hatter\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" upstream=\"master\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"7a3aa547b3f5eb3ea28d279a08384609cd2cea7c\" upstream=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" upstream=\"mad-hatter\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" upstream=\"master\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" upstream=\"master\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" upstream=\"master\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" upstream=\"v1.1.0-couchbase\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"ee64d41320d14fabe814a241a5cf4f6a6f6e827a\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" upstream=\"master\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" upstream=\"master\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-03-03T11:34:08.260+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.187.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.261+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.261+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:08.261+05:30,ns_1@cb.local:ns_config_sup<0.190.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-03-03T11:34:08.261+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.262+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:08.278+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-03-03T11:34:08.279+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2020-03-03T11:34:08.285+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[{alert_limits,
   [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
  {audit,
   [{auditd_enabled,false},
    {rotate_interval,86400},
    {rotate_size,20971520},
    {disabled,[]},
    {sync,[]},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
  {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
  {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
  {autocompaction,
   [{database_fragmentation_threshold,{30,undefined}},
    {view_fragmentation_threshold,{30,undefined}}]},
  {buckets,[{configs,[]}]},
  {cbas_memory_quota,2174},
  {cert_and_pkey,
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFfi2B3wIO/gwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAyYWJmMjVlZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMmFiZjI1\nZWUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDI7xEpYzw8VsEaLCx3\nQQVbkzsO6PmRhi08x2I8YCA1DbAT1zVEJIkEG1u91CWD7eAhWsCD3TWwBFZfcERe\n4yqxtt5zpsN84LQXkd18MWeFYeZCHlbul4N7Xhs4PavRzjWlbTk8Qh4tTNIbioFs\n5JuPzeY6csaWRKrS3j35kY37lhmPz8EOgK4wOd1Fo7vdtEF4whXV/KW/f8JJvY63\n8LScK2GEZKz1EP9HbmfcCYf+/N0tqUHx2kgz98JBm3S/6EEbxWvVrFAosEhPbA3Q\nb7GUvIuPEahHQDqhL5pRw+H/KdOoLFgCsaWYk8niAZ9DOTLrDCQIJEEzEz+xmwj1\nn9AXAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCijNJXd2H4F3KW\nRbv5SJxGN4t7rFKL4kXa9eRtrfa1CTHLU/C3+2opGhPw0354STXmE4zaBezp58M4\nNWjVgVo+uftij005x0y/daQUt0zJX6yUeV547Rxlqa/iw2u6SOWRMh+beN4vXiF3\nT3ZfIWZyx0zpG9In0EmuCEi6FgVpw3eRqDUwe52dDx0NFzVnrZVNKE3aGlPeJh1V\nJh6YsoQDsTr0n5kDcj7F3wSUnUvWTxmAeXo9IHSHAKzhqglnwaQ0ebWXN/C03ZyG\nTxONnMOyo3hAnI5YhLIUAly/nChmaZTDveDL5TLbifA/XL3UKe+VghtkTMrFSvQm\nvMw0PwM5\n-----END CERTIFICATE-----\n">>,
     <<"*****">>}]},
  {drop_request_memory_threshold_mib,undefined},
  {email_alerts,
   [{recipients,["root@localhost"]},
    {sender,"couchbase@localhost"},
    {enabled,false},
    {email_server,
     [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
    {alerts,
     [auto_failover_node,auto_failover_maximum_reached,
      auto_failover_other_nodes_down,auto_failover_cluster_too_small,
      auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
      ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
      ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
  {fts_memory_quota,512},
  {index_aware_rebalance_disabled,false},
  {log_redaction_default_cfg,[{redact_level,none}]},
  {max_bucket_count,30},
  {memcached,[]},
  {memory_quota,8886},
  {nodes_wanted,['ns_1@cb.local']},
  {password_policy,[{min_length,6},{must_present,[]}]},
  {quorum_nodes,['ns_1@cb.local']},
  {remote_clusters,[]},
  {replication,[{enabled,true}]},
  {rest,[{port,8091}]},
  {rest_creds,null},
  {secure_headers,[]},
  {server_groups,
   [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
  {set_view_update_daemon,
   [{update_interval,5000},
    {update_min_changes,5000},
    {replica_update_min_changes,5000}]},
  {{couchdb,max_parallel_indexers},4},
  {{couchdb,max_parallel_replica_indexers},2},
  {{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{5,63750434639}}]}]},
  {{metakv,<<"/indexing/settings/config">>},
   <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
  {{request_limit,capi},undefined},
  {{request_limit,rest},undefined},
  {{node,'ns_1@cb.local',address_family},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    inet]},
  {{node,'ns_1@cb.local',audit},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
  {{node,'ns_1@cb.local',capi_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8092]},
  {{node,'ns_1@cb.local',cbas_admin_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9110]},
  {{node,'ns_1@cb.local',cbas_cc_client_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9113]},
  {{node,'ns_1@cb.local',cbas_cc_cluster_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9112]},
  {{node,'ns_1@cb.local',cbas_cc_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9111]},
  {{node,'ns_1@cb.local',cbas_cluster_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9115]},
  {{node,'ns_1@cb.local',cbas_console_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9114]},
  {{node,'ns_1@cb.local',cbas_data_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9116]},
  {{node,'ns_1@cb.local',cbas_debug_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    -1]},
  {{node,'ns_1@cb.local',cbas_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8095]},
  {{node,'ns_1@cb.local',cbas_messaging_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9118]},
  {{node,'ns_1@cb.local',cbas_metadata_callback_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9119]},
  {{node,'ns_1@cb.local',cbas_metadata_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9121]},
  {{node,'ns_1@cb.local',cbas_parent_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9122]},
  {{node,'ns_1@cb.local',cbas_replication_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9120]},
  {{node,'ns_1@cb.local',cbas_result_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9117]},
  {{node,'ns_1@cb.local',cbas_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18095]},
  {{node,'ns_1@cb.local',compaction_daemon},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@cb.local',config_version},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {6,5}]},
  {{node,'ns_1@cb.local',erl_external_listeners},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {inet,false},
    {inet6,false}]},
  {{node,'ns_1@cb.local',eventing_debug_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9140]},
  {{node,'ns_1@cb.local',eventing_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8096]},
  {{node,'ns_1@cb.local',eventing_https_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18096]},
  {{node,'ns_1@cb.local',fts_grpc_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9130]},
  {{node,'ns_1@cb.local',fts_grpc_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    19130]},
  {{node,'ns_1@cb.local',fts_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8094]},
  {{node,'ns_1@cb.local',fts_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18094]},
  {{node,'ns_1@cb.local',indexer_admin_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9100]},
  {{node,'ns_1@cb.local',indexer_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9102]},
  {{node,'ns_1@cb.local',indexer_https_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    19102]},
  {{node,'ns_1@cb.local',indexer_scan_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9101]},
  {{node,'ns_1@cb.local',indexer_stcatchup_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9104]},
  {{node,'ns_1@cb.local',indexer_stinit_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9103]},
  {{node,'ns_1@cb.local',indexer_stmaint_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9105]},
  {{node,'ns_1@cb.local',is_enterprise},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    true]},
  {{node,'ns_1@cb.local',isasl},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@cb.local',membership},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    active]},
  {{node,'ns_1@cb.local',memcached},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,11210},
    {dedicated_port,11209},
    {dedicated_ssl_port,11206},
    {ssl_port,11207},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_sleeptime,19},
    {log_rotation_period,39003}]},
  {{node,'ns_1@cb.local',memcached_config},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {[{interfaces,
       {memcached_config_mgr,omit_missing_mcd_ports,
        [{[{host,<<"*">>},
           {port,port},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_port},
           {system,true},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,ssl_port},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_ssl_port},
           {system,true},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
      {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
      {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {opentracing,
       {[{enabled,opentracing_enabled},
         {module,{"~s",[opentracing_module]}},
         {config,{"~s",[opentracing_config]}}]}},
      {admin,{"~s",[admin_user]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize},
         {sleeptime,log_sleeptime}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
  {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    11206]},
  {{node,'ns_1@cb.local',memcached_defaults},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {max_connections,65000},
    {system_connections,5000},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {opentracing_enabled,false},
    {opentracing_module,[]},
    {opentracing_config,[]},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {tracing_enabled,true},
    {datatype_snappy,true},
    {num_reader_threads,<<"default">>},
    {num_writer_threads,<<"default">>}]},
  {{node,'ns_1@cb.local',moxi},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,0}]},
  {{node,'ns_1@cb.local',node_encryption},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    false]},
  {{node,'ns_1@cb.local',ns_log},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@cb.local',port_servers},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
  {{node,'ns_1@cb.local',projector_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9999]},
  {{node,'ns_1@cb.local',projector_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9999]},
  {{node,'ns_1@cb.local',query_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8093]},
  {{node,'ns_1@cb.local',rest},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@cb.local',saslauthd_enabled},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    true]},
  {{node,'ns_1@cb.local',ssl_capi_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18092]},
  {{node,'ns_1@cb.local',ssl_query_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18093]},
  {{node,'ns_1@cb.local',ssl_rest_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18091]},
  {{node,'ns_1@cb.local',uuid},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    <<"e32a4d3bd8aa759a4b96cd6ac25889ee">>]},
  {{node,'ns_1@cb.local',xdcr_rest_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9998]},
  {{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    false]}]]
[ns_server:info,2020-03-03T11:34:08.288+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   false]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9998]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   <<"e32a4d3bd8aa759a4b96cd6ac25889ee">>]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18091]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18093]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18092]},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   true]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9999]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',node_encryption},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   false]},
 {{node,'ns_1@cb.local',moxi},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,0}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   11206]},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',membership},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   active]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   true]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9100]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8094]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9130]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8096]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9140]},
 {{node,'ns_1@cb.local',erl_external_listeners},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {inet,false},
   {inet6,false}]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {6,5}]},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8095]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9110]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8092]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
 {{node,'ns_1@cb.local',address_family},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   inet]},
 {{request_limit,rest},undefined},
 {{request_limit,capi},undefined},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{5,63750434639}}]}]},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
 {secure_headers,[]},
 {rest_creds,null},
 {rest,[{port,8091}]},
 {replication,[{enabled,true}]},
 {remote_clusters,[]},
 {quorum_nodes,['ns_1@cb.local']},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {nodes_wanted,['ns_1@cb.local']},
 {memory_quota,8886},
 {memcached,[]},
 {max_bucket_count,30},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {index_aware_rebalance_disabled,false},
 {fts_memory_quota,512},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {drop_request_memory_threshold_mib,undefined},
 {cert_and_pkey,
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFfi2B3wIO/gwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAyYWJmMjVlZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMmFiZjI1\nZWUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDI7xEpYzw8VsEaLCx3\nQQVbkzsO6PmRhi08x2I8YCA1DbAT1zVEJIkEG1u91CWD7eAhWsCD3TWwBFZfcERe\n4yqxtt5zpsN84LQXkd18MWeFYeZCHlbul4N7Xhs4PavRzjWlbTk8Qh4tTNIbioFs\n5JuPzeY6csaWRKrS3j35kY37lhmPz8EOgK4wOd1Fo7vdtEF4whXV/KW/f8JJvY63\n8LScK2GEZKz1EP9HbmfcCYf+/N0tqUHx2kgz98JBm3S/6EEbxWvVrFAosEhPbA3Q\nb7GUvIuPEahHQDqhL5pRw+H/KdOoLFgCsaWYk8niAZ9DOTLrDCQIJEEzEz+xmwj1\nn9AXAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCijNJXd2H4F3KW\nRbv5SJxGN4t7rFKL4kXa9eRtrfa1CTHLU/C3+2opGhPw0354STXmE4zaBezp58M4\nNWjVgVo+uftij005x0y/daQUt0zJX6yUeV547Rxlqa/iw2u6SOWRMh+beN4vXiF3\nT3ZfIWZyx0zpG9In0EmuCEi6FgVpw3eRqDUwe52dDx0NFzVnrZVNKE3aGlPeJh1V\nJh6YsoQDsTr0n5kDcj7F3wSUnUvWTxmAeXo9IHSHAKzhqglnwaQ0ebWXN/C03ZyG\nTxONnMOyo3hAnI5YhLIUAly/nChmaZTDveDL5TLbifA/XL3UKe+VghtkTMrFSvQm\nvMw0PwM5\n-----END CERTIFICATE-----\n">>,
    <<"*****">>}]},
 {cbas_memory_quota,2174},
 {buckets,[{configs,[]}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]}]
[error_logger:info,2020-03-03T11:34:08.291+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.291+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.199.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.292+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.200.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.292+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:34:08.293+05:30,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>} ->
[{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{6,63750434648}}]}]
[error_logger:info,2020-03-03T11:34:08.293+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.202.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:08.294+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.205.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:08.297+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.208.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:08.298+05:30,ns_1@cb.local:menelaus_barrier<0.209.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-03-03T11:34:08.298+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.209.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.298+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.300+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.301+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.213.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:08.309+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2020-03-03T11:34:08.310+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2020-03-03T11:34:08.310+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2020-03-03T11:34:08.321+05:30,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:init:462]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,248,182,7,124,8,59,
             248,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
             6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
             118,101,114,32,50,97,98,102,50,53,101,101,48,30,23,13,49,51,48,
             49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
             57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
             104,98,97,115,101,32,83,101,114,118,101,114,32,50,97,98,102,50,
             53,101,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,
             3,130,1,15,0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,60,60,86,
             193,26,44,44,119,65,5,91,147,59,14,232,249,145,134,45,60,199,98,
             60,96,32,53,13,176,19,215,53,68,36,137,4,27,91,189,212,37,131,
             237,224,33,90,192,131,221,53,176,4,86,95,112,68,94,227,42,177,
             182,222,115,166,195,124,224,180,23,145,221,124,49,103,133,97,
             230,66,30,86,238,151,131,123,94,27,56,61,171,209,206,53,165,109,
             57,60,66,30,45,76,210,27,138,129,108,228,155,143,205,230,58,114,
             198,150,68,170,210,222,61,249,145,141,251,150,25,143,207,193,14,
             128,174,48,57,221,69,163,187,221,180,65,120,194,21,213,252,165,
             191,127,194,73,189,142,183,240,180,156,43,97,132,100,172,245,16,
             255,71,110,103,220,9,135,254,252,221,45,169,65,241,218,72,51,
             247,194,65,155,116,191,232,65,27,197,107,213,172,80,40,176,72,
             79,108,13,208,111,177,148,188,139,143,17,168,71,64,58,161,47,
             154,81,195,225,255,41,211,168,44,88,2,177,165,152,147,201,226,1,
             159,67,57,50,235,12,36,8,36,65,51,19,63,177,155,8,245,159,208,
             23,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,
             164,48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,
             3,85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,134,72,134,247,
             13,1,1,11,5,0,3,130,1,1,0,162,140,210,87,119,97,248,23,114,150,
             69,187,249,72,156,70,55,139,123,172,82,139,226,69,218,245,228,
             109,173,246,181,9,49,203,83,240,183,251,106,41,26,19,240,211,
             126,120,73,53,230,19,140,218,5,236,233,231,195,56,53,104,213,
             129,90,62,185,251,98,143,77,57,199,76,191,117,164,20,183,76,201,
             95,172,148,121,94,120,237,28,101,169,175,226,195,107,186,72,229,
             145,50,31,155,120,222,47,94,33,119,79,118,95,33,102,114,199,76,
             233,27,210,39,208,73,174,8,72,186,22,5,105,195,119,145,168,53,
             48,123,157,157,15,29,13,23,53,103,173,149,77,40,77,218,26,83,
             222,38,29,85,38,30,152,178,132,3,177,58,244,159,153,3,114,62,
             197,223,4,148,157,75,214,79,25,128,121,122,61,32,116,135,0,172,
             225,170,9,103,193,164,52,121,181,151,55,240,180,221,156,134,79,
             19,141,156,195,178,163,120,64,156,142,88,132,178,20,2,92,191,
             156,40,102,105,148,195,189,224,203,229,50,219,137,240,63,92,189,
             212,41,239,149,130,27,100,76,202,197,74,244,38,188,204,52,63,3,
             57>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2020-03-03T11:34:08.321+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.214.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:08.328+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-03T11:34:08.328+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-03T11:34:08.328+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-03T11:34:08.328+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:info,2020-03-03T11:34:08.339+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-03T11:34:08.339+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-03T11:34:08.339+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-03T11:34:08.339+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-03-03T11:34:08.338+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.217.0>,menelaus_web}
             started: [{pid,<0.218.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,
                                 248,182,7,124,8,59,248,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,50,97,98,
                                 102,50,53,101,101,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,50,
                                 97,98,102,50,53,101,101,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,
                                 60,60,86,193,26,44,44,119,65,5,91,147,59,14,
                                 232,249,145,134,45,60,199,98,60,96,32,53,13,
                                 176,19,215,53,68,36,137,4,27,91,189,212,37,
                                 131,237,224,33,90,192,131,221,53,176,4,86,
                                 95,112,68,94,227,42,177,182,222,115,166,195,
                                 124,224,180,23,145,221,124,49,103,133,97,
                                 230,66,30,86,238,151,131,123,94,27,56,61,
                                 171,209,206,53,165,109,57,60,66,30,45,76,
                                 210,27,138,129,108,228,155,143,205,230,58,
                                 114,198,150,68,170,210,222,61,249,145,141,
                                 251,150,25,143,207,193,14,128,174,48,57,221,
                                 69,163,187,221,180,65,120,194,21,213,252,
                                 165,191,127,194,73,189,142,183,240,180,156,
                                 43,97,132,100,172,245,16,255,71,110,103,220,
                                 9,135,254,252,221,45,169,65,241,218,72,51,
                                 247,194,65,155,116,191,232,65,27,197,107,
                                 213,172,80,40,176,72,79,108,13,208,111,177,
                                 148,188,139,143,17,168,71,64,58,161,47,154,
                                 81,195,225,255,41,211,168,44,88,2,177,165,
                                 152,147,201,226,1,159,67,57,50,235,12,36,8,
                                 36,65,51,19,63,177,155,8,245,159,208,23,2,3,
                                 1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 162,140,210,87,119,97,248,23,114,150,69,187,
                                 249,72,156,70,55,139,123,172,82,139,226,69,
                                 218,245,228,109,173,246,181,9,49,203,83,240,
                                 183,251,106,41,26,19,240,211,126,120,73,53,
                                 230,19,140,218,5,236,233,231,195,56,53,104,
                                 213,129,90,62,185,251,98,143,77,57,199,76,
                                 191,117,164,20,183,76,201,95,172,148,121,94,
                                 120,237,28,101,169,175,226,195,107,186,72,
                                 229,145,50,31,155,120,222,47,94,33,119,79,
                                 118,95,33,102,114,199,76,233,27,210,39,208,
                                 73,174,8,72,186,22,5,105,195,119,145,168,53,
                                 48,123,157,157,15,29,13,23,53,103,173,149,
                                 77,40,77,218,26,83,222,38,29,85,38,30,152,
                                 178,132,3,177,58,244,159,153,3,114,62,197,
                                 223,4,148,157,75,214,79,25,128,121,122,61,
                                 32,116,135,0,172,225,170,9,103,193,164,52,
                                 121,181,151,55,240,180,221,156,134,79,19,
                                 141,156,195,178,163,120,64,156,142,88,132,
                                 178,20,2,92,191,156,40,102,105,148,195,189,
                                 224,203,229,50,219,137,240,63,92,189,212,41,
                                 239,149,130,27,100,76,202,197,74,244,38,188,
                                 204,52,63,3,57>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:08.340+05:30,ns_1@cb.local:<0.216.0>:restartable:start_child:98]Started child process <0.217.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2020-03-03T11:34:08.340+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.217.0>,menelaus_web}
             started: [{pid,<0.236.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,
                                 248,182,7,124,8,59,248,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,50,97,98,
                                 102,50,53,101,101,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,50,
                                 97,98,102,50,53,101,101,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,
                                 60,60,86,193,26,44,44,119,65,5,91,147,59,14,
                                 232,249,145,134,45,60,199,98,60,96,32,53,13,
                                 176,19,215,53,68,36,137,4,27,91,189,212,37,
                                 131,237,224,33,90,192,131,221,53,176,4,86,
                                 95,112,68,94,227,42,177,182,222,115,166,195,
                                 124,224,180,23,145,221,124,49,103,133,97,
                                 230,66,30,86,238,151,131,123,94,27,56,61,
                                 171,209,206,53,165,109,57,60,66,30,45,76,
                                 210,27,138,129,108,228,155,143,205,230,58,
                                 114,198,150,68,170,210,222,61,249,145,141,
                                 251,150,25,143,207,193,14,128,174,48,57,221,
                                 69,163,187,221,180,65,120,194,21,213,252,
                                 165,191,127,194,73,189,142,183,240,180,156,
                                 43,97,132,100,172,245,16,255,71,110,103,220,
                                 9,135,254,252,221,45,169,65,241,218,72,51,
                                 247,194,65,155,116,191,232,65,27,197,107,
                                 213,172,80,40,176,72,79,108,13,208,111,177,
                                 148,188,139,143,17,168,71,64,58,161,47,154,
                                 81,195,225,255,41,211,168,44,88,2,177,165,
                                 152,147,201,226,1,159,67,57,50,235,12,36,8,
                                 36,65,51,19,63,177,155,8,245,159,208,23,2,3,
                                 1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 162,140,210,87,119,97,248,23,114,150,69,187,
                                 249,72,156,70,55,139,123,172,82,139,226,69,
                                 218,245,228,109,173,246,181,9,49,203,83,240,
                                 183,251,106,41,26,19,240,211,126,120,73,53,
                                 230,19,140,218,5,236,233,231,195,56,53,104,
                                 213,129,90,62,185,251,98,143,77,57,199,76,
                                 191,117,164,20,183,76,201,95,172,148,121,94,
                                 120,237,28,101,169,175,226,195,107,186,72,
                                 229,145,50,31,155,120,222,47,94,33,119,79,
                                 118,95,33,102,114,199,76,233,27,210,39,208,
                                 73,174,8,72,186,22,5,105,195,119,145,168,53,
                                 48,123,157,157,15,29,13,23,53,103,173,149,
                                 77,40,77,218,26,83,222,38,29,85,38,30,152,
                                 178,132,3,177,58,244,159,153,3,114,62,197,
                                 223,4,148,157,75,214,79,25,128,121,122,61,
                                 32,116,135,0,172,225,170,9,103,193,164,52,
                                 121,181,151,55,240,180,221,156,134,79,19,
                                 141,156,195,178,163,120,64,156,142,88,132,
                                 178,20,2,92,191,156,40,102,105,148,195,189,
                                 224,203,229,50,219,137,240,63,92,189,212,41,
                                 239,149,130,27,100,76,202,197,74,244,38,188,
                                 204,52,63,3,57>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.340+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.216.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.341+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:08.344+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.254.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.345+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.257.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.347+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.259.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:08.348+05:30,ns_1@cb.local:users_replicator<0.259.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-03-03T11:34:08.349+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.259.0>
[ns_server:debug,2020-03-03T11:34:08.349+05:30,ns_1@cb.local:users_replicator<0.259.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.260.0>
[ns_server:debug,2020-03-03T11:34:08.350+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-03-03T11:34:08.350+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.260.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.350+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.258.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:34:08.354+05:30,ns_1@cb.local:compiled_roles_cache<0.262.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-03-03T11:34:08.354+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.262.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.356+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.265.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:08.356+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.256.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:08.357+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.268.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:08.357+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.269.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:08.366+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-03-03T11:34:08.366+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 16ms
[ns_server:debug,2020-03-03T11:34:08.368+05:30,ns_1@cb.local:users_replicator<0.259.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2020-03-03T11:34:08.369+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.272.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:08.369+05:30,ns_1@cb.local:wait_link_to_couchdb_node<0.273.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[error_logger:info,2020-03-03T11:34:08.369+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:08.369+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:34:08.369+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.4239454410.2516058113.84546>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:08.370+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.4239454410.2516058113.84546>,
                                  inet_tcp_dist,<0.276.0>,
                                  #Ref<0.4239454410.2516058113.84550>}
[ns_server:debug,2020-03-03T11:34:08.370+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.4239454410.2516058113.84546>,
                               inet_tcp_dist,<0.276.0>,
                               #Ref<0.4239454410.2516058113.84550>}
[error_logger:info,2020-03-03T11:34:08.370+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.276.0>,shutdown}}
[ns_server:debug,2020-03-03T11:34:08.370+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:34:08.370+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:08.570+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-03T11:34:08.570+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:08.571+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.4239454410.2516058113.84561>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:08.571+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.4239454410.2516058113.84561>,
                                  inet_tcp_dist,<0.279.0>,
                                  #Ref<0.4239454410.2516058113.84565>}
[ns_server:debug,2020-03-03T11:34:08.604+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-03T11:34:08.805+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-03-03T11:34:09.043+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.283.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:09.086+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-03T11:34:09.093+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.4239454410.2516058113.84561>,
                               inet_tcp_dist,<0.279.0>,
                               #Ref<0.4239454410.2516058113.84565>}
[error_logger:info,2020-03-03T11:34:09.093+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.279.0>,connection_closed}}
[ns_server:info,2020-03-03T11:34:09.243+05:30,ns_1@cb.local:ns_couchdb_port<0.272.0>:ns_port_server:log:224]ns_couchdb<0.272.0>: Apache CouchDB  (LogLevel=info) is starting.
ns_couchdb<0.272.0>: Failure to start Mochiweb: eaddrinuse
ns_couchdb<0.272.0>: 4409: Booted. Waiting for shutdown request
ns_couchdb<0.272.0>: [os_mon] memory supervisor port (memsup): Erlang has closed
ns_couchdb<0.272.0>: [os_mon] cpu supervisor port (cpu_sup): Erlang has closed

[error_logger:info,2020-03-03T11:34:09.287+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:09.287+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:34:09.287+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.4239454410.2516058116.83804>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:09.287+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.4239454410.2516058116.83804>,
                                  inet_tcp_dist,<0.285.0>,
                                  #Ref<0.4239454410.2516058116.83808>}
[ns_server:debug,2020-03-03T11:34:09.287+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-03-03T11:34:09.287+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.4239454410.2516058116.83804>,
                               inet_tcp_dist,<0.285.0>,
                               #Ref<0.4239454410.2516058116.83808>}
[error_logger:info,2020-03-03T11:34:09.287+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.285.0>,shutdown}}
[error_logger:info,2020-03-03T11:34:09.287+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:09.488+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-03T11:34:09.487+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:09.488+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.4239454410.2516058116.83814>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:09.488+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.4239454410.2516058116.83814>,
                                  inet_tcp_dist,<0.288.0>,
                                  #Ref<0.4239454410.2516058116.83818>}
[ns_server:debug,2020-03-03T11:34:09.488+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.4239454410.2516058116.83814>,
                               inet_tcp_dist,<0.288.0>,
                               #Ref<0.4239454410.2516058116.83818>}
[error_logger:info,2020-03-03T11:34:09.488+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.288.0>,shutdown}}
[ns_server:debug,2020-03-03T11:34:09.488+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:34:09.488+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-03-03T11:34:09.688+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:09.688+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:34:09.689+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.4239454410.2516058113.84588>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:09.689+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.4239454410.2516058113.84588>,
                                  inet_tcp_dist,<0.291.0>,
                                  #Ref<0.4239454410.2516058113.84592>}
[ns_server:debug,2020-03-03T11:34:09.689+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-03-03T11:34:09.689+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.4239454410.2516058113.84588>,
                               inet_tcp_dist,<0.291.0>,
                               #Ref<0.4239454410.2516058113.84592>}
[error_logger:info,2020-03-03T11:34:09.689+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.291.0>,shutdown}}
[error_logger:info,2020-03-03T11:34:09.689+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:info,2020-03-03T11:34:09.713+05:30,ns_1@cb.local:ns_couchdb_port<0.272.0>:ns_port_server:log:224]ns_couchdb<0.272.0>: {"Kernel pid terminated",application_controller,"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\"},{line,102}]},{supervisor,do_start_child,2,[{file,\"supervisor.erl\"},{line,365}]},{supervisor,start_children,3,[{file,\"supervisor.erl\"},{line,348}]},{supervisor,init_children,2,[{file,\"supervisor.erl\"},{line,314}]},{gen_server,init_it,2,[{file,\"gen_server.erl\"},{line,365}]},{gen_server,init_it,6,[{file,\"gen_server.erl\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}"}
ns_couchdb<0.272.0>: Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro
ns_couchdb<0.272.0>: 
ns_couchdb<0.272.0>: Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done

[error_logger:error,2020-03-03T11:34:09.713+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]** Generic server ns_couchdb_port terminating 
** Last message in was {#Port<0.5096>,{exit_status,1}}
** When Server state == {state,#Port<0.5096>,
                            {ns_couchdb,"/opt/couchbase/lib/erlang/bin/erl",
                                ["-pa",
                                 "/opt/couchbase/lib/erlang/lib/asn1-5.0.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/compiler-7.1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEvent-2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEventDomain-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosFileTransfer-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosNotification-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosProperty-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTime-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTransactions-1.3.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/crypto-4.2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/dialyzer-3.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/diameter-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/edoc-0.9.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eldap-1.2.3.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_docgen-0.7.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_interface-3.10.2.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erts-9.3.3.9/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eunit-2.3.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/hipe-3.17.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ic-4.4.4.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/inets-6.5.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/mnesia-4.15.3.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/orber-3.8.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/os_mon-2.4.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/otp_mibs-1.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/parsetools-2.1.6/ebin",
                                 "/opt/couchbase/lib/erlang/lib/public_key-1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/reltool-0.7.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/runtime_tools-1.12.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/sasl-3.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/snmp-5.2.11/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssh-4.6.9.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssl-8.2.6.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/syntax_tools-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/tools-2.11.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/xmerl-1.3.16.1/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/gc-couchbase-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/vtree-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/wkb-1.2.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch-1.2.0a-961ad59-git/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_audit-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_dcp-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_index_merger-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_set_view-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_view_parser-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/ejson-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/erlang-oauth/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/etap/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/lhttpc-1.3/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mapreduce-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mochiweb-1.4.1/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/snappy-1.0.4/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ale/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/gen_smtp/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_babysitter/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_couchdb/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/ebin",
                                 "/opt/couchbase/lib/erlang/lib/stdlib-3.4.5.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/kernel-5.4.3.2/ebin",
                                 ".","-couch_ini",
                                 "/opt/couchbase/etc/couchdb/default.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/capi.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/geocouch.ini",
                                 "/opt/couchbase/etc/couchdb/local.ini",
                                 "-kernel","error_logger","false","-kernel",
                                 "error_logger","false","inetrc",
                                 "\"/opt/couchbase/etc/couchbase/hosts.cfg\"",
                                 "dist_config_file",
                                 "\"/opt/couchbase/var/lib/couchbase/config/dist_cfg\"",
                                 "-ssl_dist_optfile",
                                 "/opt/couchbase/etc/couchbase/ssl_dist_opts",
                                 "-setcookie",
                                 "dce5392bcba7669ee9f057b78581e574cccf9efc1961f2376ff32b0a61220948",
                                 "-name","couchdb_ns_1@cb.local","-smp",
                                 "enable","+P","327680","+K","true","-kernel",
                                 "error_logger","false","-sasl",
                                 "sasl_error_logger","false","-nouser",
                                 "-hidden","-proto_dist","cb","-epmd_module",
                                 "cb_epmd","-start_epmd","false","-run",
                                 "child_erlang","child_start","ns_couchdb"],
                                [use_stdio,
                                 {env,
                                     [{"NS_COUCHDB_ENV_ARGS",
                                       "[{ns_server_node,'ns_1@cb.local'},\n {path_config_tmpdir,\"/opt/couchbase/var/lib/couchbase/tmp\"},\n {net_kernel_verbosity,10},\n {loglevel_error_logger,debug},\n {path_config_libdir,\"/opt/couchbase/lib\"},\n {loglevel_stats,debug},\n {loglevel_menelaus,debug},\n {path_config_secdir,\"/opt/couchbase/etc/security\"},\n {loglevel_user,debug},\n {path_config_etcdir,\"/opt/couchbase/etc/couchbase\"},\n {loglevel_ns_server,debug},\n {loglevel_mapreduce_errors,debug},\n {loglevel_rebalance,debug},\n {loglevel_default,debug},\n {disk_sink_opts,[{rotation,[{compress,true},\n                             {size,41943040},\n                             {num_files,10},\n                             {buffer_size_max,52428800}]}]},\n {loglevel_cbas,debug},\n {loglevel_xdcr,debug},\n {loglevel_ns_doctor,debug},\n {loglevel_access,info},\n {error_logger_mf_dir,\"/opt/couchbase/var/lib/couchbase/logs\"},\n {path_config_datadir,\"/opt/couchbase/var/lib/couchbase\"},\n {loglevel_cluster,debug},\n {loglevel_couchdb,info},\n {loglevel_views,debug},\n {path_config_bindir,\"/opt/couchbase/bin\"}]"},
                                      {"ERL_CRASH_DUMP",
                                       "erl_crash.dump.1583215409.3666.ns_couchdb"}]}]},
                            {ringbuffer,1190,1024,
                                {[{<<"Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done">>,
                                   80},
                                  {<<>>,0},
                                  {<<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                                   200}],
                                 [{<<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>,
                                   910}]}},
                            undefined,
                            {ok,{-576460749663,
                                 #Ref<0.4239454410.2516058116.83822>}},
                            [<<"Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done">>,
                             <<>>,
                             <<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                             <<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>],
                            0}
** Reason for termination == 
** {abnormal,1}

[ns_server:error,2020-03-03T11:34:09.714+05:30,ns_1@cb.local:wait_link_to_couchdb_node<0.273.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:189]ns_couchdb_port(<0.272.0>) died with reason {abnormal,1}
[ns_server:debug,2020-03-03T11:34:09.714+05:30,ns_1@cb.local:<0.266.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.265.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:09.714+05:30,ns_1@cb.local:<0.263.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.262.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:09.714+05:30,ns_1@cb.local:<0.267.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.265.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:09.714+05:30,ns_1@cb.local:<0.264.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.262.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:09.714+05:30,ns_1@cb.local:<0.255.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.254.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:09.715+05:30,ns_1@cb.local:<0.216.0>:restartable:shutdown_child:120]Successfully terminated process <0.217.0>
[ns_server:debug,2020-03-03T11:34:09.715+05:30,ns_1@cb.local:<0.215.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.214.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:09.715+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:wait_saver:866]Done waiting for saver.
[ns_server:debug,2020-03-03T11:34:09.715+05:30,ns_1@cb.local:<0.201.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.200.0>} exited with reason shutdown
[error_logger:error,2020-03-03T11:34:09.716+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_port_server:init/1
    pid: <0.272.0>
    registered_name: ns_couchdb_port
    exception exit: {abnormal,1}
      in function  gen_server:handle_common_reply/8 (gen_server.erl, line 726)
    ancestors: [ns_server_nodes_sup,<0.206.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 1
    messages: [{'EXIT',#Port<0.5096>,normal}]
    links: [<0.207.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 6772
    stack_size: 27
    reductions: 11884
  neighbours:

[error_logger:error,2020-03-03T11:34:09.716+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: erlang:apply/2
    pid: <0.273.0>
    registered_name: wait_link_to_couchdb_node
    exception exit: {abnormal,1}
      in function  ns_server_nodes_sup:do_wait_link_to_couchdb_node/1 (src/ns_server_nodes_sup.erl, line 190)
    ancestors: [ns_server_nodes_sup,<0.206.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.207.0>,<0.274.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 987
    stack_size: 27
    reductions: 3381
  neighbours:
    neighbour:
      pid: <0.274.0>
      registered_name: []
      initial call: ns_server_nodes_sup:'-do_wait_link_to_couchdb_node/1-fun-2-'/0
      current_function: {timer,sleep,1}
      ancestors: [wait_link_to_couchdb_node,ns_server_nodes_sup,<0.206.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
      message_queue_len: 0
      links: [<0.273.0>]
      trap_exit: false
      status: waiting
      heap_size: 2586
      stack_size: 12
      reductions: 10838
      current_stacktrace: [{timer,sleep,1,[{file,"timer.erl"},{line,153}]},
                  {misc,poll_for_condition_rec,3,
                      [{file,"src/misc.erl"},{line,508}]},
                  {ns_server_nodes_sup,
                      '-do_wait_link_to_couchdb_node/1-fun-2-',2,
                      [{file,"src/ns_server_nodes_sup.erl"},{line,159}]},
                  {proc_lib,init_p,3,[{file,"proc_lib.erl"},{line,232}]}]

[error_logger:error,2020-03-03T11:34:09.717+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    start_error
     Reason:     {abnormal,1}
     Offender:   [{pid,undefined},
                  {name,wait_for_couchdb_node},
                  {mfargs,{erlang,apply,
                                  [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:error,2020-03-03T11:34:09.717+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    shutdown_error
     Reason:     {abnormal,1}
     Offender:   [{pid,<0.272.0>},
                  {name,start_couchdb_node},
                  {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                  {restart_type,{permanent,5}},
                  {shutdown,86400000},
                  {child_type,worker}]


[error_logger:error,2020-03-03T11:34:09.717+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_cluster_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,wait_for_couchdb_node,
                         {abnormal,1}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_nodes_sup},
                  {mfargs,
                      {restartable,start_link,
                          [{ns_server_nodes_sup,start_link,[]},infinity]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-03-03T11:34:09.717+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,root_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                             {failed_to_start_child,wait_for_couchdb_node,
                                 {abnormal,1}}}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_cluster_sup},
                  {mfargs,{ns_server_cluster_sup,start_link,[]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-03-03T11:34:09.717+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: application_master:init/4
    pid: <0.117.0>
    registered_name: []
    exception exit: {{shutdown,
                      {failed_to_start_child,ns_server_cluster_sup,
                       {shutdown,
                        {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                          {failed_to_start_child,wait_for_couchdb_node,
                           {abnormal,1}}}}}}},
                     {ns_server,start,[normal,[]]}}
      in function  application_master:init/4 (application_master.erl, line 134)
    ancestors: [<0.116.0>]
    message_queue_len: 1
    messages: [{'EXIT',<0.118.0>,normal}]
    links: [<0.116.0>,<0.33.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 274
  neighbours:

[error_logger:info,2020-03-03T11:34:09.717+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
         application: ns_server
              exited: {{shutdown,
                        {failed_to_start_child,ns_server_cluster_sup,
                         {shutdown,
                          {failed_to_start_child,ns_server_nodes_sup,
                           {shutdown,
                            {failed_to_start_child,wait_for_couchdb_node,
                             {abnormal,1}}}}}}},
                       {ns_server,start,[normal,[]]}}
                type: permanent

[error_logger:info,2020-03-03T11:34:09.718+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/8689"}}

[error_logger:info,2020-03-03T11:34:09.718+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/4486"}}

[error_logger:info,2020-03-03T11:34:09.718+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-calculator/154"}}

[error_logger:info,2020-03-03T11:34:09.718+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-logs/25"}}

[error_logger:info,2020-03-03T11:34:09.718+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/59"}}

[error_logger:info,2020-03-03T11:34:09.718+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gnome-system-monitor/36"}}

[error_logger:info,2020-03-03T11:34:09.718+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/98"}}

[error_logger:info,2020-03-03T11:34:09.718+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-characters/69"}}

[ns_server:info,2020-03-03T11:34:16.927+05:30,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-03-03T11:34:16.939+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-03-03T11:34:16.943+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-03-03T11:34:16.943+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-03T11:34:16.944+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2020-03-03T11:34:16.952+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:16.953+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:16.953+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.169.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:16.954+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:16.955+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.174.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:16.956+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.175.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:16.956+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:34:16.956+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2020-03-03T11:34:16.956+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.168.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:16.957+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.176.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:16.957+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.177.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:16.958+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:16.959+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-03-03T11:34:16.959+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-03-03T11:34:16.959+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2020-03-03T11:34:16.960+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-03-03T11:34:16.960+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:16.960+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.167.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-03-03T11:34:16.961+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-03-03T11:34:16.965+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-03-03T11:34:16.965+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2020-03-03T11:34:16.965+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:16.965+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:34:16.965+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.685768190.636747779.136797>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:16.965+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.685768190.636747779.136797>,
                                  inet_tcp_dist,<0.183.0>,
                                  #Ref<0.685768190.636747779.136798>}
[ns_server:debug,2020-03-03T11:34:16.967+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-03-03T11:34:16.967+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2020-03-03T11:34:16.968+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-03T11:34:16.971+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-03-03T11:34:16.971+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:16.975+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.186.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:16.976+05:30,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,3,3},{11,34,16}}},
               {memory,
                   [{total,26442976},
                    {processes,9658840},
                    {processes_used,9653744},
                    {system,16784136},
                    {atom,388625},
                    {atom_used,364408},
                    {binary,108600},
                    {code,8250921},
                    {ets,1504264}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    error_handler,application,heart,file,application_master,
                    code_server,kernel,error_logger,lists,file_io_server,code,
                    file_server,application_controller,gen_server,ets,
                    proc_lib,erl_eval,supervisor,filename,erl_parse,gen_event,
                    gen,erl_lint,erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{os_mon,"CPO  CXC 138 46","2.4.4"},
                    {sasl,"SASL  CXC 138 11","3.1.2"},
                    {ns_server,"Couchbase server","6.5.0-4960-enterprise"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ale,"Another Logger for Erlang","0.0.0"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,129},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,auth,httpd_sup,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    release_handler,lhttpc_sup,httpc_sup,lhttpc_manager,
                    alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    kernel_safe_sup,'sink-disk_xdcr',timer_server,
                    'sink-disk_debug',ale_sup,'sink-disk_error',inet_db,
                    'sink-disk_default',ssl_pem_cache_dist,ale_dynamic_sup,
                    rex,global_group,net_sup,kernel_sup,ssl_connection_sup,
                    global_name_server,ssl_admin_sup,tftp_sup,ssl_sup,
                    root_sup,erts_code_purger,os_mon_sup,file_server_2,
                    error_logger,cpu_sup,erl_epmd,init,memsup,
                    erl_signal_server,net_kernel,disksup,ale,dist_manager,
                    ssl_pem_cache,ssl_manager,ssl_dist_admin_sup,
                    ssl_dist_connection_sup,ssl_dist_sup,user,
                    ssl_tls_dist_proxy,ssl_manager_dist,sasl_safe_sup,
                    ssl_listen_tracker_sup,code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,0}]
[ns_server:info,2020-03-03T11:34:16.979+05:30,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"cfa0f75f28402d2e1aa254b2a374bead19433526\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" upstream=\"master\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"f2a16b53bb74146f20d18ba2c0443d5f10a9a550\" upstream=\"master\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4960\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" upstream=\"master\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" upstream=\"mad-hatter\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" upstream=\"master\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" upstream=\"master\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" upstream=\"master\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" upstream=\"mad-hatter\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" upstream=\"cb-master\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" upstream=\"master\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" upstream=\"master\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" upstream=\"mad-hatter\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" upstream=\"mad-hatter\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" upstream=\"mad-hatter\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" upstream=\"master\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" upstream=\"master\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" upstream=\"mad-hatter\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" upstream=\"mad-hatter\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" upstream=\"mad-hatter\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" upstream=\"mad-hatter\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" upstream=\"master\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" upstream=\"master\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" upstream=\"master\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" upstream=\"mad-hatter\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" upstream=\"master\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"7a3aa547b3f5eb3ea28d279a08384609cd2cea7c\" upstream=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" upstream=\"mad-hatter\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" upstream=\"master\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" upstream=\"master\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" upstream=\"master\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" upstream=\"v1.1.0-couchbase\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"ee64d41320d14fabe814a241a5cf4f6a6f6e827a\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" upstream=\"master\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" upstream=\"master\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-03-03T11:34:16.982+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.187.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:16.982+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:16.983+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:16.983+05:30,ns_1@cb.local:ns_config_sup<0.190.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-03-03T11:34:16.983+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:16.983+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:16.997+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-03-03T11:34:16.998+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2020-03-03T11:34:17.004+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[{alert_limits,
   [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
  {audit,
   [{auditd_enabled,false},
    {rotate_interval,86400},
    {rotate_size,20971520},
    {disabled,[]},
    {sync,[]},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
  {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
  {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
  {autocompaction,
   [{database_fragmentation_threshold,{30,undefined}},
    {view_fragmentation_threshold,{30,undefined}}]},
  {buckets,[{configs,[]}]},
  {cbas_memory_quota,2174},
  {cert_and_pkey,
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFfi2B3wIO/gwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAyYWJmMjVlZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMmFiZjI1\nZWUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDI7xEpYzw8VsEaLCx3\nQQVbkzsO6PmRhi08x2I8YCA1DbAT1zVEJIkEG1u91CWD7eAhWsCD3TWwBFZfcERe\n4yqxtt5zpsN84LQXkd18MWeFYeZCHlbul4N7Xhs4PavRzjWlbTk8Qh4tTNIbioFs\n5JuPzeY6csaWRKrS3j35kY37lhmPz8EOgK4wOd1Fo7vdtEF4whXV/KW/f8JJvY63\n8LScK2GEZKz1EP9HbmfcCYf+/N0tqUHx2kgz98JBm3S/6EEbxWvVrFAosEhPbA3Q\nb7GUvIuPEahHQDqhL5pRw+H/KdOoLFgCsaWYk8niAZ9DOTLrDCQIJEEzEz+xmwj1\nn9AXAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCijNJXd2H4F3KW\nRbv5SJxGN4t7rFKL4kXa9eRtrfa1CTHLU/C3+2opGhPw0354STXmE4zaBezp58M4\nNWjVgVo+uftij005x0y/daQUt0zJX6yUeV547Rxlqa/iw2u6SOWRMh+beN4vXiF3\nT3ZfIWZyx0zpG9In0EmuCEi6FgVpw3eRqDUwe52dDx0NFzVnrZVNKE3aGlPeJh1V\nJh6YsoQDsTr0n5kDcj7F3wSUnUvWTxmAeXo9IHSHAKzhqglnwaQ0ebWXN/C03ZyG\nTxONnMOyo3hAnI5YhLIUAly/nChmaZTDveDL5TLbifA/XL3UKe+VghtkTMrFSvQm\nvMw0PwM5\n-----END CERTIFICATE-----\n">>,
     <<"*****">>}]},
  {drop_request_memory_threshold_mib,undefined},
  {email_alerts,
   [{recipients,["root@localhost"]},
    {sender,"couchbase@localhost"},
    {enabled,false},
    {email_server,
     [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
    {alerts,
     [auto_failover_node,auto_failover_maximum_reached,
      auto_failover_other_nodes_down,auto_failover_cluster_too_small,
      auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
      ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
      ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
  {fts_memory_quota,512},
  {index_aware_rebalance_disabled,false},
  {log_redaction_default_cfg,[{redact_level,none}]},
  {max_bucket_count,30},
  {memcached,[]},
  {memory_quota,8886},
  {nodes_wanted,['ns_1@cb.local']},
  {password_policy,[{min_length,6},{must_present,[]}]},
  {quorum_nodes,['ns_1@cb.local']},
  {remote_clusters,[]},
  {replication,[{enabled,true}]},
  {rest,[{port,8091}]},
  {rest_creds,null},
  {secure_headers,[]},
  {server_groups,
   [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
  {set_view_update_daemon,
   [{update_interval,5000},
    {update_min_changes,5000},
    {replica_update_min_changes,5000}]},
  {{couchdb,max_parallel_indexers},4},
  {{couchdb,max_parallel_replica_indexers},2},
  {{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{6,63750434648}}]}]},
  {{metakv,<<"/indexing/settings/config">>},
   <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
  {{request_limit,capi},undefined},
  {{request_limit,rest},undefined},
  {{node,'ns_1@cb.local',address_family},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    inet]},
  {{node,'ns_1@cb.local',audit},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
  {{node,'ns_1@cb.local',capi_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8092]},
  {{node,'ns_1@cb.local',cbas_admin_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9110]},
  {{node,'ns_1@cb.local',cbas_cc_client_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9113]},
  {{node,'ns_1@cb.local',cbas_cc_cluster_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9112]},
  {{node,'ns_1@cb.local',cbas_cc_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9111]},
  {{node,'ns_1@cb.local',cbas_cluster_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9115]},
  {{node,'ns_1@cb.local',cbas_console_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9114]},
  {{node,'ns_1@cb.local',cbas_data_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9116]},
  {{node,'ns_1@cb.local',cbas_debug_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    -1]},
  {{node,'ns_1@cb.local',cbas_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8095]},
  {{node,'ns_1@cb.local',cbas_messaging_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9118]},
  {{node,'ns_1@cb.local',cbas_metadata_callback_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9119]},
  {{node,'ns_1@cb.local',cbas_metadata_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9121]},
  {{node,'ns_1@cb.local',cbas_parent_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9122]},
  {{node,'ns_1@cb.local',cbas_replication_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9120]},
  {{node,'ns_1@cb.local',cbas_result_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9117]},
  {{node,'ns_1@cb.local',cbas_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18095]},
  {{node,'ns_1@cb.local',compaction_daemon},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@cb.local',config_version},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {6,5}]},
  {{node,'ns_1@cb.local',erl_external_listeners},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {inet,false},
    {inet6,false}]},
  {{node,'ns_1@cb.local',eventing_debug_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9140]},
  {{node,'ns_1@cb.local',eventing_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8096]},
  {{node,'ns_1@cb.local',eventing_https_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18096]},
  {{node,'ns_1@cb.local',fts_grpc_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9130]},
  {{node,'ns_1@cb.local',fts_grpc_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    19130]},
  {{node,'ns_1@cb.local',fts_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8094]},
  {{node,'ns_1@cb.local',fts_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18094]},
  {{node,'ns_1@cb.local',indexer_admin_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9100]},
  {{node,'ns_1@cb.local',indexer_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9102]},
  {{node,'ns_1@cb.local',indexer_https_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    19102]},
  {{node,'ns_1@cb.local',indexer_scan_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9101]},
  {{node,'ns_1@cb.local',indexer_stcatchup_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9104]},
  {{node,'ns_1@cb.local',indexer_stinit_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9103]},
  {{node,'ns_1@cb.local',indexer_stmaint_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9105]},
  {{node,'ns_1@cb.local',is_enterprise},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    true]},
  {{node,'ns_1@cb.local',isasl},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@cb.local',membership},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    active]},
  {{node,'ns_1@cb.local',memcached},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,11210},
    {dedicated_port,11209},
    {dedicated_ssl_port,11206},
    {ssl_port,11207},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_sleeptime,19},
    {log_rotation_period,39003}]},
  {{node,'ns_1@cb.local',memcached_config},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {[{interfaces,
       {memcached_config_mgr,omit_missing_mcd_ports,
        [{[{host,<<"*">>},
           {port,port},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_port},
           {system,true},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,ssl_port},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_ssl_port},
           {system,true},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
      {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
      {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {opentracing,
       {[{enabled,opentracing_enabled},
         {module,{"~s",[opentracing_module]}},
         {config,{"~s",[opentracing_config]}}]}},
      {admin,{"~s",[admin_user]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize},
         {sleeptime,log_sleeptime}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
  {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    11206]},
  {{node,'ns_1@cb.local',memcached_defaults},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {max_connections,65000},
    {system_connections,5000},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {opentracing_enabled,false},
    {opentracing_module,[]},
    {opentracing_config,[]},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {tracing_enabled,true},
    {datatype_snappy,true},
    {num_reader_threads,<<"default">>},
    {num_writer_threads,<<"default">>}]},
  {{node,'ns_1@cb.local',moxi},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,0}]},
  {{node,'ns_1@cb.local',node_encryption},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    false]},
  {{node,'ns_1@cb.local',ns_log},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@cb.local',port_servers},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
  {{node,'ns_1@cb.local',projector_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9999]},
  {{node,'ns_1@cb.local',projector_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9999]},
  {{node,'ns_1@cb.local',query_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8093]},
  {{node,'ns_1@cb.local',rest},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@cb.local',saslauthd_enabled},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    true]},
  {{node,'ns_1@cb.local',ssl_capi_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18092]},
  {{node,'ns_1@cb.local',ssl_query_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18093]},
  {{node,'ns_1@cb.local',ssl_rest_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18091]},
  {{node,'ns_1@cb.local',uuid},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    <<"e32a4d3bd8aa759a4b96cd6ac25889ee">>]},
  {{node,'ns_1@cb.local',xdcr_rest_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9998]},
  {{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    false]}]]
[ns_server:info,2020-03-03T11:34:17.008+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   false]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9998]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   <<"e32a4d3bd8aa759a4b96cd6ac25889ee">>]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18091]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18093]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18092]},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   true]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9999]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',node_encryption},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   false]},
 {{node,'ns_1@cb.local',moxi},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,0}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   11206]},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',membership},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   active]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   true]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9100]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8094]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9130]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8096]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9140]},
 {{node,'ns_1@cb.local',erl_external_listeners},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {inet,false},
   {inet6,false}]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {6,5}]},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8095]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9110]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8092]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
 {{node,'ns_1@cb.local',address_family},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   inet]},
 {{request_limit,rest},undefined},
 {{request_limit,capi},undefined},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{6,63750434648}}]}]},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
 {secure_headers,[]},
 {rest_creds,null},
 {rest,[{port,8091}]},
 {replication,[{enabled,true}]},
 {remote_clusters,[]},
 {quorum_nodes,['ns_1@cb.local']},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {nodes_wanted,['ns_1@cb.local']},
 {memory_quota,8886},
 {memcached,[]},
 {max_bucket_count,30},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {index_aware_rebalance_disabled,false},
 {fts_memory_quota,512},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {drop_request_memory_threshold_mib,undefined},
 {cert_and_pkey,
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFfi2B3wIO/gwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAyYWJmMjVlZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMmFiZjI1\nZWUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDI7xEpYzw8VsEaLCx3\nQQVbkzsO6PmRhi08x2I8YCA1DbAT1zVEJIkEG1u91CWD7eAhWsCD3TWwBFZfcERe\n4yqxtt5zpsN84LQXkd18MWeFYeZCHlbul4N7Xhs4PavRzjWlbTk8Qh4tTNIbioFs\n5JuPzeY6csaWRKrS3j35kY37lhmPz8EOgK4wOd1Fo7vdtEF4whXV/KW/f8JJvY63\n8LScK2GEZKz1EP9HbmfcCYf+/N0tqUHx2kgz98JBm3S/6EEbxWvVrFAosEhPbA3Q\nb7GUvIuPEahHQDqhL5pRw+H/KdOoLFgCsaWYk8niAZ9DOTLrDCQIJEEzEz+xmwj1\nn9AXAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCijNJXd2H4F3KW\nRbv5SJxGN4t7rFKL4kXa9eRtrfa1CTHLU/C3+2opGhPw0354STXmE4zaBezp58M4\nNWjVgVo+uftij005x0y/daQUt0zJX6yUeV547Rxlqa/iw2u6SOWRMh+beN4vXiF3\nT3ZfIWZyx0zpG9In0EmuCEi6FgVpw3eRqDUwe52dDx0NFzVnrZVNKE3aGlPeJh1V\nJh6YsoQDsTr0n5kDcj7F3wSUnUvWTxmAeXo9IHSHAKzhqglnwaQ0ebWXN/C03ZyG\nTxONnMOyo3hAnI5YhLIUAly/nChmaZTDveDL5TLbifA/XL3UKe+VghtkTMrFSvQm\nvMw0PwM5\n-----END CERTIFICATE-----\n">>,
    <<"*****">>}]},
 {cbas_memory_quota,2174},
 {buckets,[{configs,[]}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]}]
[error_logger:info,2020-03-03T11:34:17.010+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:17.010+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.199.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:17.022+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.200.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:17.022+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:34:17.023+05:30,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>} ->
[{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{7,63750434657}}]}]
[error_logger:info,2020-03-03T11:34:17.023+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.202.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:17.024+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.205.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:17.028+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.208.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:17.028+05:30,ns_1@cb.local:menelaus_barrier<0.209.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-03-03T11:34:17.028+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.209.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:17.029+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:17.030+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:17.031+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.213.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:17.040+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2020-03-03T11:34:17.040+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2020-03-03T11:34:17.040+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2020-03-03T11:34:17.050+05:30,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:init:462]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,248,182,7,124,8,59,
             248,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
             6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
             118,101,114,32,50,97,98,102,50,53,101,101,48,30,23,13,49,51,48,
             49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
             57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
             104,98,97,115,101,32,83,101,114,118,101,114,32,50,97,98,102,50,
             53,101,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,
             3,130,1,15,0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,60,60,86,
             193,26,44,44,119,65,5,91,147,59,14,232,249,145,134,45,60,199,98,
             60,96,32,53,13,176,19,215,53,68,36,137,4,27,91,189,212,37,131,
             237,224,33,90,192,131,221,53,176,4,86,95,112,68,94,227,42,177,
             182,222,115,166,195,124,224,180,23,145,221,124,49,103,133,97,
             230,66,30,86,238,151,131,123,94,27,56,61,171,209,206,53,165,109,
             57,60,66,30,45,76,210,27,138,129,108,228,155,143,205,230,58,114,
             198,150,68,170,210,222,61,249,145,141,251,150,25,143,207,193,14,
             128,174,48,57,221,69,163,187,221,180,65,120,194,21,213,252,165,
             191,127,194,73,189,142,183,240,180,156,43,97,132,100,172,245,16,
             255,71,110,103,220,9,135,254,252,221,45,169,65,241,218,72,51,
             247,194,65,155,116,191,232,65,27,197,107,213,172,80,40,176,72,
             79,108,13,208,111,177,148,188,139,143,17,168,71,64,58,161,47,
             154,81,195,225,255,41,211,168,44,88,2,177,165,152,147,201,226,1,
             159,67,57,50,235,12,36,8,36,65,51,19,63,177,155,8,245,159,208,
             23,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,
             164,48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,
             3,85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,134,72,134,247,
             13,1,1,11,5,0,3,130,1,1,0,162,140,210,87,119,97,248,23,114,150,
             69,187,249,72,156,70,55,139,123,172,82,139,226,69,218,245,228,
             109,173,246,181,9,49,203,83,240,183,251,106,41,26,19,240,211,
             126,120,73,53,230,19,140,218,5,236,233,231,195,56,53,104,213,
             129,90,62,185,251,98,143,77,57,199,76,191,117,164,20,183,76,201,
             95,172,148,121,94,120,237,28,101,169,175,226,195,107,186,72,229,
             145,50,31,155,120,222,47,94,33,119,79,118,95,33,102,114,199,76,
             233,27,210,39,208,73,174,8,72,186,22,5,105,195,119,145,168,53,
             48,123,157,157,15,29,13,23,53,103,173,149,77,40,77,218,26,83,
             222,38,29,85,38,30,152,178,132,3,177,58,244,159,153,3,114,62,
             197,223,4,148,157,75,214,79,25,128,121,122,61,32,116,135,0,172,
             225,170,9,103,193,164,52,121,181,151,55,240,180,221,156,134,79,
             19,141,156,195,178,163,120,64,156,142,88,132,178,20,2,92,191,
             156,40,102,105,148,195,189,224,203,229,50,219,137,240,63,92,189,
             212,41,239,149,130,27,100,76,202,197,74,244,38,188,204,52,63,3,
             57>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2020-03-03T11:34:17.051+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.214.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:17.058+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-03T11:34:17.058+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-03T11:34:17.058+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-03T11:34:17.058+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-03-03T11:34:17.067+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.217.0>,menelaus_web}
             started: [{pid,<0.218.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,
                                 248,182,7,124,8,59,248,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,50,97,98,
                                 102,50,53,101,101,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,50,
                                 97,98,102,50,53,101,101,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,
                                 60,60,86,193,26,44,44,119,65,5,91,147,59,14,
                                 232,249,145,134,45,60,199,98,60,96,32,53,13,
                                 176,19,215,53,68,36,137,4,27,91,189,212,37,
                                 131,237,224,33,90,192,131,221,53,176,4,86,
                                 95,112,68,94,227,42,177,182,222,115,166,195,
                                 124,224,180,23,145,221,124,49,103,133,97,
                                 230,66,30,86,238,151,131,123,94,27,56,61,
                                 171,209,206,53,165,109,57,60,66,30,45,76,
                                 210,27,138,129,108,228,155,143,205,230,58,
                                 114,198,150,68,170,210,222,61,249,145,141,
                                 251,150,25,143,207,193,14,128,174,48,57,221,
                                 69,163,187,221,180,65,120,194,21,213,252,
                                 165,191,127,194,73,189,142,183,240,180,156,
                                 43,97,132,100,172,245,16,255,71,110,103,220,
                                 9,135,254,252,221,45,169,65,241,218,72,51,
                                 247,194,65,155,116,191,232,65,27,197,107,
                                 213,172,80,40,176,72,79,108,13,208,111,177,
                                 148,188,139,143,17,168,71,64,58,161,47,154,
                                 81,195,225,255,41,211,168,44,88,2,177,165,
                                 152,147,201,226,1,159,67,57,50,235,12,36,8,
                                 36,65,51,19,63,177,155,8,245,159,208,23,2,3,
                                 1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 162,140,210,87,119,97,248,23,114,150,69,187,
                                 249,72,156,70,55,139,123,172,82,139,226,69,
                                 218,245,228,109,173,246,181,9,49,203,83,240,
                                 183,251,106,41,26,19,240,211,126,120,73,53,
                                 230,19,140,218,5,236,233,231,195,56,53,104,
                                 213,129,90,62,185,251,98,143,77,57,199,76,
                                 191,117,164,20,183,76,201,95,172,148,121,94,
                                 120,237,28,101,169,175,226,195,107,186,72,
                                 229,145,50,31,155,120,222,47,94,33,119,79,
                                 118,95,33,102,114,199,76,233,27,210,39,208,
                                 73,174,8,72,186,22,5,105,195,119,145,168,53,
                                 48,123,157,157,15,29,13,23,53,103,173,149,
                                 77,40,77,218,26,83,222,38,29,85,38,30,152,
                                 178,132,3,177,58,244,159,153,3,114,62,197,
                                 223,4,148,157,75,214,79,25,128,121,122,61,
                                 32,116,135,0,172,225,170,9,103,193,164,52,
                                 121,181,151,55,240,180,221,156,134,79,19,
                                 141,156,195,178,163,120,64,156,142,88,132,
                                 178,20,2,92,191,156,40,102,105,148,195,189,
                                 224,203,229,50,219,137,240,63,92,189,212,41,
                                 239,149,130,27,100,76,202,197,74,244,38,188,
                                 204,52,63,3,57>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:17.067+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-03T11:34:17.068+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-03T11:34:17.068+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-03T11:34:17.068+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2020-03-03T11:34:17.068+05:30,ns_1@cb.local:<0.216.0>:restartable:start_child:98]Started child process <0.217.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2020-03-03T11:34:17.068+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.217.0>,menelaus_web}
             started: [{pid,<0.236.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,
                                 248,182,7,124,8,59,248,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,50,97,98,
                                 102,50,53,101,101,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,50,
                                 97,98,102,50,53,101,101,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,
                                 60,60,86,193,26,44,44,119,65,5,91,147,59,14,
                                 232,249,145,134,45,60,199,98,60,96,32,53,13,
                                 176,19,215,53,68,36,137,4,27,91,189,212,37,
                                 131,237,224,33,90,192,131,221,53,176,4,86,
                                 95,112,68,94,227,42,177,182,222,115,166,195,
                                 124,224,180,23,145,221,124,49,103,133,97,
                                 230,66,30,86,238,151,131,123,94,27,56,61,
                                 171,209,206,53,165,109,57,60,66,30,45,76,
                                 210,27,138,129,108,228,155,143,205,230,58,
                                 114,198,150,68,170,210,222,61,249,145,141,
                                 251,150,25,143,207,193,14,128,174,48,57,221,
                                 69,163,187,221,180,65,120,194,21,213,252,
                                 165,191,127,194,73,189,142,183,240,180,156,
                                 43,97,132,100,172,245,16,255,71,110,103,220,
                                 9,135,254,252,221,45,169,65,241,218,72,51,
                                 247,194,65,155,116,191,232,65,27,197,107,
                                 213,172,80,40,176,72,79,108,13,208,111,177,
                                 148,188,139,143,17,168,71,64,58,161,47,154,
                                 81,195,225,255,41,211,168,44,88,2,177,165,
                                 152,147,201,226,1,159,67,57,50,235,12,36,8,
                                 36,65,51,19,63,177,155,8,245,159,208,23,2,3,
                                 1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 162,140,210,87,119,97,248,23,114,150,69,187,
                                 249,72,156,70,55,139,123,172,82,139,226,69,
                                 218,245,228,109,173,246,181,9,49,203,83,240,
                                 183,251,106,41,26,19,240,211,126,120,73,53,
                                 230,19,140,218,5,236,233,231,195,56,53,104,
                                 213,129,90,62,185,251,98,143,77,57,199,76,
                                 191,117,164,20,183,76,201,95,172,148,121,94,
                                 120,237,28,101,169,175,226,195,107,186,72,
                                 229,145,50,31,155,120,222,47,94,33,119,79,
                                 118,95,33,102,114,199,76,233,27,210,39,208,
                                 73,174,8,72,186,22,5,105,195,119,145,168,53,
                                 48,123,157,157,15,29,13,23,53,103,173,149,
                                 77,40,77,218,26,83,222,38,29,85,38,30,152,
                                 178,132,3,177,58,244,159,153,3,114,62,197,
                                 223,4,148,157,75,214,79,25,128,121,122,61,
                                 32,116,135,0,172,225,170,9,103,193,164,52,
                                 121,181,151,55,240,180,221,156,134,79,19,
                                 141,156,195,178,163,120,64,156,142,88,132,
                                 178,20,2,92,191,156,40,102,105,148,195,189,
                                 224,203,229,50,219,137,240,63,92,189,212,41,
                                 239,149,130,27,100,76,202,197,74,244,38,188,
                                 204,52,63,3,57>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:17.069+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.216.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:17.069+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:17.073+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.254.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:17.074+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.257.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:17.077+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.259.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:17.078+05:30,ns_1@cb.local:users_replicator<0.259.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-03-03T11:34:17.079+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.259.0>
[ns_server:debug,2020-03-03T11:34:17.079+05:30,ns_1@cb.local:users_replicator<0.259.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.260.0>
[ns_server:debug,2020-03-03T11:34:17.079+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-03-03T11:34:17.080+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.260.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:17.080+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.258.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:34:17.083+05:30,ns_1@cb.local:compiled_roles_cache<0.262.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-03-03T11:34:17.084+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.262.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:17.085+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.265.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:17.085+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.256.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:17.085+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.268.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:17.086+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.269.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:17.093+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-03-03T11:34:17.093+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 13ms
[ns_server:debug,2020-03-03T11:34:17.095+05:30,ns_1@cb.local:users_replicator<0.259.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2020-03-03T11:34:17.095+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.272.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:17.096+05:30,ns_1@cb.local:wait_link_to_couchdb_node<0.273.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[error_logger:info,2020-03-03T11:34:17.096+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:17.096+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:34:17.096+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.685768190.636747779.137012>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:17.096+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.685768190.636747779.137012>,
                                  inet_tcp_dist,<0.276.0>,
                                  #Ref<0.685768190.636747779.137016>}
[ns_server:debug,2020-03-03T11:34:17.096+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:34:17.096+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.276.0>,shutdown}}
[error_logger:info,2020-03-03T11:34:17.097+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:17.097+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.685768190.636747779.137012>,
                               inet_tcp_dist,<0.276.0>,
                               #Ref<0.685768190.636747779.137016>}
[error_logger:info,2020-03-03T11:34:17.297+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:17.297+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:34:17.297+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.685768190.636747779.137021>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:17.297+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.685768190.636747779.137021>,
                                  inet_tcp_dist,<0.279.0>,
                                  #Ref<0.685768190.636747779.137023>}
[ns_server:debug,2020-03-03T11:34:17.325+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-03T11:34:17.526+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-03-03T11:34:17.735+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.283.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:17.783+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-03T11:34:17.803+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.685768190.636747779.137021>,
                               inet_tcp_dist,<0.279.0>,
                               #Ref<0.685768190.636747779.137023>}
[error_logger:info,2020-03-03T11:34:17.803+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.279.0>,connection_closed}}
[ns_server:info,2020-03-03T11:34:17.936+05:30,ns_1@cb.local:ns_couchdb_port<0.272.0>:ns_port_server:log:224]ns_couchdb<0.272.0>: Apache CouchDB  (LogLevel=info) is starting.
ns_couchdb<0.272.0>: Failure to start Mochiweb: eaddrinuse
ns_couchdb<0.272.0>: 4504: Booted. Waiting for shutdown request
ns_couchdb<0.272.0>: [os_mon] memory supervisor port (memsup): Erlang has closed
ns_couchdb<0.272.0>: [os_mon] cpu supervisor port (cpu_sup): Erlang has closed

[error_logger:info,2020-03-03T11:34:17.983+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:17.983+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:34:17.983+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.685768190.636747779.137029>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:17.983+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.685768190.636747779.137029>,
                                  inet_tcp_dist,<0.285.0>,
                                  #Ref<0.685768190.636747779.137033>}
[ns_server:debug,2020-03-03T11:34:17.984+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.685768190.636747779.137029>,
                               inet_tcp_dist,<0.285.0>,
                               #Ref<0.685768190.636747779.137033>}
[error_logger:info,2020-03-03T11:34:17.984+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.285.0>,shutdown}}
[ns_server:debug,2020-03-03T11:34:17.984+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:34:17.984+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:18.184+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-03T11:34:18.184+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:18.184+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.685768190.636747778.136692>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:18.184+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.685768190.636747778.136692>,
                                  inet_tcp_dist,<0.288.0>,
                                  #Ref<0.685768190.636747778.136696>}
[ns_server:debug,2020-03-03T11:34:18.184+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-03-03T11:34:18.184+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.685768190.636747778.136692>,
                               inet_tcp_dist,<0.288.0>,
                               #Ref<0.685768190.636747778.136696>}
[error_logger:info,2020-03-03T11:34:18.184+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.288.0>,shutdown}}
[error_logger:info,2020-03-03T11:34:18.184+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-03-03T11:34:18.385+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:18.385+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:34:18.385+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.685768190.636747779.137039>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:18.385+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.685768190.636747779.137039>,
                                  inet_tcp_dist,<0.291.0>,
                                  #Ref<0.685768190.636747779.137043>}
[ns_server:debug,2020-03-03T11:34:18.385+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.685768190.636747779.137039>,
                               inet_tcp_dist,<0.291.0>,
                               #Ref<0.685768190.636747779.137043>}
[error_logger:info,2020-03-03T11:34:18.385+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.291.0>,shutdown}}
[ns_server:debug,2020-03-03T11:34:18.385+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:34:18.385+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:info,2020-03-03T11:34:18.421+05:30,ns_1@cb.local:ns_couchdb_port<0.272.0>:ns_port_server:log:224]ns_couchdb<0.272.0>: {"Kernel pid terminated",application_controller,"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\"},{line,102}]},{supervisor,do_start_child,2,[{file,\"supervisor.erl\"},{line,365}]},{supervisor,start_children,3,[{file,\"supervisor.erl\"},{line,348}]},{supervisor,init_children,2,[{file,\"supervisor.erl\"},{line,314}]},{gen_server,init_it,2,[{file,\"gen_server.erl\"},{line,365}]},{gen_server,init_it,6,[{file,\"gen_server.erl\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}"}
ns_couchdb<0.272.0>: Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro
ns_couchdb<0.272.0>: 
ns_couchdb<0.272.0>: Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done

[ns_server:error,2020-03-03T11:34:18.423+05:30,ns_1@cb.local:wait_link_to_couchdb_node<0.273.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:189]ns_couchdb_port(<0.272.0>) died with reason {abnormal,1}
[ns_server:debug,2020-03-03T11:34:18.423+05:30,ns_1@cb.local:<0.267.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.265.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:18.423+05:30,ns_1@cb.local:<0.266.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.265.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:18.423+05:30,ns_1@cb.local:<0.264.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.262.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:18.423+05:30,ns_1@cb.local:<0.263.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.262.0>} exited with reason shutdown
[error_logger:error,2020-03-03T11:34:18.422+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]** Generic server ns_couchdb_port terminating 
** Last message in was {#Port<0.5096>,{exit_status,1}}
** When Server state == {state,#Port<0.5096>,
                            {ns_couchdb,"/opt/couchbase/lib/erlang/bin/erl",
                                ["-pa",
                                 "/opt/couchbase/lib/erlang/lib/asn1-5.0.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/compiler-7.1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEvent-2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEventDomain-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosFileTransfer-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosNotification-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosProperty-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTime-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTransactions-1.3.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/crypto-4.2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/dialyzer-3.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/diameter-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/edoc-0.9.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eldap-1.2.3.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_docgen-0.7.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_interface-3.10.2.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erts-9.3.3.9/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eunit-2.3.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/hipe-3.17.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ic-4.4.4.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/inets-6.5.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/mnesia-4.15.3.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/orber-3.8.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/os_mon-2.4.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/otp_mibs-1.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/parsetools-2.1.6/ebin",
                                 "/opt/couchbase/lib/erlang/lib/public_key-1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/reltool-0.7.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/runtime_tools-1.12.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/sasl-3.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/snmp-5.2.11/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssh-4.6.9.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssl-8.2.6.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/syntax_tools-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/tools-2.11.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/xmerl-1.3.16.1/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/gc-couchbase-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/vtree-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/wkb-1.2.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch-1.2.0a-961ad59-git/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_audit-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_dcp-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_index_merger-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_set_view-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_view_parser-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/ejson-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/erlang-oauth/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/etap/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/lhttpc-1.3/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mapreduce-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mochiweb-1.4.1/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/snappy-1.0.4/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ale/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/gen_smtp/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_babysitter/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_couchdb/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/ebin",
                                 "/opt/couchbase/lib/erlang/lib/stdlib-3.4.5.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/kernel-5.4.3.2/ebin",
                                 ".","-couch_ini",
                                 "/opt/couchbase/etc/couchdb/default.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/capi.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/geocouch.ini",
                                 "/opt/couchbase/etc/couchdb/local.ini",
                                 "-kernel","error_logger","false","-kernel",
                                 "error_logger","false","inetrc",
                                 "\"/opt/couchbase/etc/couchbase/hosts.cfg\"",
                                 "dist_config_file",
                                 "\"/opt/couchbase/var/lib/couchbase/config/dist_cfg\"",
                                 "-ssl_dist_optfile",
                                 "/opt/couchbase/etc/couchbase/ssl_dist_opts",
                                 "-setcookie",
                                 "dce5392bcba7669ee9f057b78581e574cccf9efc1961f2376ff32b0a61220948",
                                 "-name","couchdb_ns_1@cb.local","-smp",
                                 "enable","+P","327680","+K","true","-kernel",
                                 "error_logger","false","-sasl",
                                 "sasl_error_logger","false","-nouser",
                                 "-hidden","-proto_dist","cb","-epmd_module",
                                 "cb_epmd","-start_epmd","false","-run",
                                 "child_erlang","child_start","ns_couchdb"],
                                [use_stdio,
                                 {env,
                                     [{"NS_COUCHDB_ENV_ARGS",
                                       "[{ns_server_node,'ns_1@cb.local'},\n {path_config_tmpdir,\"/opt/couchbase/var/lib/couchbase/tmp\"},\n {net_kernel_verbosity,10},\n {loglevel_error_logger,debug},\n {path_config_libdir,\"/opt/couchbase/lib\"},\n {loglevel_stats,debug},\n {loglevel_menelaus,debug},\n {path_config_secdir,\"/opt/couchbase/etc/security\"},\n {loglevel_user,debug},\n {path_config_etcdir,\"/opt/couchbase/etc/couchbase\"},\n {loglevel_ns_server,debug},\n {loglevel_mapreduce_errors,debug},\n {loglevel_rebalance,debug},\n {loglevel_default,debug},\n {disk_sink_opts,[{rotation,[{compress,true},\n                             {size,41943040},\n                             {num_files,10},\n                             {buffer_size_max,52428800}]}]},\n {loglevel_cbas,debug},\n {loglevel_xdcr,debug},\n {loglevel_ns_doctor,debug},\n {loglevel_access,info},\n {error_logger_mf_dir,\"/opt/couchbase/var/lib/couchbase/logs\"},\n {path_config_datadir,\"/opt/couchbase/var/lib/couchbase\"},\n {loglevel_cluster,debug},\n {loglevel_couchdb,info},\n {loglevel_views,debug},\n {path_config_bindir,\"/opt/couchbase/bin\"}]"},
                                      {"ERL_CRASH_DUMP",
                                       "erl_crash.dump.1583215409.3666.ns_couchdb"}]}]},
                            {ringbuffer,1190,1024,
                                {[{<<"Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done">>,
                                   80},
                                  {<<>>,0},
                                  {<<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                                   200}],
                                 [{<<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>,
                                   910}]}},
                            undefined,
                            {ok,{-576460749721,
                                 #Ref<0.685768190.636747778.136699>}},
                            [<<"Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done">>,
                             <<>>,
                             <<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                             <<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>],
                            0}
** Reason for termination == 
** {abnormal,1}

[ns_server:debug,2020-03-03T11:34:18.423+05:30,ns_1@cb.local:<0.255.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.254.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:18.423+05:30,ns_1@cb.local:<0.216.0>:restartable:shutdown_child:120]Successfully terminated process <0.217.0>
[ns_server:debug,2020-03-03T11:34:18.423+05:30,ns_1@cb.local:<0.215.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.214.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:18.423+05:30,ns_1@cb.local:<0.201.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.200.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:18.423+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:wait_saver:866]Done waiting for saver.
[error_logger:error,2020-03-03T11:34:18.425+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_port_server:init/1
    pid: <0.272.0>
    registered_name: ns_couchdb_port
    exception exit: {abnormal,1}
      in function  gen_server:handle_common_reply/8 (gen_server.erl, line 726)
    ancestors: [ns_server_nodes_sup,<0.206.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 1
    messages: [{'EXIT',#Port<0.5096>,normal}]
    links: [<0.207.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 11890
  neighbours:

[error_logger:error,2020-03-03T11:34:18.425+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: erlang:apply/2
    pid: <0.273.0>
    registered_name: wait_link_to_couchdb_node
    exception exit: {abnormal,1}
      in function  ns_server_nodes_sup:do_wait_link_to_couchdb_node/1 (src/ns_server_nodes_sup.erl, line 190)
    ancestors: [ns_server_nodes_sup,<0.206.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.207.0>,<0.274.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 987
    stack_size: 27
    reductions: 3382
  neighbours:
    neighbour:
      pid: <0.274.0>
      registered_name: []
      initial call: ns_server_nodes_sup:'-do_wait_link_to_couchdb_node/1-fun-2-'/0
      current_function: {timer,sleep,1}
      ancestors: [wait_link_to_couchdb_node,ns_server_nodes_sup,<0.206.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
      message_queue_len: 0
      links: [<0.273.0>]
      trap_exit: false
      status: waiting
      heap_size: 2586
      stack_size: 12
      reductions: 10907
      current_stacktrace: [{timer,sleep,1,[{file,"timer.erl"},{line,153}]},
                  {misc,poll_for_condition_rec,3,
                      [{file,"src/misc.erl"},{line,508}]},
                  {ns_server_nodes_sup,
                      '-do_wait_link_to_couchdb_node/1-fun-2-',2,
                      [{file,"src/ns_server_nodes_sup.erl"},{line,159}]},
                  {proc_lib,init_p,3,[{file,"proc_lib.erl"},{line,232}]}]

[error_logger:error,2020-03-03T11:34:18.425+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    start_error
     Reason:     {abnormal,1}
     Offender:   [{pid,undefined},
                  {name,wait_for_couchdb_node},
                  {mfargs,{erlang,apply,
                                  [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:error,2020-03-03T11:34:18.426+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    shutdown_error
     Reason:     {abnormal,1}
     Offender:   [{pid,<0.272.0>},
                  {name,start_couchdb_node},
                  {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                  {restart_type,{permanent,5}},
                  {shutdown,86400000},
                  {child_type,worker}]


[error_logger:error,2020-03-03T11:34:18.426+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_cluster_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,wait_for_couchdb_node,
                         {abnormal,1}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_nodes_sup},
                  {mfargs,
                      {restartable,start_link,
                          [{ns_server_nodes_sup,start_link,[]},infinity]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-03-03T11:34:18.426+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,root_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                             {failed_to_start_child,wait_for_couchdb_node,
                                 {abnormal,1}}}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_cluster_sup},
                  {mfargs,{ns_server_cluster_sup,start_link,[]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-03-03T11:34:18.426+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: application_master:init/4
    pid: <0.117.0>
    registered_name: []
    exception exit: {{shutdown,
                      {failed_to_start_child,ns_server_cluster_sup,
                       {shutdown,
                        {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                          {failed_to_start_child,wait_for_couchdb_node,
                           {abnormal,1}}}}}}},
                     {ns_server,start,[normal,[]]}}
      in function  application_master:init/4 (application_master.erl, line 134)
    ancestors: [<0.116.0>]
    message_queue_len: 1
    messages: [{'EXIT',<0.118.0>,normal}]
    links: [<0.116.0>,<0.33.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 274
  neighbours:

[error_logger:info,2020-03-03T11:34:18.426+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
         application: ns_server
              exited: {{shutdown,
                        {failed_to_start_child,ns_server_cluster_sup,
                         {shutdown,
                          {failed_to_start_child,ns_server_nodes_sup,
                           {shutdown,
                            {failed_to_start_child,wait_for_couchdb_node,
                             {abnormal,1}}}}}}},
                       {ns_server,start,[normal,[]]}}
                type: permanent

[error_logger:info,2020-03-03T11:34:18.426+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/8689"}}

[error_logger:info,2020-03-03T11:34:18.426+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/4486"}}

[error_logger:info,2020-03-03T11:34:18.426+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-calculator/154"}}

[error_logger:info,2020-03-03T11:34:18.426+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-logs/25"}}

[error_logger:info,2020-03-03T11:34:18.426+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/59"}}

[error_logger:info,2020-03-03T11:34:18.427+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gnome-system-monitor/36"}}

[error_logger:info,2020-03-03T11:34:18.427+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/98"}}

[error_logger:info,2020-03-03T11:34:18.427+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-characters/69"}}

[ns_server:info,2020-03-03T11:34:26.106+05:30,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-03-03T11:34:26.118+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-03-03T11:34:26.119+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-03-03T11:34:26.123+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-03-03T11:34:26.123+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-03T11:34:26.124+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2020-03-03T11:34:26.132+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.133+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.133+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.169.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:26.135+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.137+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.174.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:26.137+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.175.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:26.137+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:26.137+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.168.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:34:26.138+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2020-03-03T11:34:26.139+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.176.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.139+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.177.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.140+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:26.143+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-03-03T11:34:26.143+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-03-03T11:34:26.143+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2020-03-03T11:34:26.145+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-03-03T11:34:26.145+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.145+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.167.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-03-03T11:34:26.155+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-03-03T11:34:26.158+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-03-03T11:34:26.158+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[ns_server:debug,2020-03-03T11:34:26.158+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-03T11:34:26.158+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:26.158+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.104293103.2785017857.81800>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:26.158+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.104293103.2785017857.81800>,
                                  inet_tcp_dist,<0.183.0>,
                                  #Ref<0.104293103.2785017857.81805>}
[ns_server:debug,2020-03-03T11:34:26.160+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-03-03T11:34:26.161+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2020-03-03T11:34:26.161+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-03T11:34:26.162+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-03-03T11:34:26.162+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.168+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.186.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:26.170+05:30,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,3,3},{11,34,26}}},
               {memory,
                   [{total,26408400},
                    {processes,9630424},
                    {processes_used,9625184},
                    {system,16777976},
                    {atom,388625},
                    {atom_used,364408},
                    {binary,105480},
                    {code,8250921},
                    {ets,1504264}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    kernel,proc_lib,supervisor,application_master,code,lists,
                    application_controller,code_server,error_logger,
                    file_server,erl_eval,error_handler,file_io_server,file,
                    application,heart,gen_server,filename,erl_lint,gen_event,
                    gen,erl_parse,ets,erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{os_mon,"CPO  CXC 138 46","2.4.4"},
                    {sasl,"SASL  CXC 138 11","3.1.2"},
                    {ns_server,"Couchbase server","6.5.0-4960-enterprise"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ale,"Another Logger for Erlang","0.0.0"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,129},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,auth,httpd_sup,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    kernel_safe_sup,release_handler,lhttpc_sup,httpc_sup,
                    lhttpc_manager,alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    'sink-disk_xdcr',timer_server,'sink-disk_debug',ale_sup,
                    'sink-disk_error',inet_db,'sink-disk_default',
                    ssl_pem_cache_dist,ale_dynamic_sup,rex,global_group,
                    net_sup,kernel_sup,ssl_connection_sup,global_name_server,
                    ssl_admin_sup,tftp_sup,ssl_sup,root_sup,erts_code_purger,
                    os_mon_sup,file_server_2,error_logger,cpu_sup,erl_epmd,
                    init,memsup,erl_signal_server,net_kernel,disksup,ale,
                    dist_manager,ssl_pem_cache,ssl_manager,ssl_dist_admin_sup,
                    ssl_dist_connection_sup,ssl_dist_sup,user,
                    ssl_tls_dist_proxy,ssl_manager_dist,sasl_safe_sup,
                    ssl_listen_tracker_sup,code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,1}]
[ns_server:info,2020-03-03T11:34:26.174+05:30,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"cfa0f75f28402d2e1aa254b2a374bead19433526\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" upstream=\"master\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"f2a16b53bb74146f20d18ba2c0443d5f10a9a550\" upstream=\"master\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4960\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" upstream=\"master\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" upstream=\"mad-hatter\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" upstream=\"master\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" upstream=\"master\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" upstream=\"master\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" upstream=\"mad-hatter\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" upstream=\"cb-master\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" upstream=\"master\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" upstream=\"master\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" upstream=\"mad-hatter\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" upstream=\"mad-hatter\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" upstream=\"mad-hatter\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" upstream=\"master\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" upstream=\"master\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" upstream=\"mad-hatter\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" upstream=\"mad-hatter\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" upstream=\"mad-hatter\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" upstream=\"mad-hatter\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" upstream=\"master\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" upstream=\"master\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" upstream=\"master\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" upstream=\"mad-hatter\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" upstream=\"master\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"7a3aa547b3f5eb3ea28d279a08384609cd2cea7c\" upstream=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" upstream=\"mad-hatter\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" upstream=\"master\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" upstream=\"master\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" upstream=\"master\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" upstream=\"v1.1.0-couchbase\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"ee64d41320d14fabe814a241a5cf4f6a6f6e827a\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" upstream=\"master\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" upstream=\"master\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-03-03T11:34:26.177+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.187.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.178+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.178+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:26.179+05:30,ns_1@cb.local:ns_config_sup<0.190.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-03-03T11:34:26.179+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.179+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:26.199+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-03-03T11:34:26.199+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2020-03-03T11:34:26.209+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[{alert_limits,
   [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
  {audit,
   [{auditd_enabled,false},
    {rotate_interval,86400},
    {rotate_size,20971520},
    {disabled,[]},
    {sync,[]},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
  {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
  {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
  {autocompaction,
   [{database_fragmentation_threshold,{30,undefined}},
    {view_fragmentation_threshold,{30,undefined}}]},
  {buckets,[{configs,[]}]},
  {cbas_memory_quota,2174},
  {cert_and_pkey,
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFfi2B3wIO/gwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAyYWJmMjVlZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMmFiZjI1\nZWUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDI7xEpYzw8VsEaLCx3\nQQVbkzsO6PmRhi08x2I8YCA1DbAT1zVEJIkEG1u91CWD7eAhWsCD3TWwBFZfcERe\n4yqxtt5zpsN84LQXkd18MWeFYeZCHlbul4N7Xhs4PavRzjWlbTk8Qh4tTNIbioFs\n5JuPzeY6csaWRKrS3j35kY37lhmPz8EOgK4wOd1Fo7vdtEF4whXV/KW/f8JJvY63\n8LScK2GEZKz1EP9HbmfcCYf+/N0tqUHx2kgz98JBm3S/6EEbxWvVrFAosEhPbA3Q\nb7GUvIuPEahHQDqhL5pRw+H/KdOoLFgCsaWYk8niAZ9DOTLrDCQIJEEzEz+xmwj1\nn9AXAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCijNJXd2H4F3KW\nRbv5SJxGN4t7rFKL4kXa9eRtrfa1CTHLU/C3+2opGhPw0354STXmE4zaBezp58M4\nNWjVgVo+uftij005x0y/daQUt0zJX6yUeV547Rxlqa/iw2u6SOWRMh+beN4vXiF3\nT3ZfIWZyx0zpG9In0EmuCEi6FgVpw3eRqDUwe52dDx0NFzVnrZVNKE3aGlPeJh1V\nJh6YsoQDsTr0n5kDcj7F3wSUnUvWTxmAeXo9IHSHAKzhqglnwaQ0ebWXN/C03ZyG\nTxONnMOyo3hAnI5YhLIUAly/nChmaZTDveDL5TLbifA/XL3UKe+VghtkTMrFSvQm\nvMw0PwM5\n-----END CERTIFICATE-----\n">>,
     <<"*****">>}]},
  {drop_request_memory_threshold_mib,undefined},
  {email_alerts,
   [{recipients,["root@localhost"]},
    {sender,"couchbase@localhost"},
    {enabled,false},
    {email_server,
     [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
    {alerts,
     [auto_failover_node,auto_failover_maximum_reached,
      auto_failover_other_nodes_down,auto_failover_cluster_too_small,
      auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
      ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
      ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
  {fts_memory_quota,512},
  {index_aware_rebalance_disabled,false},
  {log_redaction_default_cfg,[{redact_level,none}]},
  {max_bucket_count,30},
  {memcached,[]},
  {memory_quota,8886},
  {nodes_wanted,['ns_1@cb.local']},
  {password_policy,[{min_length,6},{must_present,[]}]},
  {quorum_nodes,['ns_1@cb.local']},
  {remote_clusters,[]},
  {replication,[{enabled,true}]},
  {rest,[{port,8091}]},
  {rest_creds,null},
  {secure_headers,[]},
  {server_groups,
   [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
  {set_view_update_daemon,
   [{update_interval,5000},
    {update_min_changes,5000},
    {replica_update_min_changes,5000}]},
  {{couchdb,max_parallel_indexers},4},
  {{couchdb,max_parallel_replica_indexers},2},
  {{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{7,63750434657}}]}]},
  {{metakv,<<"/indexing/settings/config">>},
   <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
  {{request_limit,capi},undefined},
  {{request_limit,rest},undefined},
  {{node,'ns_1@cb.local',address_family},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    inet]},
  {{node,'ns_1@cb.local',audit},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
  {{node,'ns_1@cb.local',capi_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8092]},
  {{node,'ns_1@cb.local',cbas_admin_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9110]},
  {{node,'ns_1@cb.local',cbas_cc_client_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9113]},
  {{node,'ns_1@cb.local',cbas_cc_cluster_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9112]},
  {{node,'ns_1@cb.local',cbas_cc_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9111]},
  {{node,'ns_1@cb.local',cbas_cluster_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9115]},
  {{node,'ns_1@cb.local',cbas_console_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9114]},
  {{node,'ns_1@cb.local',cbas_data_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9116]},
  {{node,'ns_1@cb.local',cbas_debug_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    -1]},
  {{node,'ns_1@cb.local',cbas_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8095]},
  {{node,'ns_1@cb.local',cbas_messaging_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9118]},
  {{node,'ns_1@cb.local',cbas_metadata_callback_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9119]},
  {{node,'ns_1@cb.local',cbas_metadata_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9121]},
  {{node,'ns_1@cb.local',cbas_parent_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9122]},
  {{node,'ns_1@cb.local',cbas_replication_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9120]},
  {{node,'ns_1@cb.local',cbas_result_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9117]},
  {{node,'ns_1@cb.local',cbas_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18095]},
  {{node,'ns_1@cb.local',compaction_daemon},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@cb.local',config_version},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {6,5}]},
  {{node,'ns_1@cb.local',erl_external_listeners},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {inet,false},
    {inet6,false}]},
  {{node,'ns_1@cb.local',eventing_debug_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9140]},
  {{node,'ns_1@cb.local',eventing_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8096]},
  {{node,'ns_1@cb.local',eventing_https_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18096]},
  {{node,'ns_1@cb.local',fts_grpc_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9130]},
  {{node,'ns_1@cb.local',fts_grpc_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    19130]},
  {{node,'ns_1@cb.local',fts_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8094]},
  {{node,'ns_1@cb.local',fts_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18094]},
  {{node,'ns_1@cb.local',indexer_admin_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9100]},
  {{node,'ns_1@cb.local',indexer_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9102]},
  {{node,'ns_1@cb.local',indexer_https_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    19102]},
  {{node,'ns_1@cb.local',indexer_scan_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9101]},
  {{node,'ns_1@cb.local',indexer_stcatchup_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9104]},
  {{node,'ns_1@cb.local',indexer_stinit_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9103]},
  {{node,'ns_1@cb.local',indexer_stmaint_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9105]},
  {{node,'ns_1@cb.local',is_enterprise},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    true]},
  {{node,'ns_1@cb.local',isasl},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@cb.local',membership},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    active]},
  {{node,'ns_1@cb.local',memcached},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,11210},
    {dedicated_port,11209},
    {dedicated_ssl_port,11206},
    {ssl_port,11207},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_sleeptime,19},
    {log_rotation_period,39003}]},
  {{node,'ns_1@cb.local',memcached_config},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {[{interfaces,
       {memcached_config_mgr,omit_missing_mcd_ports,
        [{[{host,<<"*">>},
           {port,port},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_port},
           {system,true},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,ssl_port},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_ssl_port},
           {system,true},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
      {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
      {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {opentracing,
       {[{enabled,opentracing_enabled},
         {module,{"~s",[opentracing_module]}},
         {config,{"~s",[opentracing_config]}}]}},
      {admin,{"~s",[admin_user]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize},
         {sleeptime,log_sleeptime}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
  {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    11206]},
  {{node,'ns_1@cb.local',memcached_defaults},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {max_connections,65000},
    {system_connections,5000},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {opentracing_enabled,false},
    {opentracing_module,[]},
    {opentracing_config,[]},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {tracing_enabled,true},
    {datatype_snappy,true},
    {num_reader_threads,<<"default">>},
    {num_writer_threads,<<"default">>}]},
  {{node,'ns_1@cb.local',moxi},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,0}]},
  {{node,'ns_1@cb.local',node_encryption},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    false]},
  {{node,'ns_1@cb.local',ns_log},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@cb.local',port_servers},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
  {{node,'ns_1@cb.local',projector_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9999]},
  {{node,'ns_1@cb.local',projector_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9999]},
  {{node,'ns_1@cb.local',query_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8093]},
  {{node,'ns_1@cb.local',rest},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@cb.local',saslauthd_enabled},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    true]},
  {{node,'ns_1@cb.local',ssl_capi_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18092]},
  {{node,'ns_1@cb.local',ssl_query_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18093]},
  {{node,'ns_1@cb.local',ssl_rest_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18091]},
  {{node,'ns_1@cb.local',uuid},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    <<"e32a4d3bd8aa759a4b96cd6ac25889ee">>]},
  {{node,'ns_1@cb.local',xdcr_rest_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9998]},
  {{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    false]}]]
[ns_server:info,2020-03-03T11:34:26.212+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   false]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9998]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   <<"e32a4d3bd8aa759a4b96cd6ac25889ee">>]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18091]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18093]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18092]},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   true]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9999]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',node_encryption},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   false]},
 {{node,'ns_1@cb.local',moxi},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,0}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   11206]},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',membership},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   active]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   true]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9100]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8094]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9130]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8096]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9140]},
 {{node,'ns_1@cb.local',erl_external_listeners},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {inet,false},
   {inet6,false}]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {6,5}]},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8095]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9110]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8092]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
 {{node,'ns_1@cb.local',address_family},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   inet]},
 {{request_limit,rest},undefined},
 {{request_limit,capi},undefined},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{7,63750434657}}]}]},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
 {secure_headers,[]},
 {rest_creds,null},
 {rest,[{port,8091}]},
 {replication,[{enabled,true}]},
 {remote_clusters,[]},
 {quorum_nodes,['ns_1@cb.local']},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {nodes_wanted,['ns_1@cb.local']},
 {memory_quota,8886},
 {memcached,[]},
 {max_bucket_count,30},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {index_aware_rebalance_disabled,false},
 {fts_memory_quota,512},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {drop_request_memory_threshold_mib,undefined},
 {cert_and_pkey,
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFfi2B3wIO/gwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAyYWJmMjVlZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMmFiZjI1\nZWUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDI7xEpYzw8VsEaLCx3\nQQVbkzsO6PmRhi08x2I8YCA1DbAT1zVEJIkEG1u91CWD7eAhWsCD3TWwBFZfcERe\n4yqxtt5zpsN84LQXkd18MWeFYeZCHlbul4N7Xhs4PavRzjWlbTk8Qh4tTNIbioFs\n5JuPzeY6csaWRKrS3j35kY37lhmPz8EOgK4wOd1Fo7vdtEF4whXV/KW/f8JJvY63\n8LScK2GEZKz1EP9HbmfcCYf+/N0tqUHx2kgz98JBm3S/6EEbxWvVrFAosEhPbA3Q\nb7GUvIuPEahHQDqhL5pRw+H/KdOoLFgCsaWYk8niAZ9DOTLrDCQIJEEzEz+xmwj1\nn9AXAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCijNJXd2H4F3KW\nRbv5SJxGN4t7rFKL4kXa9eRtrfa1CTHLU/C3+2opGhPw0354STXmE4zaBezp58M4\nNWjVgVo+uftij005x0y/daQUt0zJX6yUeV547Rxlqa/iw2u6SOWRMh+beN4vXiF3\nT3ZfIWZyx0zpG9In0EmuCEi6FgVpw3eRqDUwe52dDx0NFzVnrZVNKE3aGlPeJh1V\nJh6YsoQDsTr0n5kDcj7F3wSUnUvWTxmAeXo9IHSHAKzhqglnwaQ0ebWXN/C03ZyG\nTxONnMOyo3hAnI5YhLIUAly/nChmaZTDveDL5TLbifA/XL3UKe+VghtkTMrFSvQm\nvMw0PwM5\n-----END CERTIFICATE-----\n">>,
    <<"*****">>}]},
 {cbas_memory_quota,2174},
 {buckets,[{configs,[]}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]}]
[error_logger:info,2020-03-03T11:34:26.214+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.216+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.199.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.216+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.200.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.217+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:34:26.218+05:30,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>} ->
[{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{8,63750434666}}]}]
[error_logger:info,2020-03-03T11:34:26.218+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.202.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:26.219+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.205.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:26.224+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.208.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:26.224+05:30,ns_1@cb.local:menelaus_barrier<0.209.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-03-03T11:34:26.224+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.209.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.225+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.226+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.227+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.213.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:26.236+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2020-03-03T11:34:26.236+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2020-03-03T11:34:26.236+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2020-03-03T11:34:26.252+05:30,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:init:462]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,248,182,7,124,8,59,
             248,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
             6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
             118,101,114,32,50,97,98,102,50,53,101,101,48,30,23,13,49,51,48,
             49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
             57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
             104,98,97,115,101,32,83,101,114,118,101,114,32,50,97,98,102,50,
             53,101,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,
             3,130,1,15,0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,60,60,86,
             193,26,44,44,119,65,5,91,147,59,14,232,249,145,134,45,60,199,98,
             60,96,32,53,13,176,19,215,53,68,36,137,4,27,91,189,212,37,131,
             237,224,33,90,192,131,221,53,176,4,86,95,112,68,94,227,42,177,
             182,222,115,166,195,124,224,180,23,145,221,124,49,103,133,97,
             230,66,30,86,238,151,131,123,94,27,56,61,171,209,206,53,165,109,
             57,60,66,30,45,76,210,27,138,129,108,228,155,143,205,230,58,114,
             198,150,68,170,210,222,61,249,145,141,251,150,25,143,207,193,14,
             128,174,48,57,221,69,163,187,221,180,65,120,194,21,213,252,165,
             191,127,194,73,189,142,183,240,180,156,43,97,132,100,172,245,16,
             255,71,110,103,220,9,135,254,252,221,45,169,65,241,218,72,51,
             247,194,65,155,116,191,232,65,27,197,107,213,172,80,40,176,72,
             79,108,13,208,111,177,148,188,139,143,17,168,71,64,58,161,47,
             154,81,195,225,255,41,211,168,44,88,2,177,165,152,147,201,226,1,
             159,67,57,50,235,12,36,8,36,65,51,19,63,177,155,8,245,159,208,
             23,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,
             164,48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,
             3,85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,134,72,134,247,
             13,1,1,11,5,0,3,130,1,1,0,162,140,210,87,119,97,248,23,114,150,
             69,187,249,72,156,70,55,139,123,172,82,139,226,69,218,245,228,
             109,173,246,181,9,49,203,83,240,183,251,106,41,26,19,240,211,
             126,120,73,53,230,19,140,218,5,236,233,231,195,56,53,104,213,
             129,90,62,185,251,98,143,77,57,199,76,191,117,164,20,183,76,201,
             95,172,148,121,94,120,237,28,101,169,175,226,195,107,186,72,229,
             145,50,31,155,120,222,47,94,33,119,79,118,95,33,102,114,199,76,
             233,27,210,39,208,73,174,8,72,186,22,5,105,195,119,145,168,53,
             48,123,157,157,15,29,13,23,53,103,173,149,77,40,77,218,26,83,
             222,38,29,85,38,30,152,178,132,3,177,58,244,159,153,3,114,62,
             197,223,4,148,157,75,214,79,25,128,121,122,61,32,116,135,0,172,
             225,170,9,103,193,164,52,121,181,151,55,240,180,221,156,134,79,
             19,141,156,195,178,163,120,64,156,142,88,132,178,20,2,92,191,
             156,40,102,105,148,195,189,224,203,229,50,219,137,240,63,92,189,
             212,41,239,149,130,27,100,76,202,197,74,244,38,188,204,52,63,3,
             57>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2020-03-03T11:34:26.253+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.214.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:26.269+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-03T11:34:26.269+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-03T11:34:26.270+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-03T11:34:26.270+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:info,2020-03-03T11:34:26.286+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-03T11:34:26.286+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-03T11:34:26.286+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-03T11:34:26.286+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-03-03T11:34:26.285+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.217.0>,menelaus_web}
             started: [{pid,<0.218.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,
                                 248,182,7,124,8,59,248,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,50,97,98,
                                 102,50,53,101,101,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,50,
                                 97,98,102,50,53,101,101,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,
                                 60,60,86,193,26,44,44,119,65,5,91,147,59,14,
                                 232,249,145,134,45,60,199,98,60,96,32,53,13,
                                 176,19,215,53,68,36,137,4,27,91,189,212,37,
                                 131,237,224,33,90,192,131,221,53,176,4,86,
                                 95,112,68,94,227,42,177,182,222,115,166,195,
                                 124,224,180,23,145,221,124,49,103,133,97,
                                 230,66,30,86,238,151,131,123,94,27,56,61,
                                 171,209,206,53,165,109,57,60,66,30,45,76,
                                 210,27,138,129,108,228,155,143,205,230,58,
                                 114,198,150,68,170,210,222,61,249,145,141,
                                 251,150,25,143,207,193,14,128,174,48,57,221,
                                 69,163,187,221,180,65,120,194,21,213,252,
                                 165,191,127,194,73,189,142,183,240,180,156,
                                 43,97,132,100,172,245,16,255,71,110,103,220,
                                 9,135,254,252,221,45,169,65,241,218,72,51,
                                 247,194,65,155,116,191,232,65,27,197,107,
                                 213,172,80,40,176,72,79,108,13,208,111,177,
                                 148,188,139,143,17,168,71,64,58,161,47,154,
                                 81,195,225,255,41,211,168,44,88,2,177,165,
                                 152,147,201,226,1,159,67,57,50,235,12,36,8,
                                 36,65,51,19,63,177,155,8,245,159,208,23,2,3,
                                 1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 162,140,210,87,119,97,248,23,114,150,69,187,
                                 249,72,156,70,55,139,123,172,82,139,226,69,
                                 218,245,228,109,173,246,181,9,49,203,83,240,
                                 183,251,106,41,26,19,240,211,126,120,73,53,
                                 230,19,140,218,5,236,233,231,195,56,53,104,
                                 213,129,90,62,185,251,98,143,77,57,199,76,
                                 191,117,164,20,183,76,201,95,172,148,121,94,
                                 120,237,28,101,169,175,226,195,107,186,72,
                                 229,145,50,31,155,120,222,47,94,33,119,79,
                                 118,95,33,102,114,199,76,233,27,210,39,208,
                                 73,174,8,72,186,22,5,105,195,119,145,168,53,
                                 48,123,157,157,15,29,13,23,53,103,173,149,
                                 77,40,77,218,26,83,222,38,29,85,38,30,152,
                                 178,132,3,177,58,244,159,153,3,114,62,197,
                                 223,4,148,157,75,214,79,25,128,121,122,61,
                                 32,116,135,0,172,225,170,9,103,193,164,52,
                                 121,181,151,55,240,180,221,156,134,79,19,
                                 141,156,195,178,163,120,64,156,142,88,132,
                                 178,20,2,92,191,156,40,102,105,148,195,189,
                                 224,203,229,50,219,137,240,63,92,189,212,41,
                                 239,149,130,27,100,76,202,197,74,244,38,188,
                                 204,52,63,3,57>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:26.287+05:30,ns_1@cb.local:<0.216.0>:restartable:start_child:98]Started child process <0.217.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2020-03-03T11:34:26.287+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.217.0>,menelaus_web}
             started: [{pid,<0.236.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,
                                 248,182,7,124,8,59,248,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,50,97,98,
                                 102,50,53,101,101,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,50,
                                 97,98,102,50,53,101,101,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,
                                 60,60,86,193,26,44,44,119,65,5,91,147,59,14,
                                 232,249,145,134,45,60,199,98,60,96,32,53,13,
                                 176,19,215,53,68,36,137,4,27,91,189,212,37,
                                 131,237,224,33,90,192,131,221,53,176,4,86,
                                 95,112,68,94,227,42,177,182,222,115,166,195,
                                 124,224,180,23,145,221,124,49,103,133,97,
                                 230,66,30,86,238,151,131,123,94,27,56,61,
                                 171,209,206,53,165,109,57,60,66,30,45,76,
                                 210,27,138,129,108,228,155,143,205,230,58,
                                 114,198,150,68,170,210,222,61,249,145,141,
                                 251,150,25,143,207,193,14,128,174,48,57,221,
                                 69,163,187,221,180,65,120,194,21,213,252,
                                 165,191,127,194,73,189,142,183,240,180,156,
                                 43,97,132,100,172,245,16,255,71,110,103,220,
                                 9,135,254,252,221,45,169,65,241,218,72,51,
                                 247,194,65,155,116,191,232,65,27,197,107,
                                 213,172,80,40,176,72,79,108,13,208,111,177,
                                 148,188,139,143,17,168,71,64,58,161,47,154,
                                 81,195,225,255,41,211,168,44,88,2,177,165,
                                 152,147,201,226,1,159,67,57,50,235,12,36,8,
                                 36,65,51,19,63,177,155,8,245,159,208,23,2,3,
                                 1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 162,140,210,87,119,97,248,23,114,150,69,187,
                                 249,72,156,70,55,139,123,172,82,139,226,69,
                                 218,245,228,109,173,246,181,9,49,203,83,240,
                                 183,251,106,41,26,19,240,211,126,120,73,53,
                                 230,19,140,218,5,236,233,231,195,56,53,104,
                                 213,129,90,62,185,251,98,143,77,57,199,76,
                                 191,117,164,20,183,76,201,95,172,148,121,94,
                                 120,237,28,101,169,175,226,195,107,186,72,
                                 229,145,50,31,155,120,222,47,94,33,119,79,
                                 118,95,33,102,114,199,76,233,27,210,39,208,
                                 73,174,8,72,186,22,5,105,195,119,145,168,53,
                                 48,123,157,157,15,29,13,23,53,103,173,149,
                                 77,40,77,218,26,83,222,38,29,85,38,30,152,
                                 178,132,3,177,58,244,159,153,3,114,62,197,
                                 223,4,148,157,75,214,79,25,128,121,122,61,
                                 32,116,135,0,172,225,170,9,103,193,164,52,
                                 121,181,151,55,240,180,221,156,134,79,19,
                                 141,156,195,178,163,120,64,156,142,88,132,
                                 178,20,2,92,191,156,40,102,105,148,195,189,
                                 224,203,229,50,219,137,240,63,92,189,212,41,
                                 239,149,130,27,100,76,202,197,74,244,38,188,
                                 204,52,63,3,57>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.287+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.216.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.288+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:26.296+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.254.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.297+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.257.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.300+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.259.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:26.301+05:30,ns_1@cb.local:users_replicator<0.259.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-03-03T11:34:26.302+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.259.0>
[ns_server:debug,2020-03-03T11:34:26.302+05:30,ns_1@cb.local:users_replicator<0.259.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.260.0>
[ns_server:debug,2020-03-03T11:34:26.304+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-03-03T11:34:26.304+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.260.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.304+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.258.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:34:26.307+05:30,ns_1@cb.local:compiled_roles_cache<0.262.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-03-03T11:34:26.308+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.262.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.309+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.265.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:26.310+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.256.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:26.310+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.268.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:26.310+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.269.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:26.317+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-03-03T11:34:26.318+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 13ms
[ns_server:debug,2020-03-03T11:34:26.325+05:30,ns_1@cb.local:users_replicator<0.259.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2020-03-03T11:34:26.326+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.272.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:26.326+05:30,ns_1@cb.local:wait_link_to_couchdb_node<0.273.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[error_logger:info,2020-03-03T11:34:26.326+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:26.326+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:34:26.326+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.104293103.2785017858.81061>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:26.327+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.104293103.2785017858.81061>,
                                  inet_tcp_dist,<0.276.0>,
                                  #Ref<0.104293103.2785017858.81065>}
[ns_server:debug,2020-03-03T11:34:26.327+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.104293103.2785017858.81061>,
                               inet_tcp_dist,<0.276.0>,
                               #Ref<0.104293103.2785017858.81065>}
[error_logger:info,2020-03-03T11:34:26.327+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.276.0>,shutdown}}
[ns_server:debug,2020-03-03T11:34:26.327+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:34:26.327+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-03-03T11:34:26.528+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:26.528+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:34:26.528+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.104293103.2785017858.81075>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:26.528+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.104293103.2785017858.81075>,
                                  inet_tcp_dist,<0.279.0>,
                                  #Ref<0.104293103.2785017858.81079>}
[ns_server:debug,2020-03-03T11:34:26.657+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-03T11:34:26.857+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-03T11:34:27.058+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-03-03T11:34:27.237+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.283.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:27.301+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-03T11:34:27.309+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.104293103.2785017858.81075>,
                               inet_tcp_dist,<0.279.0>,
                               #Ref<0.104293103.2785017858.81079>}
[error_logger:info,2020-03-03T11:34:27.309+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.279.0>,connection_closed}}
[ns_server:info,2020-03-03T11:34:27.438+05:30,ns_1@cb.local:ns_couchdb_port<0.272.0>:ns_port_server:log:224]ns_couchdb<0.272.0>: Apache CouchDB  (LogLevel=info) is starting.
ns_couchdb<0.272.0>: Failure to start Mochiweb: eaddrinuse
ns_couchdb<0.272.0>: 4617: Booted. Waiting for shutdown request
ns_couchdb<0.272.0>: [os_mon] memory supervisor port (memsup): Erlang has closed
ns_couchdb<0.272.0>: [os_mon] cpu supervisor port (cpu_sup): Erlang has closed

[error_logger:info,2020-03-03T11:34:27.502+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:27.502+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:34:27.502+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.104293103.2785017860.81109>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:27.502+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.104293103.2785017860.81109>,
                                  inet_tcp_dist,<0.286.0>,
                                  #Ref<0.104293103.2785017858.81103>}
[ns_server:debug,2020-03-03T11:34:27.503+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.104293103.2785017860.81109>,
                               inet_tcp_dist,<0.286.0>,
                               #Ref<0.104293103.2785017858.81103>}
[ns_server:debug,2020-03-03T11:34:27.503+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:34:27.503+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.286.0>,shutdown}}
[error_logger:info,2020-03-03T11:34:27.503+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:27.703+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-03T11:34:27.703+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:27.703+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.104293103.2785017860.81114>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:27.703+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.104293103.2785017860.81114>,
                                  inet_tcp_dist,<0.289.0>,
                                  #Ref<0.104293103.2785017860.81118>}
[error_logger:info,2020-03-03T11:34:27.704+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.289.0>,shutdown}}
[error_logger:info,2020-03-03T11:34:27.704+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:27.704+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-03-03T11:34:27.704+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.104293103.2785017860.81114>,
                               inet_tcp_dist,<0.289.0>,
                               #Ref<0.104293103.2785017860.81118>}
[error_logger:info,2020-03-03T11:34:27.904+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:27.904+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:34:27.904+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.104293103.2785017859.81200>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:27.904+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.104293103.2785017859.81200>,
                                  inet_tcp_dist,<0.292.0>,
                                  #Ref<0.104293103.2785017859.81204>}
[ns_server:debug,2020-03-03T11:34:27.904+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.104293103.2785017859.81200>,
                               inet_tcp_dist,<0.292.0>,
                               #Ref<0.104293103.2785017859.81204>}
[error_logger:info,2020-03-03T11:34:27.904+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.292.0>,shutdown}}
[ns_server:debug,2020-03-03T11:34:27.905+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:34:27.905+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:info,2020-03-03T11:34:27.930+05:30,ns_1@cb.local:ns_couchdb_port<0.272.0>:ns_port_server:log:224]ns_couchdb<0.272.0>: {"Kernel pid terminated",application_controller,"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\"},{line,102}]},{supervisor,do_start_child,2,[{file,\"supervisor.erl\"},{line,365}]},{supervisor,start_children,3,[{file,\"supervisor.erl\"},{line,348}]},{supervisor,init_children,2,[{file,\"supervisor.erl\"},{line,314}]},{gen_server,init_it,2,[{file,\"gen_server.erl\"},{line,365}]},{gen_server,init_it,6,[{file,\"gen_server.erl\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}"}
ns_couchdb<0.272.0>: Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro
ns_couchdb<0.272.0>: 
ns_couchdb<0.272.0>: Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done

[error_logger:error,2020-03-03T11:34:27.930+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]** Generic server ns_couchdb_port terminating 
** Last message in was {#Port<0.5096>,{exit_status,1}}
** When Server state == {state,#Port<0.5096>,
                            {ns_couchdb,"/opt/couchbase/lib/erlang/bin/erl",
                                ["-pa",
                                 "/opt/couchbase/lib/erlang/lib/asn1-5.0.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/compiler-7.1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEvent-2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEventDomain-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosFileTransfer-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosNotification-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosProperty-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTime-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTransactions-1.3.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/crypto-4.2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/dialyzer-3.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/diameter-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/edoc-0.9.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eldap-1.2.3.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_docgen-0.7.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_interface-3.10.2.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erts-9.3.3.9/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eunit-2.3.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/hipe-3.17.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ic-4.4.4.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/inets-6.5.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/mnesia-4.15.3.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/orber-3.8.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/os_mon-2.4.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/otp_mibs-1.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/parsetools-2.1.6/ebin",
                                 "/opt/couchbase/lib/erlang/lib/public_key-1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/reltool-0.7.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/runtime_tools-1.12.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/sasl-3.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/snmp-5.2.11/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssh-4.6.9.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssl-8.2.6.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/syntax_tools-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/tools-2.11.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/xmerl-1.3.16.1/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/gc-couchbase-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/vtree-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/wkb-1.2.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch-1.2.0a-961ad59-git/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_audit-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_dcp-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_index_merger-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_set_view-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_view_parser-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/ejson-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/erlang-oauth/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/etap/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/lhttpc-1.3/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mapreduce-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mochiweb-1.4.1/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/snappy-1.0.4/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ale/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/gen_smtp/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_babysitter/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_couchdb/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/ebin",
                                 "/opt/couchbase/lib/erlang/lib/stdlib-3.4.5.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/kernel-5.4.3.2/ebin",
                                 ".","-couch_ini",
                                 "/opt/couchbase/etc/couchdb/default.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/capi.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/geocouch.ini",
                                 "/opt/couchbase/etc/couchdb/local.ini",
                                 "-kernel","error_logger","false","-kernel",
                                 "error_logger","false","inetrc",
                                 "\"/opt/couchbase/etc/couchbase/hosts.cfg\"",
                                 "dist_config_file",
                                 "\"/opt/couchbase/var/lib/couchbase/config/dist_cfg\"",
                                 "-ssl_dist_optfile",
                                 "/opt/couchbase/etc/couchbase/ssl_dist_opts",
                                 "-setcookie",
                                 "dce5392bcba7669ee9f057b78581e574cccf9efc1961f2376ff32b0a61220948",
                                 "-name","couchdb_ns_1@cb.local","-smp",
                                 "enable","+P","327680","+K","true","-kernel",
                                 "error_logger","false","-sasl",
                                 "sasl_error_logger","false","-nouser",
                                 "-hidden","-proto_dist","cb","-epmd_module",
                                 "cb_epmd","-start_epmd","false","-run",
                                 "child_erlang","child_start","ns_couchdb"],
                                [use_stdio,
                                 {env,
                                     [{"NS_COUCHDB_ENV_ARGS",
                                       "[{ns_server_node,'ns_1@cb.local'},\n {path_config_tmpdir,\"/opt/couchbase/var/lib/couchbase/tmp\"},\n {net_kernel_verbosity,10},\n {loglevel_error_logger,debug},\n {path_config_libdir,\"/opt/couchbase/lib\"},\n {loglevel_stats,debug},\n {loglevel_menelaus,debug},\n {path_config_secdir,\"/opt/couchbase/etc/security\"},\n {loglevel_user,debug},\n {path_config_etcdir,\"/opt/couchbase/etc/couchbase\"},\n {loglevel_ns_server,debug},\n {loglevel_mapreduce_errors,debug},\n {loglevel_rebalance,debug},\n {loglevel_default,debug},\n {disk_sink_opts,[{rotation,[{compress,true},\n                             {size,41943040},\n                             {num_files,10},\n                             {buffer_size_max,52428800}]}]},\n {loglevel_cbas,debug},\n {loglevel_xdcr,debug},\n {loglevel_ns_doctor,debug},\n {loglevel_access,info},\n {error_logger_mf_dir,\"/opt/couchbase/var/lib/couchbase/logs\"},\n {path_config_datadir,\"/opt/couchbase/var/lib/couchbase\"},\n {loglevel_cluster,debug},\n {loglevel_couchdb,info},\n {loglevel_views,debug},\n {path_config_bindir,\"/opt/couchbase/bin\"}]"},
                                      {"ERL_CRASH_DUMP",
                                       "erl_crash.dump.1583215409.3666.ns_couchdb"}]}]},
                            {ringbuffer,1190,1024,
                                {[{<<"Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done">>,
                                   80},
                                  {<<>>,0},
                                  {<<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                                   200}],
                                 [{<<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>,
                                   910}]}},
                            undefined,
                            {ok,{-576460748925,
                                 #Ref<0.104293103.2785017858.81114>}},
                            [<<"Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done">>,
                             <<>>,
                             <<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                             <<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>],
                            0}
** Reason for termination == 
** {abnormal,1}

[ns_server:error,2020-03-03T11:34:27.932+05:30,ns_1@cb.local:wait_link_to_couchdb_node<0.273.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:189]ns_couchdb_port(<0.272.0>) died with reason {abnormal,1}
[ns_server:debug,2020-03-03T11:34:27.932+05:30,ns_1@cb.local:<0.264.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.262.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:27.932+05:30,ns_1@cb.local:<0.263.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.262.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:27.932+05:30,ns_1@cb.local:<0.267.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.265.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:27.932+05:30,ns_1@cb.local:<0.266.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.265.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:27.932+05:30,ns_1@cb.local:<0.255.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.254.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:27.933+05:30,ns_1@cb.local:<0.216.0>:restartable:shutdown_child:120]Successfully terminated process <0.217.0>
[ns_server:debug,2020-03-03T11:34:27.933+05:30,ns_1@cb.local:<0.215.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.214.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:27.933+05:30,ns_1@cb.local:<0.201.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.200.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:27.933+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:wait_saver:866]Done waiting for saver.
[error_logger:error,2020-03-03T11:34:27.934+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_port_server:init/1
    pid: <0.272.0>
    registered_name: ns_couchdb_port
    exception exit: {abnormal,1}
      in function  gen_server:handle_common_reply/8 (gen_server.erl, line 726)
    ancestors: [ns_server_nodes_sup,<0.206.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 1
    messages: [{'EXIT',#Port<0.5096>,normal}]
    links: [<0.207.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 11890
  neighbours:

[error_logger:error,2020-03-03T11:34:27.934+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: erlang:apply/2
    pid: <0.273.0>
    registered_name: wait_link_to_couchdb_node
    exception exit: {abnormal,1}
      in function  ns_server_nodes_sup:do_wait_link_to_couchdb_node/1 (src/ns_server_nodes_sup.erl, line 190)
    ancestors: [ns_server_nodes_sup,<0.206.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.207.0>,<0.274.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 987
    stack_size: 27
    reductions: 3382
  neighbours:
    neighbour:
      pid: <0.274.0>
      registered_name: []
      initial call: ns_server_nodes_sup:'-do_wait_link_to_couchdb_node/1-fun-2-'/0
      current_function: {timer,sleep,1}
      ancestors: [wait_link_to_couchdb_node,ns_server_nodes_sup,<0.206.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
      message_queue_len: 0
      links: [<0.273.0>]
      trap_exit: false
      status: waiting
      heap_size: 2586
      stack_size: 12
      reductions: 12342
      current_stacktrace: [{timer,sleep,1,[{file,"timer.erl"},{line,153}]},
                  {misc,poll_for_condition_rec,3,
                      [{file,"src/misc.erl"},{line,508}]},
                  {ns_server_nodes_sup,
                      '-do_wait_link_to_couchdb_node/1-fun-2-',2,
                      [{file,"src/ns_server_nodes_sup.erl"},{line,159}]},
                  {proc_lib,init_p,3,[{file,"proc_lib.erl"},{line,232}]}]

[error_logger:error,2020-03-03T11:34:27.934+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    start_error
     Reason:     {abnormal,1}
     Offender:   [{pid,undefined},
                  {name,wait_for_couchdb_node},
                  {mfargs,{erlang,apply,
                                  [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:error,2020-03-03T11:34:27.934+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    shutdown_error
     Reason:     {abnormal,1}
     Offender:   [{pid,<0.272.0>},
                  {name,start_couchdb_node},
                  {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                  {restart_type,{permanent,5}},
                  {shutdown,86400000},
                  {child_type,worker}]


[error_logger:error,2020-03-03T11:34:27.934+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_cluster_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,wait_for_couchdb_node,
                         {abnormal,1}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_nodes_sup},
                  {mfargs,
                      {restartable,start_link,
                          [{ns_server_nodes_sup,start_link,[]},infinity]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-03-03T11:34:27.934+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,root_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                             {failed_to_start_child,wait_for_couchdb_node,
                                 {abnormal,1}}}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_cluster_sup},
                  {mfargs,{ns_server_cluster_sup,start_link,[]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-03-03T11:34:27.935+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: application_master:init/4
    pid: <0.117.0>
    registered_name: []
    exception exit: {{shutdown,
                      {failed_to_start_child,ns_server_cluster_sup,
                       {shutdown,
                        {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                          {failed_to_start_child,wait_for_couchdb_node,
                           {abnormal,1}}}}}}},
                     {ns_server,start,[normal,[]]}}
      in function  application_master:init/4 (application_master.erl, line 134)
    ancestors: [<0.116.0>]
    message_queue_len: 1
    messages: [{'EXIT',<0.118.0>,normal}]
    links: [<0.116.0>,<0.33.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 274
  neighbours:

[error_logger:info,2020-03-03T11:34:27.935+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
         application: ns_server
              exited: {{shutdown,
                        {failed_to_start_child,ns_server_cluster_sup,
                         {shutdown,
                          {failed_to_start_child,ns_server_nodes_sup,
                           {shutdown,
                            {failed_to_start_child,wait_for_couchdb_node,
                             {abnormal,1}}}}}}},
                       {ns_server,start,[normal,[]]}}
                type: permanent

[error_logger:info,2020-03-03T11:34:27.935+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/8689"}}

[error_logger:info,2020-03-03T11:34:27.935+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/4486"}}

[error_logger:info,2020-03-03T11:34:27.935+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-calculator/154"}}

[error_logger:info,2020-03-03T11:34:27.935+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-logs/25"}}

[error_logger:info,2020-03-03T11:34:27.935+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/59"}}

[error_logger:info,2020-03-03T11:34:27.935+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gnome-system-monitor/36"}}

[error_logger:info,2020-03-03T11:34:27.936+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/98"}}

[error_logger:info,2020-03-03T11:34:27.936+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-characters/69"}}

[ns_server:info,2020-03-03T11:34:35.706+05:30,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-03-03T11:34:35.746+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-03-03T11:34:35.746+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.746+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.746+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.746+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.746+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.746+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.746+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.746+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.746+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.746+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.746+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.746+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.747+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.747+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.747+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.747+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.747+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.747+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.747+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.747+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.747+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.747+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.747+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.747+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.747+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-03-03T11:34:35.747+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-03-03T11:34:35.756+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-03-03T11:34:35.757+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-03T11:34:35.759+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2020-03-03T11:34:35.785+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:35.786+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:35.786+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.169.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:35.788+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:35.795+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.174.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:35.796+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.175.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:35.796+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:35.796+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.168.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:34:35.797+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2020-03-03T11:34:35.799+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.176.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:35.800+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.177.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:35.801+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:35.826+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-03-03T11:34:35.826+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-03-03T11:34:35.826+05:30,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2020-03-03T11:34:35.829+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-03-03T11:34:35.829+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:35.829+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.167.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-03-03T11:34:35.831+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-03-03T11:34:35.836+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-03-03T11:34:35.836+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2020-03-03T11:34:35.836+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:35.836+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:34:35.836+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2604029378.1173880834.144402>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:35.837+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2604029378.1173880834.144402>,
                                  inet_tcp_dist,<0.183.0>,
                                  #Ref<0.2604029378.1173880834.144407>}
[ns_server:debug,2020-03-03T11:34:35.841+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-03-03T11:34:35.841+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2020-03-03T11:34:35.841+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-03T11:34:35.844+05:30,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-03-03T11:34:35.845+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:35.852+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.186.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:35.854+05:30,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,3,3},{11,34,35}}},
               {memory,
                   [{total,26412512},
                    {processes,9630240},
                    {processes_used,9624752},
                    {system,16782272},
                    {atom,388625},
                    {atom_used,364408},
                    {binary,106976},
                    {code,8250921},
                    {ets,1504320}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    application,heart,application_master,kernel,
                    application_controller,code,error_logger,file_server,ets,
                    file_io_server,gen_server,error_handler,file,gen_event,
                    supervisor,code_server,proc_lib,erl_parse,filename,lists,
                    erl_eval,erl_lint,gen,erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{os_mon,"CPO  CXC 138 46","2.4.4"},
                    {sasl,"SASL  CXC 138 11","3.1.2"},
                    {ns_server,"Couchbase server","6.5.0-4960-enterprise"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ale,"Another Logger for Erlang","0.0.0"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,129},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,auth,httpd_sup,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    release_handler,lhttpc_sup,httpc_sup,lhttpc_manager,
                    alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    'sink-disk_xdcr',timer_server,'sink-disk_debug',
                    kernel_safe_sup,ale_sup,'sink-disk_error',inet_db,
                    'sink-disk_default',ssl_pem_cache_dist,ale_dynamic_sup,
                    rex,global_group,net_sup,kernel_sup,ssl_connection_sup,
                    global_name_server,ssl_admin_sup,tftp_sup,ssl_sup,
                    root_sup,erts_code_purger,os_mon_sup,file_server_2,
                    error_logger,cpu_sup,erl_epmd,init,memsup,
                    erl_signal_server,net_kernel,disksup,ale,dist_manager,
                    ssl_pem_cache,ssl_manager,ssl_dist_admin_sup,
                    ssl_dist_connection_sup,ssl_dist_sup,user,
                    ssl_tls_dist_proxy,ssl_manager_dist,sasl_safe_sup,
                    ssl_listen_tracker_sup,code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,1}]
[ns_server:info,2020-03-03T11:34:35.862+05:30,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"cfa0f75f28402d2e1aa254b2a374bead19433526\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" upstream=\"master\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"f2a16b53bb74146f20d18ba2c0443d5f10a9a550\" upstream=\"master\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4960\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" upstream=\"master\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" upstream=\"mad-hatter\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" upstream=\"master\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" upstream=\"master\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" upstream=\"master\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" upstream=\"mad-hatter\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" upstream=\"cb-master\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" upstream=\"master\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" upstream=\"master\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" upstream=\"mad-hatter\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" upstream=\"mad-hatter\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" upstream=\"mad-hatter\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" upstream=\"master\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" upstream=\"master\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" upstream=\"mad-hatter\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" upstream=\"mad-hatter\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" upstream=\"mad-hatter\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" upstream=\"mad-hatter\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" upstream=\"master\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" upstream=\"master\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" upstream=\"master\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" upstream=\"mad-hatter\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" upstream=\"master\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"7a3aa547b3f5eb3ea28d279a08384609cd2cea7c\" upstream=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" upstream=\"mad-hatter\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" upstream=\"master\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" upstream=\"master\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" upstream=\"master\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" upstream=\"v1.1.0-couchbase\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"ee64d41320d14fabe814a241a5cf4f6a6f6e827a\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" upstream=\"master\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" upstream=\"master\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-03-03T11:34:35.871+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.187.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:35.872+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:35.872+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:35.873+05:30,ns_1@cb.local:ns_config_sup<0.190.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-03-03T11:34:35.873+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:35.873+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:35.939+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-03-03T11:34:35.939+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2020-03-03T11:34:35.957+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[{alert_limits,
   [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
  {audit,
   [{auditd_enabled,false},
    {rotate_interval,86400},
    {rotate_size,20971520},
    {disabled,[]},
    {sync,[]},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
  {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
  {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
  {autocompaction,
   [{database_fragmentation_threshold,{30,undefined}},
    {view_fragmentation_threshold,{30,undefined}}]},
  {buckets,[{configs,[]}]},
  {cbas_memory_quota,2174},
  {cert_and_pkey,
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFfi2B3wIO/gwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAyYWJmMjVlZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMmFiZjI1\nZWUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDI7xEpYzw8VsEaLCx3\nQQVbkzsO6PmRhi08x2I8YCA1DbAT1zVEJIkEG1u91CWD7eAhWsCD3TWwBFZfcERe\n4yqxtt5zpsN84LQXkd18MWeFYeZCHlbul4N7Xhs4PavRzjWlbTk8Qh4tTNIbioFs\n5JuPzeY6csaWRKrS3j35kY37lhmPz8EOgK4wOd1Fo7vdtEF4whXV/KW/f8JJvY63\n8LScK2GEZKz1EP9HbmfcCYf+/N0tqUHx2kgz98JBm3S/6EEbxWvVrFAosEhPbA3Q\nb7GUvIuPEahHQDqhL5pRw+H/KdOoLFgCsaWYk8niAZ9DOTLrDCQIJEEzEz+xmwj1\nn9AXAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCijNJXd2H4F3KW\nRbv5SJxGN4t7rFKL4kXa9eRtrfa1CTHLU/C3+2opGhPw0354STXmE4zaBezp58M4\nNWjVgVo+uftij005x0y/daQUt0zJX6yUeV547Rxlqa/iw2u6SOWRMh+beN4vXiF3\nT3ZfIWZyx0zpG9In0EmuCEi6FgVpw3eRqDUwe52dDx0NFzVnrZVNKE3aGlPeJh1V\nJh6YsoQDsTr0n5kDcj7F3wSUnUvWTxmAeXo9IHSHAKzhqglnwaQ0ebWXN/C03ZyG\nTxONnMOyo3hAnI5YhLIUAly/nChmaZTDveDL5TLbifA/XL3UKe+VghtkTMrFSvQm\nvMw0PwM5\n-----END CERTIFICATE-----\n">>,
     <<"*****">>}]},
  {drop_request_memory_threshold_mib,undefined},
  {email_alerts,
   [{recipients,["root@localhost"]},
    {sender,"couchbase@localhost"},
    {enabled,false},
    {email_server,
     [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
    {alerts,
     [auto_failover_node,auto_failover_maximum_reached,
      auto_failover_other_nodes_down,auto_failover_cluster_too_small,
      auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
      ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
      ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
  {fts_memory_quota,512},
  {index_aware_rebalance_disabled,false},
  {log_redaction_default_cfg,[{redact_level,none}]},
  {max_bucket_count,30},
  {memcached,[]},
  {memory_quota,8886},
  {nodes_wanted,['ns_1@cb.local']},
  {password_policy,[{min_length,6},{must_present,[]}]},
  {quorum_nodes,['ns_1@cb.local']},
  {remote_clusters,[]},
  {replication,[{enabled,true}]},
  {rest,[{port,8091}]},
  {rest_creds,null},
  {secure_headers,[]},
  {server_groups,
   [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
  {set_view_update_daemon,
   [{update_interval,5000},
    {update_min_changes,5000},
    {replica_update_min_changes,5000}]},
  {{couchdb,max_parallel_indexers},4},
  {{couchdb,max_parallel_replica_indexers},2},
  {{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{8,63750434666}}]}]},
  {{metakv,<<"/indexing/settings/config">>},
   <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
  {{request_limit,capi},undefined},
  {{request_limit,rest},undefined},
  {{node,'ns_1@cb.local',address_family},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    inet]},
  {{node,'ns_1@cb.local',audit},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
  {{node,'ns_1@cb.local',capi_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8092]},
  {{node,'ns_1@cb.local',cbas_admin_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9110]},
  {{node,'ns_1@cb.local',cbas_cc_client_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9113]},
  {{node,'ns_1@cb.local',cbas_cc_cluster_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9112]},
  {{node,'ns_1@cb.local',cbas_cc_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9111]},
  {{node,'ns_1@cb.local',cbas_cluster_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9115]},
  {{node,'ns_1@cb.local',cbas_console_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9114]},
  {{node,'ns_1@cb.local',cbas_data_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9116]},
  {{node,'ns_1@cb.local',cbas_debug_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    -1]},
  {{node,'ns_1@cb.local',cbas_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8095]},
  {{node,'ns_1@cb.local',cbas_messaging_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9118]},
  {{node,'ns_1@cb.local',cbas_metadata_callback_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9119]},
  {{node,'ns_1@cb.local',cbas_metadata_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9121]},
  {{node,'ns_1@cb.local',cbas_parent_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9122]},
  {{node,'ns_1@cb.local',cbas_replication_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9120]},
  {{node,'ns_1@cb.local',cbas_result_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9117]},
  {{node,'ns_1@cb.local',cbas_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18095]},
  {{node,'ns_1@cb.local',compaction_daemon},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@cb.local',config_version},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {6,5}]},
  {{node,'ns_1@cb.local',erl_external_listeners},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {inet,false},
    {inet6,false}]},
  {{node,'ns_1@cb.local',eventing_debug_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9140]},
  {{node,'ns_1@cb.local',eventing_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8096]},
  {{node,'ns_1@cb.local',eventing_https_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18096]},
  {{node,'ns_1@cb.local',fts_grpc_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9130]},
  {{node,'ns_1@cb.local',fts_grpc_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    19130]},
  {{node,'ns_1@cb.local',fts_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8094]},
  {{node,'ns_1@cb.local',fts_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18094]},
  {{node,'ns_1@cb.local',indexer_admin_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9100]},
  {{node,'ns_1@cb.local',indexer_http_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9102]},
  {{node,'ns_1@cb.local',indexer_https_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    19102]},
  {{node,'ns_1@cb.local',indexer_scan_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9101]},
  {{node,'ns_1@cb.local',indexer_stcatchup_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9104]},
  {{node,'ns_1@cb.local',indexer_stinit_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9103]},
  {{node,'ns_1@cb.local',indexer_stmaint_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9105]},
  {{node,'ns_1@cb.local',is_enterprise},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    true]},
  {{node,'ns_1@cb.local',isasl},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@cb.local',membership},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    active]},
  {{node,'ns_1@cb.local',memcached},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,11210},
    {dedicated_port,11209},
    {dedicated_ssl_port,11206},
    {ssl_port,11207},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_sleeptime,19},
    {log_rotation_period,39003}]},
  {{node,'ns_1@cb.local',memcached_config},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    {[{interfaces,
       {memcached_config_mgr,omit_missing_mcd_ports,
        [{[{host,<<"*">>},
           {port,port},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_port},
           {system,true},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,ssl_port},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_ssl_port},
           {system,true},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
      {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
      {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {opentracing,
       {[{enabled,opentracing_enabled},
         {module,{"~s",[opentracing_module]}},
         {config,{"~s",[opentracing_config]}}]}},
      {admin,{"~s",[admin_user]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize},
         {sleeptime,log_sleeptime}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
  {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    11206]},
  {{node,'ns_1@cb.local',memcached_defaults},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {max_connections,65000},
    {system_connections,5000},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {opentracing_enabled,false},
    {opentracing_module,[]},
    {opentracing_config,[]},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {tracing_enabled,true},
    {datatype_snappy,true},
    {num_reader_threads,<<"default">>},
    {num_writer_threads,<<"default">>}]},
  {{node,'ns_1@cb.local',moxi},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,0}]},
  {{node,'ns_1@cb.local',node_encryption},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    false]},
  {{node,'ns_1@cb.local',ns_log},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@cb.local',port_servers},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
  {{node,'ns_1@cb.local',projector_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9999]},
  {{node,'ns_1@cb.local',projector_ssl_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9999]},
  {{node,'ns_1@cb.local',query_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    8093]},
  {{node,'ns_1@cb.local',rest},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@cb.local',saslauthd_enabled},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    true]},
  {{node,'ns_1@cb.local',ssl_capi_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18092]},
  {{node,'ns_1@cb.local',ssl_query_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18093]},
  {{node,'ns_1@cb.local',ssl_rest_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    18091]},
  {{node,'ns_1@cb.local',uuid},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    <<"e32a4d3bd8aa759a4b96cd6ac25889ee">>]},
  {{node,'ns_1@cb.local',xdcr_rest_port},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    9998]},
  {{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
    false]}]]
[ns_server:info,2020-03-03T11:34:35.962+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   false]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9998]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   <<"e32a4d3bd8aa759a4b96cd6ac25889ee">>]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18091]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18093]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18092]},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   true]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9999]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',node_encryption},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   false]},
 {{node,'ns_1@cb.local',moxi},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,0}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   11206]},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',membership},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   active]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   true]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9100]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8094]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9130]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8096]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9140]},
 {{node,'ns_1@cb.local',erl_external_listeners},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {inet,false},
   {inet6,false}]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {6,5}]},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8095]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   9110]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   8092]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}]},
 {{node,'ns_1@cb.local',address_family},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   inet]},
 {{request_limit,rest},undefined},
 {{request_limit,capi},undefined},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>},
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{8,63750434666}}]}]},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
 {secure_headers,[]},
 {rest_creds,null},
 {rest,[{port,8091}]},
 {replication,[{enabled,true}]},
 {remote_clusters,[]},
 {quorum_nodes,['ns_1@cb.local']},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {nodes_wanted,['ns_1@cb.local']},
 {memory_quota,8886},
 {memcached,[]},
 {max_bucket_count,30},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {index_aware_rebalance_disabled,false},
 {fts_memory_quota,512},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {drop_request_memory_threshold_mib,undefined},
 {cert_and_pkey,
  [{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{1,63750434612}}]}|
   {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFfi2B3wIO/gwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAyYWJmMjVlZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMmFiZjI1\nZWUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDI7xEpYzw8VsEaLCx3\nQQVbkzsO6PmRhi08x2I8YCA1DbAT1zVEJIkEG1u91CWD7eAhWsCD3TWwBFZfcERe\n4yqxtt5zpsN84LQXkd18MWeFYeZCHlbul4N7Xhs4PavRzjWlbTk8Qh4tTNIbioFs\n5JuPzeY6csaWRKrS3j35kY37lhmPz8EOgK4wOd1Fo7vdtEF4whXV/KW/f8JJvY63\n8LScK2GEZKz1EP9HbmfcCYf+/N0tqUHx2kgz98JBm3S/6EEbxWvVrFAosEhPbA3Q\nb7GUvIuPEahHQDqhL5pRw+H/KdOoLFgCsaWYk8niAZ9DOTLrDCQIJEEzEz+xmwj1\nn9AXAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCijNJXd2H4F3KW\nRbv5SJxGN4t7rFKL4kXa9eRtrfa1CTHLU/C3+2opGhPw0354STXmE4zaBezp58M4\nNWjVgVo+uftij005x0y/daQUt0zJX6yUeV547Rxlqa/iw2u6SOWRMh+beN4vXiF3\nT3ZfIWZyx0zpG9In0EmuCEi6FgVpw3eRqDUwe52dDx0NFzVnrZVNKE3aGlPeJh1V\nJh6YsoQDsTr0n5kDcj7F3wSUnUvWTxmAeXo9IHSHAKzhqglnwaQ0ebWXN/C03ZyG\nTxONnMOyo3hAnI5YhLIUAly/nChmaZTDveDL5TLbifA/XL3UKe+VghtkTMrFSvQm\nvMw0PwM5\n-----END CERTIFICATE-----\n">>,
    <<"*****">>}]},
 {cbas_memory_quota,2174},
 {buckets,[{configs,[]}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]}]
[error_logger:info,2020-03-03T11:34:35.966+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:35.970+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.199.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:35.971+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.200.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:35.972+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:35.973+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.202.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:34:35.973+05:30,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>} ->
[{'_vclock',[{<<"e32a4d3bd8aa759a4b96cd6ac25889ee">>,{9,63750434675}}]}]
[error_logger:info,2020-03-03T11:34:35.978+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.205.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:35.993+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.208.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:35.995+05:30,ns_1@cb.local:menelaus_barrier<0.209.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-03-03T11:34:35.995+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.209.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:35.995+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:36.000+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:36.001+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.213.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:36.018+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2020-03-03T11:34:36.018+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2020-03-03T11:34:36.018+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2020-03-03T11:34:36.049+05:30,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:init:462]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,248,182,7,124,8,59,
             248,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
             6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
             118,101,114,32,50,97,98,102,50,53,101,101,48,30,23,13,49,51,48,
             49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
             57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
             104,98,97,115,101,32,83,101,114,118,101,114,32,50,97,98,102,50,
             53,101,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,
             3,130,1,15,0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,60,60,86,
             193,26,44,44,119,65,5,91,147,59,14,232,249,145,134,45,60,199,98,
             60,96,32,53,13,176,19,215,53,68,36,137,4,27,91,189,212,37,131,
             237,224,33,90,192,131,221,53,176,4,86,95,112,68,94,227,42,177,
             182,222,115,166,195,124,224,180,23,145,221,124,49,103,133,97,
             230,66,30,86,238,151,131,123,94,27,56,61,171,209,206,53,165,109,
             57,60,66,30,45,76,210,27,138,129,108,228,155,143,205,230,58,114,
             198,150,68,170,210,222,61,249,145,141,251,150,25,143,207,193,14,
             128,174,48,57,221,69,163,187,221,180,65,120,194,21,213,252,165,
             191,127,194,73,189,142,183,240,180,156,43,97,132,100,172,245,16,
             255,71,110,103,220,9,135,254,252,221,45,169,65,241,218,72,51,
             247,194,65,155,116,191,232,65,27,197,107,213,172,80,40,176,72,
             79,108,13,208,111,177,148,188,139,143,17,168,71,64,58,161,47,
             154,81,195,225,255,41,211,168,44,88,2,177,165,152,147,201,226,1,
             159,67,57,50,235,12,36,8,36,65,51,19,63,177,155,8,245,159,208,
             23,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,
             164,48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,
             3,85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,134,72,134,247,
             13,1,1,11,5,0,3,130,1,1,0,162,140,210,87,119,97,248,23,114,150,
             69,187,249,72,156,70,55,139,123,172,82,139,226,69,218,245,228,
             109,173,246,181,9,49,203,83,240,183,251,106,41,26,19,240,211,
             126,120,73,53,230,19,140,218,5,236,233,231,195,56,53,104,213,
             129,90,62,185,251,98,143,77,57,199,76,191,117,164,20,183,76,201,
             95,172,148,121,94,120,237,28,101,169,175,226,195,107,186,72,229,
             145,50,31,155,120,222,47,94,33,119,79,118,95,33,102,114,199,76,
             233,27,210,39,208,73,174,8,72,186,22,5,105,195,119,145,168,53,
             48,123,157,157,15,29,13,23,53,103,173,149,77,40,77,218,26,83,
             222,38,29,85,38,30,152,178,132,3,177,58,244,159,153,3,114,62,
             197,223,4,148,157,75,214,79,25,128,121,122,61,32,116,135,0,172,
             225,170,9,103,193,164,52,121,181,151,55,240,180,221,156,134,79,
             19,141,156,195,178,163,120,64,156,142,88,132,178,20,2,92,191,
             156,40,102,105,148,195,189,224,203,229,50,219,137,240,63,92,189,
             212,41,239,149,130,27,100,76,202,197,74,244,38,188,204,52,63,3,
             57>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2020-03-03T11:34:36.050+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.214.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:36.069+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-03T11:34:36.069+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-03T11:34:36.070+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-03T11:34:36.070+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:info,2020-03-03T11:34:36.094+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-03T11:34:36.095+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-03T11:34:36.095+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-03T11:34:36.095+05:30,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2020-03-03T11:34:36.096+05:30,ns_1@cb.local:<0.216.0>:restartable:start_child:98]Started child process <0.217.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2020-03-03T11:34:36.094+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.217.0>,menelaus_web}
             started: [{pid,<0.218.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,
                                 248,182,7,124,8,59,248,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,50,97,98,
                                 102,50,53,101,101,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,50,
                                 97,98,102,50,53,101,101,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,
                                 60,60,86,193,26,44,44,119,65,5,91,147,59,14,
                                 232,249,145,134,45,60,199,98,60,96,32,53,13,
                                 176,19,215,53,68,36,137,4,27,91,189,212,37,
                                 131,237,224,33,90,192,131,221,53,176,4,86,
                                 95,112,68,94,227,42,177,182,222,115,166,195,
                                 124,224,180,23,145,221,124,49,103,133,97,
                                 230,66,30,86,238,151,131,123,94,27,56,61,
                                 171,209,206,53,165,109,57,60,66,30,45,76,
                                 210,27,138,129,108,228,155,143,205,230,58,
                                 114,198,150,68,170,210,222,61,249,145,141,
                                 251,150,25,143,207,193,14,128,174,48,57,221,
                                 69,163,187,221,180,65,120,194,21,213,252,
                                 165,191,127,194,73,189,142,183,240,180,156,
                                 43,97,132,100,172,245,16,255,71,110,103,220,
                                 9,135,254,252,221,45,169,65,241,218,72,51,
                                 247,194,65,155,116,191,232,65,27,197,107,
                                 213,172,80,40,176,72,79,108,13,208,111,177,
                                 148,188,139,143,17,168,71,64,58,161,47,154,
                                 81,195,225,255,41,211,168,44,88,2,177,165,
                                 152,147,201,226,1,159,67,57,50,235,12,36,8,
                                 36,65,51,19,63,177,155,8,245,159,208,23,2,3,
                                 1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 162,140,210,87,119,97,248,23,114,150,69,187,
                                 249,72,156,70,55,139,123,172,82,139,226,69,
                                 218,245,228,109,173,246,181,9,49,203,83,240,
                                 183,251,106,41,26,19,240,211,126,120,73,53,
                                 230,19,140,218,5,236,233,231,195,56,53,104,
                                 213,129,90,62,185,251,98,143,77,57,199,76,
                                 191,117,164,20,183,76,201,95,172,148,121,94,
                                 120,237,28,101,169,175,226,195,107,186,72,
                                 229,145,50,31,155,120,222,47,94,33,119,79,
                                 118,95,33,102,114,199,76,233,27,210,39,208,
                                 73,174,8,72,186,22,5,105,195,119,145,168,53,
                                 48,123,157,157,15,29,13,23,53,103,173,149,
                                 77,40,77,218,26,83,222,38,29,85,38,30,152,
                                 178,132,3,177,58,244,159,153,3,114,62,197,
                                 223,4,148,157,75,214,79,25,128,121,122,61,
                                 32,116,135,0,172,225,170,9,103,193,164,52,
                                 121,181,151,55,240,180,221,156,134,79,19,
                                 141,156,195,178,163,120,64,156,142,88,132,
                                 178,20,2,92,191,156,40,102,105,148,195,189,
                                 224,203,229,50,219,137,240,63,92,189,212,41,
                                 239,149,130,27,100,76,202,197,74,244,38,188,
                                 204,52,63,3,57>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:36.098+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.217.0>,menelaus_web}
             started: [{pid,<0.236.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,21,
                                 248,182,7,124,8,59,248,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,50,97,98,
                                 102,50,53,101,101,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,50,
                                 97,98,102,50,53,101,101,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,200,239,17,41,99,
                                 60,60,86,193,26,44,44,119,65,5,91,147,59,14,
                                 232,249,145,134,45,60,199,98,60,96,32,53,13,
                                 176,19,215,53,68,36,137,4,27,91,189,212,37,
                                 131,237,224,33,90,192,131,221,53,176,4,86,
                                 95,112,68,94,227,42,177,182,222,115,166,195,
                                 124,224,180,23,145,221,124,49,103,133,97,
                                 230,66,30,86,238,151,131,123,94,27,56,61,
                                 171,209,206,53,165,109,57,60,66,30,45,76,
                                 210,27,138,129,108,228,155,143,205,230,58,
                                 114,198,150,68,170,210,222,61,249,145,141,
                                 251,150,25,143,207,193,14,128,174,48,57,221,
                                 69,163,187,221,180,65,120,194,21,213,252,
                                 165,191,127,194,73,189,142,183,240,180,156,
                                 43,97,132,100,172,245,16,255,71,110,103,220,
                                 9,135,254,252,221,45,169,65,241,218,72,51,
                                 247,194,65,155,116,191,232,65,27,197,107,
                                 213,172,80,40,176,72,79,108,13,208,111,177,
                                 148,188,139,143,17,168,71,64,58,161,47,154,
                                 81,195,225,255,41,211,168,44,88,2,177,165,
                                 152,147,201,226,1,159,67,57,50,235,12,36,8,
                                 36,65,51,19,63,177,155,8,245,159,208,23,2,3,
                                 1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 162,140,210,87,119,97,248,23,114,150,69,187,
                                 249,72,156,70,55,139,123,172,82,139,226,69,
                                 218,245,228,109,173,246,181,9,49,203,83,240,
                                 183,251,106,41,26,19,240,211,126,120,73,53,
                                 230,19,140,218,5,236,233,231,195,56,53,104,
                                 213,129,90,62,185,251,98,143,77,57,199,76,
                                 191,117,164,20,183,76,201,95,172,148,121,94,
                                 120,237,28,101,169,175,226,195,107,186,72,
                                 229,145,50,31,155,120,222,47,94,33,119,79,
                                 118,95,33,102,114,199,76,233,27,210,39,208,
                                 73,174,8,72,186,22,5,105,195,119,145,168,53,
                                 48,123,157,157,15,29,13,23,53,103,173,149,
                                 77,40,77,218,26,83,222,38,29,85,38,30,152,
                                 178,132,3,177,58,244,159,153,3,114,62,197,
                                 223,4,148,157,75,214,79,25,128,121,122,61,
                                 32,116,135,0,172,225,170,9,103,193,164,52,
                                 121,181,151,55,240,180,221,156,134,79,19,
                                 141,156,195,178,163,120,64,156,142,88,132,
                                 178,20,2,92,191,156,40,102,105,148,195,189,
                                 224,203,229,50,219,137,240,63,92,189,212,41,
                                 239,149,130,27,100,76,202,197,74,244,38,188,
                                 204,52,63,3,57>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:36.099+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.216.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:36.099+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:36.115+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.254.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:36.120+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.257.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:36.127+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.259.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:36.129+05:30,ns_1@cb.local:users_replicator<0.259.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-03-03T11:34:36.130+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.259.0>
[ns_server:debug,2020-03-03T11:34:36.130+05:30,ns_1@cb.local:users_replicator<0.259.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.260.0>
[ns_server:debug,2020-03-03T11:34:36.131+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-03-03T11:34:36.131+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.260.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:36.131+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.258.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-03T11:34:36.136+05:30,ns_1@cb.local:compiled_roles_cache<0.262.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-03-03T11:34:36.136+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.262.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:36.138+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.265.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-03T11:34:36.138+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.256.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:36.139+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.268.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-03T11:34:36.139+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.269.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:34:36.159+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-03-03T11:34:36.160+05:30,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 28ms
[ns_server:debug,2020-03-03T11:34:36.162+05:30,ns_1@cb.local:users_replicator<0.259.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[ns_server:debug,2020-03-03T11:34:36.164+05:30,ns_1@cb.local:wait_link_to_couchdb_node<0.273.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[error_logger:info,2020-03-03T11:34:36.164+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.272.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:36.164+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-03T11:34:36.164+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:36.164+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2604029378.1173880834.144511>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:36.164+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2604029378.1173880834.144511>,
                                  inet_tcp_dist,<0.276.0>,
                                  #Ref<0.2604029378.1173880833.144364>}
[ns_server:debug,2020-03-03T11:34:36.164+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2604029378.1173880834.144511>,
                               inet_tcp_dist,<0.276.0>,
                               #Ref<0.2604029378.1173880833.144364>}
[ns_server:debug,2020-03-03T11:34:36.164+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:34:36.165+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.276.0>,shutdown}}
[error_logger:info,2020-03-03T11:34:36.165+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-03-03T11:34:36.365+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:36.365+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:34:36.365+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2604029378.1173880833.144369>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:36.365+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2604029378.1173880833.144369>,
                                  inet_tcp_dist,<0.279.0>,
                                  #Ref<0.2604029378.1173880833.144373>}
[ns_server:debug,2020-03-03T11:34:36.365+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2604029378.1173880833.144369>,
                               inet_tcp_dist,<0.279.0>,
                               #Ref<0.2604029378.1173880833.144373>}
[error_logger:info,2020-03-03T11:34:36.365+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.279.0>,shutdown}}
[ns_server:debug,2020-03-03T11:34:36.365+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:34:36.365+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:36.566+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-03T11:34:36.566+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:36.566+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2604029378.1173880834.144531>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:36.566+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2604029378.1173880834.144531>,
                                  inet_tcp_dist,<0.282.0>,
                                  #Ref<0.2604029378.1173880834.144535>}
[ns_server:debug,2020-03-03T11:34:36.610+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-03T11:34:36.811+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-03T11:34:37.016+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-03-03T11:34:37.141+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.286.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-03T11:34:37.216+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-03T11:34:37.228+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2604029378.1173880834.144531>,
                               inet_tcp_dist,<0.282.0>,
                               #Ref<0.2604029378.1173880834.144535>}
[error_logger:info,2020-03-03T11:34:37.228+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.282.0>,connection_closed}}
[ns_server:info,2020-03-03T11:34:37.342+05:30,ns_1@cb.local:ns_couchdb_port<0.272.0>:ns_port_server:log:224]ns_couchdb<0.272.0>: Apache CouchDB  (LogLevel=info) is starting.
ns_couchdb<0.272.0>: Failure to start Mochiweb: eaddrinuse
ns_couchdb<0.272.0>: 4723: Booted. Waiting for shutdown request
ns_couchdb<0.272.0>: [os_mon] cpu supervisor port (cpu_sup): Erlang has closed
ns_couchdb<0.272.0>: [os_mon] memory supervisor port (memsup): Erlang has closed

[error_logger:info,2020-03-03T11:34:37.417+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:37.417+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:34:37.417+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2604029378.1173880834.144541>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:37.417+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2604029378.1173880834.144541>,
                                  inet_tcp_dist,<0.289.0>,
                                  #Ref<0.2604029378.1173880834.144543>}
[ns_server:debug,2020-03-03T11:34:37.417+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2604029378.1173880834.144541>,
                               inet_tcp_dist,<0.289.0>,
                               #Ref<0.2604029378.1173880834.144543>}
[ns_server:debug,2020-03-03T11:34:37.417+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:34:37.417+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.289.0>,shutdown}}
[error_logger:info,2020-03-03T11:34:37.417+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:37.618+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-03T11:34:37.618+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:37.618+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2604029378.1173880833.144406>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:37.618+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2604029378.1173880833.144406>,
                                  inet_tcp_dist,<0.292.0>,
                                  #Ref<0.2604029378.1173880834.144544>}
[ns_server:debug,2020-03-03T11:34:37.618+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2604029378.1173880833.144406>,
                               inet_tcp_dist,<0.292.0>,
                               #Ref<0.2604029378.1173880834.144544>}
[ns_server:debug,2020-03-03T11:34:37.618+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:34:37.618+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.292.0>,shutdown}}
[error_logger:info,2020-03-03T11:34:37.619+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-03-03T11:34:37.819+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-03T11:34:37.819+05:30,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-03T11:34:37.819+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2604029378.1173880834.144550>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-03T11:34:37.819+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2604029378.1173880834.144550>,
                                  inet_tcp_dist,<0.295.0>,
                                  #Ref<0.2604029378.1173880834.144554>}
[ns_server:debug,2020-03-03T11:34:37.819+05:30,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2604029378.1173880834.144550>,
                               inet_tcp_dist,<0.295.0>,
                               #Ref<0.2604029378.1173880834.144554>}
[error_logger:info,2020-03-03T11:34:37.819+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.295.0>,shutdown}}
[ns_server:debug,2020-03-03T11:34:37.819+05:30,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-03T11:34:37.819+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:info,2020-03-03T11:34:37.847+05:30,ns_1@cb.local:ns_couchdb_port<0.272.0>:ns_port_server:log:224]ns_couchdb<0.272.0>: {"Kernel pid terminated",application_controller,"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\"},{line,102}]},{supervisor,do_start_child,2,[{file,\"supervisor.erl\"},{line,365}]},{supervisor,start_children,3,[{file,\"supervisor.erl\"},{line,348}]},{supervisor,init_children,2,[{file,\"supervisor.erl\"},{line,314}]},{gen_server,init_it,2,[{file,\"gen_server.erl\"},{line,365}]},{gen_server,init_it,6,[{file,\"gen_server.erl\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}"}
ns_couchdb<0.272.0>: Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro
ns_couchdb<0.272.0>: 
ns_couchdb<0.272.0>: Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done

[error_logger:error,2020-03-03T11:34:37.848+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]** Generic server ns_couchdb_port terminating 
** Last message in was {#Port<0.5096>,{exit_status,1}}
** When Server state == {state,#Port<0.5096>,
                            {ns_couchdb,"/opt/couchbase/lib/erlang/bin/erl",
                                ["-pa",
                                 "/opt/couchbase/lib/erlang/lib/asn1-5.0.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/compiler-7.1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEvent-2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEventDomain-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosFileTransfer-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosNotification-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosProperty-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTime-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTransactions-1.3.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/crypto-4.2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/dialyzer-3.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/diameter-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/edoc-0.9.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eldap-1.2.3.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_docgen-0.7.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_interface-3.10.2.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erts-9.3.3.9/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eunit-2.3.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/hipe-3.17.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ic-4.4.4.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/inets-6.5.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/mnesia-4.15.3.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/orber-3.8.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/os_mon-2.4.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/otp_mibs-1.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/parsetools-2.1.6/ebin",
                                 "/opt/couchbase/lib/erlang/lib/public_key-1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/reltool-0.7.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/runtime_tools-1.12.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/sasl-3.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/snmp-5.2.11/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssh-4.6.9.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssl-8.2.6.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/syntax_tools-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/tools-2.11.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/xmerl-1.3.16.1/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/gc-couchbase-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/vtree-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/wkb-1.2.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch-1.2.0a-961ad59-git/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_audit-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_dcp-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_index_merger-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_set_view-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_view_parser-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/ejson-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/erlang-oauth/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/etap/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/lhttpc-1.3/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mapreduce-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mochiweb-1.4.1/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/snappy-1.0.4/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ale/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/gen_smtp/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_babysitter/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_couchdb/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/ebin",
                                 "/opt/couchbase/lib/erlang/lib/stdlib-3.4.5.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/kernel-5.4.3.2/ebin",
                                 ".","-couch_ini",
                                 "/opt/couchbase/etc/couchdb/default.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/capi.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/geocouch.ini",
                                 "/opt/couchbase/etc/couchdb/local.ini",
                                 "-kernel","error_logger","false","-kernel",
                                 "error_logger","false","inetrc",
                                 "\"/opt/couchbase/etc/couchbase/hosts.cfg\"",
                                 "dist_config_file",
                                 "\"/opt/couchbase/var/lib/couchbase/config/dist_cfg\"",
                                 "-ssl_dist_optfile",
                                 "/opt/couchbase/etc/couchbase/ssl_dist_opts",
                                 "-setcookie",
                                 "dce5392bcba7669ee9f057b78581e574cccf9efc1961f2376ff32b0a61220948",
                                 "-name","couchdb_ns_1@cb.local","-smp",
                                 "enable","+P","327680","+K","true","-kernel",
                                 "error_logger","false","-sasl",
                                 "sasl_error_logger","false","-nouser",
                                 "-hidden","-proto_dist","cb","-epmd_module",
                                 "cb_epmd","-start_epmd","false","-run",
                                 "child_erlang","child_start","ns_couchdb"],
                                [use_stdio,
                                 {env,
                                     [{"NS_COUCHDB_ENV_ARGS",
                                       "[{ns_server_node,'ns_1@cb.local'},\n {path_config_tmpdir,\"/opt/couchbase/var/lib/couchbase/tmp\"},\n {net_kernel_verbosity,10},\n {loglevel_error_logger,debug},\n {path_config_libdir,\"/opt/couchbase/lib\"},\n {loglevel_stats,debug},\n {loglevel_menelaus,debug},\n {path_config_secdir,\"/opt/couchbase/etc/security\"},\n {loglevel_user,debug},\n {path_config_etcdir,\"/opt/couchbase/etc/couchbase\"},\n {loglevel_ns_server,debug},\n {loglevel_mapreduce_errors,debug},\n {loglevel_rebalance,debug},\n {loglevel_default,debug},\n {disk_sink_opts,[{rotation,[{compress,true},\n                             {size,41943040},\n                             {num_files,10},\n                             {buffer_size_max,52428800}]}]},\n {loglevel_cbas,debug},\n {loglevel_xdcr,debug},\n {loglevel_ns_doctor,debug},\n {loglevel_access,info},\n {error_logger_mf_dir,\"/opt/couchbase/var/lib/couchbase/logs\"},\n {path_config_datadir,\"/opt/couchbase/var/lib/couchbase\"},\n {loglevel_cluster,debug},\n {loglevel_couchdb,info},\n {loglevel_views,debug},\n {path_config_bindir,\"/opt/couchbase/bin\"}]"},
                                      {"ERL_CRASH_DUMP",
                                       "erl_crash.dump.1583215409.3666.ns_couchdb"}]}]},
                            {ringbuffer,1190,1024,
                                {[{<<"Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done">>,
                                   80},
                                  {<<>>,0},
                                  {<<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                                   200}],
                                 [{<<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>,
                                   910}]}},
                            undefined,
                            {ok,{-576460748519,
                                 #Ref<0.2604029378.1173880835.144183>}},
                            [<<"Crash dump is being written to: erl_crash.dump.1583215409.3666.ns_couchdb...done">>,
                             <<>>,
                             <<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                             <<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>],
                            0}
** Reason for termination == 
** {abnormal,1}

[ns_server:error,2020-03-03T11:34:37.849+05:30,ns_1@cb.local:wait_link_to_couchdb_node<0.273.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:189]ns_couchdb_port(<0.272.0>) died with reason {abnormal,1}
[ns_server:debug,2020-03-03T11:34:37.849+05:30,ns_1@cb.local:<0.267.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.265.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:37.849+05:30,ns_1@cb.local:<0.264.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.262.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:37.849+05:30,ns_1@cb.local:<0.263.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.262.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:37.849+05:30,ns_1@cb.local:<0.266.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.265.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:37.849+05:30,ns_1@cb.local:<0.255.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.254.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:37.850+05:30,ns_1@cb.local:<0.216.0>:restartable:shutdown_child:120]Successfully terminated process <0.217.0>
[ns_server:debug,2020-03-03T11:34:37.850+05:30,ns_1@cb.local:<0.215.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.214.0>} exited with reason shutdown
[ns_server:debug,2020-03-03T11:34:37.850+05:30,ns_1@cb.local:ns_config<0.193.0>:ns_config:wait_saver:866]Done waiting for saver.
[ns_server:debug,2020-03-03T11:34:37.850+05:30,ns_1@cb.local:<0.201.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.200.0>} exited with reason shutdown
[error_logger:error,2020-03-03T11:34:37.852+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_port_server:init/1
    pid: <0.272.0>
    registered_name: ns_couchdb_port
    exception exit: {abnormal,1}
      in function  gen_server:handle_common_reply/8 (gen_server.erl, line 726)
    ancestors: [ns_server_nodes_sup,<0.206.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 1
    messages: [{'EXIT',#Port<0.5096>,normal}]
    links: [<0.207.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 6772
    stack_size: 27
    reductions: 11879
  neighbours:

[error_logger:error,2020-03-03T11:34:37.852+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: erlang:apply/2
    pid: <0.273.0>
    registered_name: wait_link_to_couchdb_node
    exception exit: {abnormal,1}
      in function  ns_server_nodes_sup:do_wait_link_to_couchdb_node/1 (src/ns_server_nodes_sup.erl, line 190)
    ancestors: [ns_server_nodes_sup,<0.206.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.207.0>,<0.274.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 987
    stack_size: 27
    reductions: 3382
  neighbours:
    neighbour:
      pid: <0.274.0>
      registered_name: []
      initial call: ns_server_nodes_sup:'-do_wait_link_to_couchdb_node/1-fun-2-'/0
      current_function: {timer,sleep,1}
      ancestors: [wait_link_to_couchdb_node,ns_server_nodes_sup,<0.206.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
      message_queue_len: 0
      links: [<0.273.0>]
      trap_exit: false
      status: waiting
      heap_size: 2586
      stack_size: 12
      reductions: 13904
      current_stacktrace: [{timer,sleep,1,[{file,"timer.erl"},{line,153}]},
                  {misc,poll_for_condition_rec,3,
                      [{file,"src/misc.erl"},{line,508}]},
                  {ns_server_nodes_sup,
                      '-do_wait_link_to_couchdb_node/1-fun-2-',2,
                      [{file,"src/ns_server_nodes_sup.erl"},{line,159}]},
                  {proc_lib,init_p,3,[{file,"proc_lib.erl"},{line,232}]}]

[error_logger:error,2020-03-03T11:34:37.852+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    start_error
     Reason:     {abnormal,1}
     Offender:   [{pid,undefined},
                  {name,wait_for_couchdb_node},
                  {mfargs,{erlang,apply,
                                  [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:error,2020-03-03T11:34:37.853+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    shutdown_error
     Reason:     {abnormal,1}
     Offender:   [{pid,<0.272.0>},
                  {name,start_couchdb_node},
                  {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                  {restart_type,{permanent,5}},
                  {shutdown,86400000},
                  {child_type,worker}]


[error_logger:error,2020-03-03T11:34:37.853+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_cluster_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,wait_for_couchdb_node,
                         {abnormal,1}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_nodes_sup},
                  {mfargs,
                      {restartable,start_link,
                          [{ns_server_nodes_sup,start_link,[]},infinity]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-03-03T11:34:37.853+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,root_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                             {failed_to_start_child,wait_for_couchdb_node,
                                 {abnormal,1}}}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_cluster_sup},
                  {mfargs,{ns_server_cluster_sup,start_link,[]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-03-03T11:34:37.853+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: application_master:init/4
    pid: <0.117.0>
    registered_name: []
    exception exit: {{shutdown,
                      {failed_to_start_child,ns_server_cluster_sup,
                       {shutdown,
                        {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                          {failed_to_start_child,wait_for_couchdb_node,
                           {abnormal,1}}}}}}},
                     {ns_server,start,[normal,[]]}}
      in function  application_master:init/4 (application_master.erl, line 134)
    ancestors: [<0.116.0>]
    message_queue_len: 1
    messages: [{'EXIT',<0.118.0>,normal}]
    links: [<0.116.0>,<0.33.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 274
  neighbours:

[error_logger:info,2020-03-03T11:34:37.853+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
         application: ns_server
              exited: {{shutdown,
                        {failed_to_start_child,ns_server_cluster_sup,
                         {shutdown,
                          {failed_to_start_child,ns_server_nodes_sup,
                           {shutdown,
                            {failed_to_start_child,wait_for_couchdb_node,
                             {abnormal,1}}}}}}},
                       {ns_server,start,[normal,[]]}}
                type: permanent

[error_logger:info,2020-03-03T11:34:37.853+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,system_memory_high_watermark}

[error_logger:info,2020-03-03T11:34:37.853+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/8689"}}

[error_logger:info,2020-03-03T11:34:37.853+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/4486"}}

[error_logger:info,2020-03-03T11:34:37.853+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-calculator/154"}}

[error_logger:info,2020-03-03T11:34:37.854+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-logs/25"}}

[error_logger:info,2020-03-03T11:34:37.854+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/59"}}

[error_logger:info,2020-03-03T11:34:37.854+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gnome-system-monitor/36"}}

[error_logger:info,2020-03-03T11:34:37.854+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/98"}}

[error_logger:info,2020-03-03T11:34:37.854+05:30,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-characters/69"}}

[ns_server:info,2020-03-03T11:35:16.671+05:30,nonode@nohost:<0.89.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-03-03T11:35:16.674+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-03-03T11:35:16.675+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.675+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.675+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.675+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.675+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.675+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.675+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.675+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.675+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.675+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.675+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.675+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.675+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.675+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.676+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.676+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.676+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.676+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.676+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.676+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.676+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.676+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.676+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.676+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-03-03T11:35:16.676+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[error_logger:info,2020-03-03T11:35:16.679+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.138.0>},
                       {name,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:35:16.681+05:30,nonode@nohost:ns_server_cluster_sup<0.137.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"R16B03-1"},
               {erl_version,"5.10.4.0.0.1"},
               {erl_version_long,
                   "Erlang R16B03-1 (erts-5.10.4.0.0.1) [source-378cfabb7f] [64-bit] [smp:4:4] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,3,3},{11,35,16}}},
               {memory,
                   [{total,29122176},
                    {processes,9452440},
                    {processes_used,9451280},
                    {system,19669736},
                    {atom,339441},
                    {atom_used,322610},
                    {binary,72648},
                    {code,7796727},
                    {ets,2241664}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,path_config,calendar,
                    ale_default_formatter,'ale_logger-metakv',
                    'ale_logger-rebalance','ale_logger-menelaus',
                    'ale_logger-stats','ale_logger-json_rpc',
                    'ale_logger-access','ale_logger-ns_server',
                    'ale_logger-user','ale_logger-ns_doctor',
                    'ale_logger-cluster','ale_logger-xdcr',otp_internal,
                    ns_log_sink,ale_disk_sink,misc,io_lib_fread,couch_util,
                    ns_server,filelib,cpu_sup,memsup,disksup,os_mon,io,
                    release_handler,overload,alarm_handler,sasl,timer,
                    tftp_sup,httpd_sup,httpc_handler_sup,httpc_cookie,
                    inets_trace,httpc_manager,httpc,httpc_profile_sup,
                    httpc_sup,ftp_sup,inets_sup,inets_app,ssl,lhttpc_manager,
                    lhttpc_sup,lhttpc,tls_connection_sup,ssl_session_cache,
                    ssl_pkix_db,ssl_manager,ssl_sup,ssl_app,crypto_server,
                    crypto_sup,crypto_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,beam_dict,beam_asm,beam_validator,beam_z,
                    beam_flatten,beam_trim,beam_receive,beam_bsm,beam_peep,
                    beam_dead,beam_split,beam_type,beam_bool,beam_except,
                    beam_clean,beam_utils,beam_block,beam_jump,beam_a,
                    v3_codegen,v3_life,v3_kernel,sys_core_dsetel,erl_bifs,
                    sys_core_fold,cerl_trees,sys_core_inline,core_lib,cerl,
                    v3_core,erl_bits,erl_expand_records,sys_pre_expand,sofs,
                    erl_internal,sets,ordsets,erl_lint,compile,
                    dynamic_compile,ale_utils,io_lib_pretty,io_lib_format,
                    io_lib,ale_codegen,dict,ale,ale_dynamic_sup,ale_sup,
                    ale_app,epp,ns_bootstrap,child_erlang,file_io_server,
                    orddict,erl_eval,file,c,kernel_config,user_io,user_sup,
                    supervisor_bridge,standard_error,code_server,unicode,
                    hipe_unified_loader,gb_sets,ets,binary,code,file_server,
                    net_kernel,global_group,erl_distribution,filename,
                    inet_gethost_native,os,inet_parse,inet,inet_udp,
                    inet_config,inet_db,global,gb_trees,rpc,supervisor,kernel,
                    application_master,sys,application,gen_server,erl_parse,
                    proplists,erl_scan,lists,application_controller,proc_lib,
                    gen,gen_event,error_logger,heart,error_handler,
                    erts_internal,erlang,erl_prim_loader,prim_zip,zlib,
                    prim_file,prim_inet,prim_eval,init,otp_ring0]},
               {applications,
                   [{lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {os_mon,"CPO  CXC 138 46","2.2.14"},
                    {public_key,"Public key infrastructure","0.21"},
                    {asn1,"The Erlang ASN1 compiler version 2.0.4","2.0.4"},
                    {kernel,"ERTS  CXC 138 10","2.16.4"},
                    {ale,"Another Logger for Erlang","6.0.4-3082-enterprise"},
                    {inets,"INETS  CXC 138 49","5.9.8"},
                    {ns_server,"Couchbase server","6.0.4-3082-enterprise"},
                    {crypto,"CRYPTO version 2","3.2"},
                    {ssl,"Erlang/OTP SSL application","5.3.3"},
                    {sasl,"SASL  CXC 138 11","2.3.4"},
                    {stdlib,"ERTS  CXC 138 10","1.19.4"}]},
               {pre_loaded,
                   [erts_internal,erlang,erl_prim_loader,prim_zip,zlib,
                    prim_file,prim_inet,prim_eval,init,otp_ring0]},
               {process_count,105},
               {node,nonode@nohost},
               {nodes,[]},
               {registered,
                   [lhttpc_sup,code_server,'sink-disk_default',
                    ale_stats_events,ns_server_cluster_sup,lhttpc_manager,
                    application_controller,ale,httpd_sup,release_handler,
                    kernel_safe_sup,standard_error,ale_sup,overload,
                    error_logger,alarm_handler,ale_dynamic_sup,timer_server,
                    standard_error_sup,crypto_server,crypto_sup,sasl_safe_sup,
                    tftp_sup,'sink-ns_log',inet_db,init,os_mon_sup,rex,
                    tls_connection_sup,user,ssl_sup,kernel_sup,cpu_sup,
                    global_name_server,memsup,disksup,httpc_sup,file_server_2,
                    'sink-disk_json_rpc',ssl_manager,local_tasks,global_group,
                    'sink-disk_metakv',httpc_profile_sup,httpc_manager,
                    'sink-disk_access_int',httpc_handler_sup,
                    'sink-disk_access',ftp_sup,'sink-disk_reports',sasl_sup,
                    'sink-disk_stats',erl_prim_loader,'sink-disk_xdcr',
                    'sink-disk_debug',inets_sup,'sink-disk_error']},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,1}]
[ns_server:info,2020-03-03T11:35:16.685+05:30,nonode@nohost:ns_server_cluster_sup<0.137.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"d200fc0f68695d4aef1fad5c3c8cc55f8c033014\" upstream=\"refs/tags/0.9.7\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"74a44a626e8e8aba2f2f62fb96aa25ddf6ee227b\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"d0710cefe032017800887fe68cbbb8a25a8f28a1\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"62e3145e2f8e30ccd46e460f400bf34dfb2ce4c8\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"9e3465560240ffb242b50a47cb7f19251a12ee42\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"f5f59722bc8b1015c35e98b26eb95ba64393cd90\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"f551b6d4f32bb920a83dd28c705bddd5de0d03b2\" />",
 "  <project name=\"blevex\" path=\"godeps/src/github.com/blevesearch/blevex\" remote=\"blevesearch\" revision=\"4b158bb555a3297565afecf6fae675c74f1e47df\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"dc55cf8e9613ddf1bd6db4a662d1ecc3668223da\" upstream=\"alice\">",
 "    <annotation name=\"RELEASE\" value=\"alice\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"3082\" />",
 "    <annotation name=\"VERSION\" value=\"6.0.4\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbaselabs/cbas\" remote=\"couchbase-priv\" revision=\"caccf4ec99780ad99ae43683df7b4868b1305e0a\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"b044bbb55b77baf1a55677727add910d9b099305\" upstream=\"alice\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"d8f601c3109887a30deccab09b2177bf499c8711\" upstream=\"alice\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"0df84c7e3c6d95ff435c12a3c08c6f064db11e97\" />",
 "  <project name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"80d2ad8892d806f5103f602fec0d80adaa4b628f\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"794a9aa0c45837797b9ee11628bf3a7baa02cc52\" upstream=\"alice\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"0a94f40b9080e0ecb11d3b7531a58c5e6a4a4465\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbq-gui\" path=\"goproj/src/github.com/couchbase/cbq-gui\" remote=\"couchbase-priv\" revision=\"93d26dbc120c4f9874abb65aafa4d815334273c6\" upstream=\"alice\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"dbfa1c0d73f0e49f6f04e390f03de8f9a6cee769\" />",
 "  <project name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"dcae66272b24600ae0005fa06b511cfae8914d3d\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" />",
 "  <project name=\"couchbase-cli\" revision=\"1333235b35a8faf8e68cc0a0327f9042232af1fb\" upstream=\"alice\" />",
 "  <project name=\"couchdb\" revision=\"8ccc45cdcc160f9896812965d61a29ddec6f69ba\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"4dc357bf919ec257cd87b05e7ea9f32de23f1b03\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"143858d76c865b10039436c3bfc723cbdf5c180f\" upstream=\"alice\" />",
 "  <project name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"f23ba3a5ee43012fcb4b92e1a2a405a92554f4f2\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"05067021a042a1b63e100a486afd7ebddab4c535\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" />",
 "  <project name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"6daf6338e56953bac41f3e5e5f3468ec3f31bc35\" upstream=\"alice\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"kv\" name=\"forestdb\" revision=\"caf0a806e2d1881c4b6558b06fae30adcff84d20\" upstream=\"vulcan\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"2a0e73f43451045f157640eec59ced72da18471f\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"5c0ac9ef0ba7a0356279cee7eadd8874c4a8f8c1\" upstream=\"alice\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" />",
 "  <project name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"47fc4e5e9153645da45af6a86a5bce95e63a0f9e\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d2f15a425a9c8e4d8447e5f5b89ce14845f7fa05\" upstream=\"vulcan\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" />",
 "  <project name=\"gocb\" path=\"godeps/src/github.com/couchbase/gocb\" revision=\"699b13a51af5dd4f80ff3deedf41bba60debad32\" upstream=\"refs/tags/v1.3.7\" />",
 "  <project name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"710456e087a6d497e87f41d0a9d98d6a75672186\" />",
 "  <project name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"a0d26c2d6f5de912499d35a5aba573006e5e036f\" upstream=\"refs/tags/v7.1.7\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"aecdbe5a5a91f0688df7bdf260ca962178c06828\" upstream=\"vulcan\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"e05ec3550789be1da2981787ef6444fef75e10bc\" upstream=\"vulcan\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"4809dd3aa2ab88013b1dfce15516d717a35cb6e8\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"f98adca8eb365032cab838ef4d99453931afa112\" upstream=\"vulcan\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"74af34a22201c501af7ac4116be5d662b309c38e\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"e4cb4e14e5db0ed0205270652c7626d07ca8cf78\" upstream=\"alice\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"c4ede32c575a231e71d82f5fee71887e14ef22b5\" upstream=\"alice\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"081e8b16b991bf706eb77f8243935c6fba31b895\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"3681c2a912330352991ecdd642f257efe5b85518\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"956632ec1bc3e28276d00ee2f22c3202f06efb12\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"abd3b58b422dbc2e9463a589d0f3d93441726e23\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project groups=\"kv\" name=\"moxi\" revision=\"cd8da46b9b953800d430c8b0aa4667790727ed6f\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"62685c2d7ca23c807425dca88b11a3e2323dab41\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"f3bef3551997be504612a2d05a8b324b3bfdfe1b\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"4494252972a2a00d8039d42211e6cfb5088295aa\" upstream=\"alice\" />",
 "  <project name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"96501c57bb0fd61c85cba6f63101aed2bcf41d38\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"937e735fe17ea2e9dcf522f25ec0b9d44e30128d\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"de77d527bd23012dc67794c81a7b1e76e46e3abd\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"8cf7d25a1dcb5434ea44983c0bed202fb9cc8351\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"655cdfa588ea190e901bc5590e65d5621688847c\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"01ccaa4589911755e95e3ef0270e2d52e6d965d0\" upstream=\"alice\" />",
 "  <project name=\"query-ui\" revision=\"15a6461f437fe810e411a6613ec7c143991cd1c6\" upstream=\"alice\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"fe09428be4c233d726797a1380f7438f4f71a31a\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"73353fe6dad8f3d67409feefb9b17f90f6de917b\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"4fba14c79f356ae48d6141c561bf9fd7ba33fabd\" upstream=\"refs/tags/v0.14.0\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"c30c3d4c250e68e81c57aa1e8ae91ffd21243cdb\" />",
 "  <project name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"9d4e42a20653790449273b3c85e67d6d8bae6e2e\" />",
 "  <project name=\"testrunner\" revision=\"3f6036c76c0e8d09921016d80fa6b7c3c28365cd\" upstream=\"alice\" />",
 "  <project name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"601048ad6acbab6cedd582db09b8c4839ff25b15\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"6dadf98e19803cc10ce9552bf4d9121cc2528965\" upstream=\"alice\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"0ceea4a37442f76199b9259840baf48d17af3c1a\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"62f7f8c3f9923bcccd6ed07c0bbfddc3280b3384\" upstream=\"alice\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-03-03T11:35:16.708+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{set_lock,{me,<0.139.0>},
          {{code_version,time_compat},<0.139.0>},
          {nodes,[nonode@nohost]},
          {retries,infinity},
          {times,1}}
[error_logger:info,2020-03-03T11:35:16.708+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{handle_set_lock,{{code_version,time_compat},<0.139.0>},<0.139.0>}
[error_logger:info,2020-03-03T11:35:16.708+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{set_lock,{me,<0.139.0>},
          {{code_version,time_compat},<0.139.0>},
          {nodes,[nonode@nohost]},
          {replies,[{nonode@nohost,true}]}}
[error_logger:info,2020-03-03T11:35:16.709+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{set_lock_true,{{code_version,time_compat},<0.139.0>}}
[error_logger:info,2020-03-03T11:35:16.709+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{del_lock,{me,<0.139.0>},
          {{code_version,time_compat},<0.139.0>},
          {nodes,[nonode@nohost]}}
[error_logger:info,2020-03-03T11:35:16.709+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{handle_del_lock,{pid,<0.139.0>},{id,{{code_version,time_compat},<0.139.0>}}}
[error_logger:info,2020-03-03T11:35:16.709+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{remove_lock_1,{id,{code_version,time_compat}},{pid,<0.139.0>}}
[error_logger:info,2020-03-03T11:35:16.709+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.139.0>},
                       {name,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:35:16.710+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:read_address_config_from_path:159]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-03-03T11:35:16.710+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:read_address_config_from_path:159]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[error_logger:info,2020-03-03T11:35:16.711+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.143.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2020-03-03T11:35:16.711+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.142.0>},
                       {name,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-03-03T11:35:21.731+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:35:21.732+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:35:27.734+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:35:27.734+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:35:33.737+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:35:33.737+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:35:39.743+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:35:39.743+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:35:45.748+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:35:45.748+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:35:51.754+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:35:51.754+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:35:57.760+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:35:57.760+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:36:03.765+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:36:03.765+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:36:09.770+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:36:09.770+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:36:15.774+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:36:15.774+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:error,2020-03-03T11:36:16.775+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:init:258]Configured address `cb.local` seems to be invalid. Will refuse to start for safety reasons.
[ns_server:info,2020-03-03T11:36:17.720+05:30,nonode@nohost:<0.89.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.723+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.724+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.724+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.724+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-03-03T11:36:17.724+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[error_logger:info,2020-03-03T11:36:17.726+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.138.0>},
                       {name,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:36:17.729+05:30,nonode@nohost:ns_server_cluster_sup<0.137.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"R16B03-1"},
               {erl_version,"5.10.4.0.0.1"},
               {erl_version_long,
                   "Erlang R16B03-1 (erts-5.10.4.0.0.1) [source-378cfabb7f] [64-bit] [smp:4:4] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,3,3},{11,36,17}}},
               {memory,
                   [{total,29115568},
                    {processes,9447576},
                    {processes_used,9446416},
                    {system,19667992},
                    {atom,339441},
                    {atom_used,322610},
                    {binary,72624},
                    {code,7796727},
                    {ets,2241720}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,path_config,calendar,
                    ale_default_formatter,'ale_logger-metakv',
                    'ale_logger-rebalance','ale_logger-menelaus',
                    'ale_logger-stats','ale_logger-json_rpc',
                    'ale_logger-access','ale_logger-ns_server',
                    'ale_logger-user','ale_logger-ns_doctor',
                    'ale_logger-cluster','ale_logger-xdcr',otp_internal,
                    ns_log_sink,ale_disk_sink,io_lib_fread,misc,couch_util,
                    ns_server,filelib,cpu_sup,memsup,disksup,os_mon,io,
                    release_handler,overload,alarm_handler,sasl,timer,
                    tftp_sup,httpd_sup,httpc_handler_sup,httpc_cookie,
                    inets_trace,httpc_manager,httpc,httpc_profile_sup,
                    httpc_sup,ftp_sup,inets_sup,inets_app,ssl,lhttpc_manager,
                    lhttpc_sup,lhttpc,tls_connection_sup,ssl_session_cache,
                    ssl_pkix_db,ssl_manager,ssl_sup,ssl_app,crypto_server,
                    crypto_sup,crypto_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,beam_dict,beam_asm,beam_validator,beam_z,
                    beam_flatten,beam_trim,beam_receive,beam_bsm,beam_peep,
                    beam_dead,beam_split,beam_type,beam_bool,beam_except,
                    beam_clean,beam_utils,beam_block,beam_jump,beam_a,
                    v3_codegen,v3_life,v3_kernel,sys_core_dsetel,erl_bifs,
                    sys_core_fold,cerl_trees,sys_core_inline,core_lib,cerl,
                    v3_core,erl_bits,erl_expand_records,sys_pre_expand,sofs,
                    erl_internal,sets,ordsets,erl_lint,compile,
                    dynamic_compile,ale_utils,io_lib_pretty,io_lib_format,
                    io_lib,ale_codegen,dict,ale,ale_dynamic_sup,ale_sup,
                    ale_app,epp,ns_bootstrap,child_erlang,file_io_server,
                    orddict,erl_eval,file,c,kernel_config,user_io,user_sup,
                    supervisor_bridge,standard_error,code_server,unicode,
                    hipe_unified_loader,gb_sets,ets,binary,code,file_server,
                    net_kernel,global_group,erl_distribution,filename,
                    inet_gethost_native,os,inet_parse,inet,inet_udp,
                    inet_config,inet_db,global,gb_trees,rpc,supervisor,kernel,
                    application_master,sys,application,gen_server,erl_parse,
                    proplists,erl_scan,lists,application_controller,proc_lib,
                    gen,gen_event,error_logger,heart,error_handler,
                    erts_internal,erlang,erl_prim_loader,prim_zip,zlib,
                    prim_file,prim_inet,prim_eval,init,otp_ring0]},
               {applications,
                   [{lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {os_mon,"CPO  CXC 138 46","2.2.14"},
                    {public_key,"Public key infrastructure","0.21"},
                    {asn1,"The Erlang ASN1 compiler version 2.0.4","2.0.4"},
                    {kernel,"ERTS  CXC 138 10","2.16.4"},
                    {ale,"Another Logger for Erlang","6.0.4-3082-enterprise"},
                    {inets,"INETS  CXC 138 49","5.9.8"},
                    {ns_server,"Couchbase server","6.0.4-3082-enterprise"},
                    {crypto,"CRYPTO version 2","3.2"},
                    {ssl,"Erlang/OTP SSL application","5.3.3"},
                    {sasl,"SASL  CXC 138 11","2.3.4"},
                    {stdlib,"ERTS  CXC 138 10","1.19.4"}]},
               {pre_loaded,
                   [erts_internal,erlang,erl_prim_loader,prim_zip,zlib,
                    prim_file,prim_inet,prim_eval,init,otp_ring0]},
               {process_count,105},
               {node,nonode@nohost},
               {nodes,[]},
               {registered,
                   [lhttpc_sup,code_server,'sink-disk_default',
                    ale_stats_events,ns_server_cluster_sup,lhttpc_manager,
                    application_controller,ale,httpd_sup,release_handler,
                    kernel_safe_sup,standard_error,ale_sup,overload,
                    error_logger,alarm_handler,ale_dynamic_sup,timer_server,
                    standard_error_sup,crypto_server,crypto_sup,sasl_safe_sup,
                    tftp_sup,'sink-ns_log',inet_db,init,os_mon_sup,rex,
                    tls_connection_sup,user,ssl_sup,kernel_sup,cpu_sup,
                    global_name_server,memsup,disksup,httpc_sup,file_server_2,
                    'sink-disk_json_rpc',ssl_manager,local_tasks,global_group,
                    'sink-disk_metakv',httpc_profile_sup,httpc_manager,
                    'sink-disk_access_int',httpc_handler_sup,
                    'sink-disk_access',ftp_sup,'sink-disk_reports',sasl_sup,
                    'sink-disk_stats',erl_prim_loader,'sink-disk_xdcr',
                    'sink-disk_debug',inets_sup,'sink-disk_error']},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,0}]
[ns_server:info,2020-03-03T11:36:17.733+05:30,nonode@nohost:ns_server_cluster_sup<0.137.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"d200fc0f68695d4aef1fad5c3c8cc55f8c033014\" upstream=\"refs/tags/0.9.7\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"74a44a626e8e8aba2f2f62fb96aa25ddf6ee227b\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"d0710cefe032017800887fe68cbbb8a25a8f28a1\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"62e3145e2f8e30ccd46e460f400bf34dfb2ce4c8\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"9e3465560240ffb242b50a47cb7f19251a12ee42\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"f5f59722bc8b1015c35e98b26eb95ba64393cd90\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"f551b6d4f32bb920a83dd28c705bddd5de0d03b2\" />",
 "  <project name=\"blevex\" path=\"godeps/src/github.com/blevesearch/blevex\" remote=\"blevesearch\" revision=\"4b158bb555a3297565afecf6fae675c74f1e47df\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"dc55cf8e9613ddf1bd6db4a662d1ecc3668223da\" upstream=\"alice\">",
 "    <annotation name=\"RELEASE\" value=\"alice\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"3082\" />",
 "    <annotation name=\"VERSION\" value=\"6.0.4\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbaselabs/cbas\" remote=\"couchbase-priv\" revision=\"caccf4ec99780ad99ae43683df7b4868b1305e0a\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"b044bbb55b77baf1a55677727add910d9b099305\" upstream=\"alice\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"d8f601c3109887a30deccab09b2177bf499c8711\" upstream=\"alice\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"0df84c7e3c6d95ff435c12a3c08c6f064db11e97\" />",
 "  <project name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"80d2ad8892d806f5103f602fec0d80adaa4b628f\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"794a9aa0c45837797b9ee11628bf3a7baa02cc52\" upstream=\"alice\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"0a94f40b9080e0ecb11d3b7531a58c5e6a4a4465\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbq-gui\" path=\"goproj/src/github.com/couchbase/cbq-gui\" remote=\"couchbase-priv\" revision=\"93d26dbc120c4f9874abb65aafa4d815334273c6\" upstream=\"alice\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"dbfa1c0d73f0e49f6f04e390f03de8f9a6cee769\" />",
 "  <project name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"dcae66272b24600ae0005fa06b511cfae8914d3d\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" />",
 "  <project name=\"couchbase-cli\" revision=\"1333235b35a8faf8e68cc0a0327f9042232af1fb\" upstream=\"alice\" />",
 "  <project name=\"couchdb\" revision=\"8ccc45cdcc160f9896812965d61a29ddec6f69ba\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"4dc357bf919ec257cd87b05e7ea9f32de23f1b03\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"143858d76c865b10039436c3bfc723cbdf5c180f\" upstream=\"alice\" />",
 "  <project name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"f23ba3a5ee43012fcb4b92e1a2a405a92554f4f2\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"05067021a042a1b63e100a486afd7ebddab4c535\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" />",
 "  <project name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"6daf6338e56953bac41f3e5e5f3468ec3f31bc35\" upstream=\"alice\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"kv\" name=\"forestdb\" revision=\"caf0a806e2d1881c4b6558b06fae30adcff84d20\" upstream=\"vulcan\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"2a0e73f43451045f157640eec59ced72da18471f\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"5c0ac9ef0ba7a0356279cee7eadd8874c4a8f8c1\" upstream=\"alice\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" />",
 "  <project name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"47fc4e5e9153645da45af6a86a5bce95e63a0f9e\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d2f15a425a9c8e4d8447e5f5b89ce14845f7fa05\" upstream=\"vulcan\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" />",
 "  <project name=\"gocb\" path=\"godeps/src/github.com/couchbase/gocb\" revision=\"699b13a51af5dd4f80ff3deedf41bba60debad32\" upstream=\"refs/tags/v1.3.7\" />",
 "  <project name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"710456e087a6d497e87f41d0a9d98d6a75672186\" />",
 "  <project name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"a0d26c2d6f5de912499d35a5aba573006e5e036f\" upstream=\"refs/tags/v7.1.7\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"aecdbe5a5a91f0688df7bdf260ca962178c06828\" upstream=\"vulcan\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"e05ec3550789be1da2981787ef6444fef75e10bc\" upstream=\"vulcan\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"4809dd3aa2ab88013b1dfce15516d717a35cb6e8\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"f98adca8eb365032cab838ef4d99453931afa112\" upstream=\"vulcan\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"74af34a22201c501af7ac4116be5d662b309c38e\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"e4cb4e14e5db0ed0205270652c7626d07ca8cf78\" upstream=\"alice\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"c4ede32c575a231e71d82f5fee71887e14ef22b5\" upstream=\"alice\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"081e8b16b991bf706eb77f8243935c6fba31b895\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"3681c2a912330352991ecdd642f257efe5b85518\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"956632ec1bc3e28276d00ee2f22c3202f06efb12\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"abd3b58b422dbc2e9463a589d0f3d93441726e23\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project groups=\"kv\" name=\"moxi\" revision=\"cd8da46b9b953800d430c8b0aa4667790727ed6f\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"62685c2d7ca23c807425dca88b11a3e2323dab41\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"f3bef3551997be504612a2d05a8b324b3bfdfe1b\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"4494252972a2a00d8039d42211e6cfb5088295aa\" upstream=\"alice\" />",
 "  <project name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"96501c57bb0fd61c85cba6f63101aed2bcf41d38\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"937e735fe17ea2e9dcf522f25ec0b9d44e30128d\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"de77d527bd23012dc67794c81a7b1e76e46e3abd\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"8cf7d25a1dcb5434ea44983c0bed202fb9cc8351\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"655cdfa588ea190e901bc5590e65d5621688847c\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"01ccaa4589911755e95e3ef0270e2d52e6d965d0\" upstream=\"alice\" />",
 "  <project name=\"query-ui\" revision=\"15a6461f437fe810e411a6613ec7c143991cd1c6\" upstream=\"alice\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"fe09428be4c233d726797a1380f7438f4f71a31a\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"73353fe6dad8f3d67409feefb9b17f90f6de917b\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"4fba14c79f356ae48d6141c561bf9fd7ba33fabd\" upstream=\"refs/tags/v0.14.0\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"c30c3d4c250e68e81c57aa1e8ae91ffd21243cdb\" />",
 "  <project name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"9d4e42a20653790449273b3c85e67d6d8bae6e2e\" />",
 "  <project name=\"testrunner\" revision=\"3f6036c76c0e8d09921016d80fa6b7c3c28365cd\" upstream=\"alice\" />",
 "  <project name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"601048ad6acbab6cedd582db09b8c4839ff25b15\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"6dadf98e19803cc10ce9552bf4d9121cc2528965\" upstream=\"alice\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"0ceea4a37442f76199b9259840baf48d17af3c1a\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"62f7f8c3f9923bcccd6ed07c0bbfddc3280b3384\" upstream=\"alice\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-03-03T11:36:17.757+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{set_lock,{me,<0.139.0>},
          {{code_version,time_compat},<0.139.0>},
          {nodes,[nonode@nohost]},
          {retries,infinity},
          {times,1}}
[error_logger:info,2020-03-03T11:36:17.757+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{handle_set_lock,{{code_version,time_compat},<0.139.0>},<0.139.0>}
[error_logger:info,2020-03-03T11:36:17.757+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{set_lock,{me,<0.139.0>},
          {{code_version,time_compat},<0.139.0>},
          {nodes,[nonode@nohost]},
          {replies,[{nonode@nohost,true}]}}
[error_logger:info,2020-03-03T11:36:17.757+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{set_lock_true,{{code_version,time_compat},<0.139.0>}}
[error_logger:info,2020-03-03T11:36:17.758+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{del_lock,{me,<0.139.0>},
          {{code_version,time_compat},<0.139.0>},
          {nodes,[nonode@nohost]}}
[error_logger:info,2020-03-03T11:36:17.758+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{handle_del_lock,{pid,<0.139.0>},{id,{{code_version,time_compat},<0.139.0>}}}
[error_logger:info,2020-03-03T11:36:17.758+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{remove_lock_1,{id,{code_version,time_compat}},{pid,<0.139.0>}}
[error_logger:info,2020-03-03T11:36:17.758+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.139.0>},
                       {name,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:36:17.759+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:read_address_config_from_path:159]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-03-03T11:36:17.759+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:read_address_config_from_path:159]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[error_logger:info,2020-03-03T11:36:17.759+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.143.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2020-03-03T11:36:17.759+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.142.0>},
                       {name,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-03-03T11:36:22.782+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:36:22.782+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:36:28.787+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:36:28.787+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:36:34.790+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:36:34.790+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:36:40.799+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:36:40.799+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:36:46.800+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:36:46.801+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:36:52.806+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:36:52.806+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:36:58.812+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:36:58.812+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:37:04.818+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:37:04.818+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:37:10.822+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:37:10.822+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:37:16.826+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:37:16.826+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:error,2020-03-03T11:37:17.827+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:init:258]Configured address `cb.local` seems to be invalid. Will refuse to start for safety reasons.
[ns_server:info,2020-03-03T11:37:19.021+05:30,nonode@nohost:<0.89.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-03-03T11:37:19.030+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-03-03T11:37:19.030+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.030+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.030+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.030+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.030+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.030+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.030+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.030+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.031+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.031+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.031+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.031+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.031+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.031+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.031+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.031+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.031+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.031+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.031+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.031+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.031+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.031+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.031+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.031+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-03-03T11:37:19.031+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[error_logger:info,2020-03-03T11:37:19.035+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.138.0>},
                       {name,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:37:19.040+05:30,nonode@nohost:ns_server_cluster_sup<0.137.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"R16B03-1"},
               {erl_version,"5.10.4.0.0.1"},
               {erl_version_long,
                   "Erlang R16B03-1 (erts-5.10.4.0.0.1) [source-378cfabb7f] [64-bit] [smp:4:4] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,3,3},{11,37,19}}},
               {memory,
                   [{total,29136976},
                    {processes,9464232},
                    {processes_used,9462928},
                    {system,19672744},
                    {atom,339441},
                    {atom_used,322610},
                    {binary,72672},
                    {code,7796727},
                    {ets,2241664}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,path_config,calendar,
                    ale_default_formatter,'ale_logger-metakv',
                    'ale_logger-rebalance','ale_logger-menelaus',
                    'ale_logger-stats','ale_logger-json_rpc',
                    'ale_logger-access','ale_logger-ns_server',
                    'ale_logger-user','ale_logger-ns_doctor',
                    'ale_logger-cluster','ale_logger-xdcr',otp_internal,
                    ns_log_sink,ale_disk_sink,misc,couch_util,io_lib_fread,
                    ns_server,filelib,cpu_sup,memsup,disksup,os_mon,io,
                    release_handler,overload,alarm_handler,sasl,timer,
                    tftp_sup,httpd_sup,httpc_handler_sup,httpc_cookie,
                    inets_trace,httpc_manager,httpc,httpc_profile_sup,
                    httpc_sup,ftp_sup,inets_sup,inets_app,ssl,lhttpc_manager,
                    lhttpc_sup,lhttpc,tls_connection_sup,ssl_session_cache,
                    ssl_pkix_db,ssl_manager,ssl_sup,ssl_app,crypto_server,
                    crypto_sup,crypto_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,beam_dict,beam_asm,beam_validator,beam_z,
                    beam_flatten,beam_trim,beam_receive,beam_bsm,beam_peep,
                    beam_dead,beam_split,beam_type,beam_bool,beam_except,
                    beam_clean,beam_utils,beam_block,beam_jump,beam_a,
                    v3_codegen,v3_life,v3_kernel,sys_core_dsetel,erl_bifs,
                    sys_core_fold,cerl_trees,sys_core_inline,core_lib,cerl,
                    v3_core,erl_bits,erl_expand_records,sys_pre_expand,sofs,
                    erl_internal,sets,ordsets,erl_lint,compile,
                    dynamic_compile,ale_utils,io_lib_pretty,io_lib_format,
                    io_lib,ale_codegen,dict,ale,ale_dynamic_sup,ale_sup,
                    ale_app,epp,ns_bootstrap,child_erlang,file_io_server,
                    orddict,erl_eval,file,c,kernel_config,user_io,user_sup,
                    supervisor_bridge,standard_error,code_server,unicode,
                    hipe_unified_loader,gb_sets,ets,binary,code,file_server,
                    net_kernel,global_group,erl_distribution,filename,
                    inet_gethost_native,os,inet_parse,inet,inet_udp,
                    inet_config,inet_db,global,gb_trees,rpc,supervisor,kernel,
                    application_master,sys,application,gen_server,erl_parse,
                    proplists,erl_scan,lists,application_controller,proc_lib,
                    gen,gen_event,error_logger,heart,error_handler,
                    erts_internal,erlang,erl_prim_loader,prim_zip,zlib,
                    prim_file,prim_inet,prim_eval,init,otp_ring0]},
               {applications,
                   [{lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {os_mon,"CPO  CXC 138 46","2.2.14"},
                    {public_key,"Public key infrastructure","0.21"},
                    {asn1,"The Erlang ASN1 compiler version 2.0.4","2.0.4"},
                    {kernel,"ERTS  CXC 138 10","2.16.4"},
                    {ale,"Another Logger for Erlang","6.0.4-3082-enterprise"},
                    {inets,"INETS  CXC 138 49","5.9.8"},
                    {ns_server,"Couchbase server","6.0.4-3082-enterprise"},
                    {crypto,"CRYPTO version 2","3.2"},
                    {ssl,"Erlang/OTP SSL application","5.3.3"},
                    {sasl,"SASL  CXC 138 11","2.3.4"},
                    {stdlib,"ERTS  CXC 138 10","1.19.4"}]},
               {pre_loaded,
                   [erts_internal,erlang,erl_prim_loader,prim_zip,zlib,
                    prim_file,prim_inet,prim_eval,init,otp_ring0]},
               {process_count,105},
               {node,nonode@nohost},
               {nodes,[]},
               {registered,
                   [lhttpc_sup,code_server,'sink-disk_default',
                    ale_stats_events,ns_server_cluster_sup,lhttpc_manager,
                    application_controller,ale,httpd_sup,release_handler,
                    kernel_safe_sup,standard_error,ale_sup,overload,
                    error_logger,alarm_handler,ale_dynamic_sup,timer_server,
                    standard_error_sup,crypto_server,crypto_sup,sasl_safe_sup,
                    tftp_sup,'sink-ns_log',inet_db,init,os_mon_sup,rex,
                    tls_connection_sup,user,ssl_sup,kernel_sup,cpu_sup,
                    global_name_server,memsup,disksup,httpc_sup,file_server_2,
                    'sink-disk_json_rpc',ssl_manager,local_tasks,global_group,
                    'sink-disk_metakv',httpc_profile_sup,httpc_manager,
                    'sink-disk_access_int',httpc_handler_sup,
                    'sink-disk_access',ftp_sup,'sink-disk_reports',sasl_sup,
                    'sink-disk_stats',erl_prim_loader,'sink-disk_xdcr',
                    'sink-disk_debug',inets_sup,'sink-disk_error']},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,1}]
[ns_server:info,2020-03-03T11:37:19.046+05:30,nonode@nohost:ns_server_cluster_sup<0.137.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"d200fc0f68695d4aef1fad5c3c8cc55f8c033014\" upstream=\"refs/tags/0.9.7\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"74a44a626e8e8aba2f2f62fb96aa25ddf6ee227b\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"d0710cefe032017800887fe68cbbb8a25a8f28a1\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"62e3145e2f8e30ccd46e460f400bf34dfb2ce4c8\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"9e3465560240ffb242b50a47cb7f19251a12ee42\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"f5f59722bc8b1015c35e98b26eb95ba64393cd90\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"f551b6d4f32bb920a83dd28c705bddd5de0d03b2\" />",
 "  <project name=\"blevex\" path=\"godeps/src/github.com/blevesearch/blevex\" remote=\"blevesearch\" revision=\"4b158bb555a3297565afecf6fae675c74f1e47df\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"dc55cf8e9613ddf1bd6db4a662d1ecc3668223da\" upstream=\"alice\">",
 "    <annotation name=\"RELEASE\" value=\"alice\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"3082\" />",
 "    <annotation name=\"VERSION\" value=\"6.0.4\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbaselabs/cbas\" remote=\"couchbase-priv\" revision=\"caccf4ec99780ad99ae43683df7b4868b1305e0a\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"b044bbb55b77baf1a55677727add910d9b099305\" upstream=\"alice\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"d8f601c3109887a30deccab09b2177bf499c8711\" upstream=\"alice\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"0df84c7e3c6d95ff435c12a3c08c6f064db11e97\" />",
 "  <project name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"80d2ad8892d806f5103f602fec0d80adaa4b628f\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"794a9aa0c45837797b9ee11628bf3a7baa02cc52\" upstream=\"alice\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"0a94f40b9080e0ecb11d3b7531a58c5e6a4a4465\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbq-gui\" path=\"goproj/src/github.com/couchbase/cbq-gui\" remote=\"couchbase-priv\" revision=\"93d26dbc120c4f9874abb65aafa4d815334273c6\" upstream=\"alice\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"dbfa1c0d73f0e49f6f04e390f03de8f9a6cee769\" />",
 "  <project name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"dcae66272b24600ae0005fa06b511cfae8914d3d\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" />",
 "  <project name=\"couchbase-cli\" revision=\"1333235b35a8faf8e68cc0a0327f9042232af1fb\" upstream=\"alice\" />",
 "  <project name=\"couchdb\" revision=\"8ccc45cdcc160f9896812965d61a29ddec6f69ba\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"4dc357bf919ec257cd87b05e7ea9f32de23f1b03\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"143858d76c865b10039436c3bfc723cbdf5c180f\" upstream=\"alice\" />",
 "  <project name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"f23ba3a5ee43012fcb4b92e1a2a405a92554f4f2\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"05067021a042a1b63e100a486afd7ebddab4c535\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" />",
 "  <project name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"6daf6338e56953bac41f3e5e5f3468ec3f31bc35\" upstream=\"alice\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"kv\" name=\"forestdb\" revision=\"caf0a806e2d1881c4b6558b06fae30adcff84d20\" upstream=\"vulcan\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"2a0e73f43451045f157640eec59ced72da18471f\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"5c0ac9ef0ba7a0356279cee7eadd8874c4a8f8c1\" upstream=\"alice\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" />",
 "  <project name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"47fc4e5e9153645da45af6a86a5bce95e63a0f9e\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d2f15a425a9c8e4d8447e5f5b89ce14845f7fa05\" upstream=\"vulcan\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" />",
 "  <project name=\"gocb\" path=\"godeps/src/github.com/couchbase/gocb\" revision=\"699b13a51af5dd4f80ff3deedf41bba60debad32\" upstream=\"refs/tags/v1.3.7\" />",
 "  <project name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"710456e087a6d497e87f41d0a9d98d6a75672186\" />",
 "  <project name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"a0d26c2d6f5de912499d35a5aba573006e5e036f\" upstream=\"refs/tags/v7.1.7\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"aecdbe5a5a91f0688df7bdf260ca962178c06828\" upstream=\"vulcan\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"e05ec3550789be1da2981787ef6444fef75e10bc\" upstream=\"vulcan\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"4809dd3aa2ab88013b1dfce15516d717a35cb6e8\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"f98adca8eb365032cab838ef4d99453931afa112\" upstream=\"vulcan\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"74af34a22201c501af7ac4116be5d662b309c38e\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"e4cb4e14e5db0ed0205270652c7626d07ca8cf78\" upstream=\"alice\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"c4ede32c575a231e71d82f5fee71887e14ef22b5\" upstream=\"alice\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"081e8b16b991bf706eb77f8243935c6fba31b895\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"3681c2a912330352991ecdd642f257efe5b85518\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"956632ec1bc3e28276d00ee2f22c3202f06efb12\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"abd3b58b422dbc2e9463a589d0f3d93441726e23\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project groups=\"kv\" name=\"moxi\" revision=\"cd8da46b9b953800d430c8b0aa4667790727ed6f\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"62685c2d7ca23c807425dca88b11a3e2323dab41\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"f3bef3551997be504612a2d05a8b324b3bfdfe1b\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"4494252972a2a00d8039d42211e6cfb5088295aa\" upstream=\"alice\" />",
 "  <project name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"96501c57bb0fd61c85cba6f63101aed2bcf41d38\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"937e735fe17ea2e9dcf522f25ec0b9d44e30128d\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"de77d527bd23012dc67794c81a7b1e76e46e3abd\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"8cf7d25a1dcb5434ea44983c0bed202fb9cc8351\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"655cdfa588ea190e901bc5590e65d5621688847c\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"01ccaa4589911755e95e3ef0270e2d52e6d965d0\" upstream=\"alice\" />",
 "  <project name=\"query-ui\" revision=\"15a6461f437fe810e411a6613ec7c143991cd1c6\" upstream=\"alice\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"fe09428be4c233d726797a1380f7438f4f71a31a\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"73353fe6dad8f3d67409feefb9b17f90f6de917b\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"4fba14c79f356ae48d6141c561bf9fd7ba33fabd\" upstream=\"refs/tags/v0.14.0\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"c30c3d4c250e68e81c57aa1e8ae91ffd21243cdb\" />",
 "  <project name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"9d4e42a20653790449273b3c85e67d6d8bae6e2e\" />",
 "  <project name=\"testrunner\" revision=\"3f6036c76c0e8d09921016d80fa6b7c3c28365cd\" upstream=\"alice\" />",
 "  <project name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"601048ad6acbab6cedd582db09b8c4839ff25b15\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"6dadf98e19803cc10ce9552bf4d9121cc2528965\" upstream=\"alice\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"0ceea4a37442f76199b9259840baf48d17af3c1a\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"62f7f8c3f9923bcccd6ed07c0bbfddc3280b3384\" upstream=\"alice\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-03-03T11:37:19.091+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{set_lock,{me,<0.139.0>},
          {{code_version,time_compat},<0.139.0>},
          {nodes,[nonode@nohost]},
          {retries,infinity},
          {times,1}}
[error_logger:info,2020-03-03T11:37:19.091+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{handle_set_lock,{{code_version,time_compat},<0.139.0>},<0.139.0>}
[error_logger:info,2020-03-03T11:37:19.092+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{set_lock,{me,<0.139.0>},
          {{code_version,time_compat},<0.139.0>},
          {nodes,[nonode@nohost]},
          {replies,[{nonode@nohost,true}]}}
[error_logger:info,2020-03-03T11:37:19.092+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{set_lock_true,{{code_version,time_compat},<0.139.0>}}
[error_logger:info,2020-03-03T11:37:19.093+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{del_lock,{me,<0.139.0>},
          {{code_version,time_compat},<0.139.0>},
          {nodes,[nonode@nohost]}}
[error_logger:info,2020-03-03T11:37:19.093+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{handle_del_lock,{pid,<0.139.0>},{id,{{code_version,time_compat},<0.139.0>}}}
[error_logger:info,2020-03-03T11:37:19.093+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{remove_lock_1,{id,{code_version,time_compat}},{pid,<0.139.0>}}
[error_logger:info,2020-03-03T11:37:19.093+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.139.0>},
                       {name,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:37:19.095+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:read_address_config_from_path:159]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-03-03T11:37:19.095+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:read_address_config_from_path:159]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[error_logger:info,2020-03-03T11:37:19.095+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.143.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2020-03-03T11:37:19.095+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.142.0>},
                       {name,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-03-03T11:37:24.116+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:37:24.116+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:37:30.122+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:37:30.122+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:37:36.123+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:37:36.123+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:37:42.124+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:37:42.124+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:37:48.130+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:37:48.130+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:37:54.134+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:37:54.134+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:38:00.138+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:38:00.138+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:38:06.142+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:38:06.142+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:38:12.148+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:38:12.148+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:38:18.149+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:38:18.149+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:error,2020-03-03T11:38:19.150+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:init:258]Configured address `cb.local` seems to be invalid. Will refuse to start for safety reasons.
[ns_server:info,2020-03-03T11:38:21.084+05:30,nonode@nohost:<0.89.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-03-03T11:38:21.109+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-03-03T11:38:21.109+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.109+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.109+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.109+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.109+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.109+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.109+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.109+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.109+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.109+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.110+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.110+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.110+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.110+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.110+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.110+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.110+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.110+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.110+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.110+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.110+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.110+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.110+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.110+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-03-03T11:38:21.110+05:30,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[error_logger:info,2020-03-03T11:38:21.122+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.138.0>},
                       {name,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:38:21.143+05:30,nonode@nohost:ns_server_cluster_sup<0.137.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"R16B03-1"},
               {erl_version,"5.10.4.0.0.1"},
               {erl_version_long,
                   "Erlang R16B03-1 (erts-5.10.4.0.0.1) [source-378cfabb7f] [64-bit] [smp:4:4] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,3,3},{11,38,21}}},
               {memory,
                   [{total,29104368},
                    {processes,9434528},
                    {processes_used,9432456},
                    {system,19669840},
                    {atom,339441},
                    {atom_used,322610},
                    {binary,72624},
                    {code,7796727},
                    {ets,2241720}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,path_config,calendar,
                    ale_default_formatter,'ale_logger-metakv',
                    'ale_logger-rebalance','ale_logger-menelaus',
                    'ale_logger-stats','ale_logger-json_rpc',
                    'ale_logger-access','ale_logger-ns_server',
                    'ale_logger-user','ale_logger-ns_doctor',
                    'ale_logger-cluster','ale_logger-xdcr',otp_internal,
                    ns_log_sink,ale_disk_sink,misc,couch_util,io_lib_fread,
                    ns_server,filelib,cpu_sup,memsup,disksup,os_mon,io,
                    release_handler,overload,alarm_handler,sasl,timer,
                    tftp_sup,httpd_sup,httpc_handler_sup,httpc_cookie,
                    inets_trace,httpc_manager,httpc,httpc_profile_sup,
                    httpc_sup,ftp_sup,inets_sup,inets_app,ssl,lhttpc_manager,
                    lhttpc_sup,lhttpc,tls_connection_sup,ssl_session_cache,
                    ssl_pkix_db,ssl_manager,ssl_sup,ssl_app,crypto_server,
                    crypto_sup,crypto_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,beam_dict,beam_asm,beam_validator,beam_z,
                    beam_flatten,beam_trim,beam_receive,beam_bsm,beam_peep,
                    beam_dead,beam_split,beam_type,beam_bool,beam_except,
                    beam_clean,beam_utils,beam_block,beam_jump,beam_a,
                    v3_codegen,v3_life,v3_kernel,sys_core_dsetel,erl_bifs,
                    sys_core_fold,cerl_trees,sys_core_inline,core_lib,cerl,
                    v3_core,erl_bits,erl_expand_records,sys_pre_expand,sofs,
                    erl_internal,sets,ordsets,erl_lint,compile,
                    dynamic_compile,ale_utils,io_lib_pretty,io_lib_format,
                    io_lib,ale_codegen,dict,ale,ale_dynamic_sup,ale_sup,
                    ale_app,epp,ns_bootstrap,child_erlang,file_io_server,
                    orddict,erl_eval,file,c,kernel_config,user_io,user_sup,
                    supervisor_bridge,standard_error,code_server,unicode,
                    hipe_unified_loader,gb_sets,ets,binary,code,file_server,
                    net_kernel,global_group,erl_distribution,filename,
                    inet_gethost_native,os,inet_parse,inet,inet_udp,
                    inet_config,inet_db,global,gb_trees,rpc,supervisor,kernel,
                    application_master,sys,application,gen_server,erl_parse,
                    proplists,erl_scan,lists,application_controller,proc_lib,
                    gen,gen_event,error_logger,heart,error_handler,
                    erts_internal,erlang,erl_prim_loader,prim_zip,zlib,
                    prim_file,prim_inet,prim_eval,init,otp_ring0]},
               {applications,
                   [{lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {os_mon,"CPO  CXC 138 46","2.2.14"},
                    {public_key,"Public key infrastructure","0.21"},
                    {asn1,"The Erlang ASN1 compiler version 2.0.4","2.0.4"},
                    {kernel,"ERTS  CXC 138 10","2.16.4"},
                    {ale,"Another Logger for Erlang","6.0.4-3082-enterprise"},
                    {inets,"INETS  CXC 138 49","5.9.8"},
                    {ns_server,"Couchbase server","6.0.4-3082-enterprise"},
                    {crypto,"CRYPTO version 2","3.2"},
                    {ssl,"Erlang/OTP SSL application","5.3.3"},
                    {sasl,"SASL  CXC 138 11","2.3.4"},
                    {stdlib,"ERTS  CXC 138 10","1.19.4"}]},
               {pre_loaded,
                   [erts_internal,erlang,erl_prim_loader,prim_zip,zlib,
                    prim_file,prim_inet,prim_eval,init,otp_ring0]},
               {process_count,105},
               {node,nonode@nohost},
               {nodes,[]},
               {registered,
                   [lhttpc_sup,code_server,'sink-disk_default',
                    ale_stats_events,ns_server_cluster_sup,lhttpc_manager,
                    application_controller,ale,httpd_sup,release_handler,
                    kernel_safe_sup,standard_error,ale_sup,overload,
                    error_logger,alarm_handler,ale_dynamic_sup,timer_server,
                    standard_error_sup,crypto_server,crypto_sup,sasl_safe_sup,
                    tftp_sup,'sink-ns_log',inet_db,init,os_mon_sup,rex,
                    tls_connection_sup,user,ssl_sup,kernel_sup,cpu_sup,
                    global_name_server,memsup,disksup,httpc_sup,file_server_2,
                    'sink-disk_json_rpc',ssl_manager,local_tasks,global_group,
                    'sink-disk_metakv',httpc_profile_sup,httpc_manager,
                    'sink-disk_access_int',httpc_handler_sup,
                    'sink-disk_access',ftp_sup,'sink-disk_reports',sasl_sup,
                    'sink-disk_stats',erl_prim_loader,'sink-disk_xdcr',
                    'sink-disk_debug',inets_sup,'sink-disk_error']},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,1}]
[ns_server:info,2020-03-03T11:38:21.147+05:30,nonode@nohost:ns_server_cluster_sup<0.137.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"d200fc0f68695d4aef1fad5c3c8cc55f8c033014\" upstream=\"refs/tags/0.9.7\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"74a44a626e8e8aba2f2f62fb96aa25ddf6ee227b\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"d0710cefe032017800887fe68cbbb8a25a8f28a1\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"62e3145e2f8e30ccd46e460f400bf34dfb2ce4c8\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"9e3465560240ffb242b50a47cb7f19251a12ee42\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"f5f59722bc8b1015c35e98b26eb95ba64393cd90\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"f551b6d4f32bb920a83dd28c705bddd5de0d03b2\" />",
 "  <project name=\"blevex\" path=\"godeps/src/github.com/blevesearch/blevex\" remote=\"blevesearch\" revision=\"4b158bb555a3297565afecf6fae675c74f1e47df\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"dc55cf8e9613ddf1bd6db4a662d1ecc3668223da\" upstream=\"alice\">",
 "    <annotation name=\"RELEASE\" value=\"alice\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"3082\" />",
 "    <annotation name=\"VERSION\" value=\"6.0.4\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbaselabs/cbas\" remote=\"couchbase-priv\" revision=\"caccf4ec99780ad99ae43683df7b4868b1305e0a\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"b044bbb55b77baf1a55677727add910d9b099305\" upstream=\"alice\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"d8f601c3109887a30deccab09b2177bf499c8711\" upstream=\"alice\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"0df84c7e3c6d95ff435c12a3c08c6f064db11e97\" />",
 "  <project name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"80d2ad8892d806f5103f602fec0d80adaa4b628f\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"794a9aa0c45837797b9ee11628bf3a7baa02cc52\" upstream=\"alice\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"0a94f40b9080e0ecb11d3b7531a58c5e6a4a4465\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbq-gui\" path=\"goproj/src/github.com/couchbase/cbq-gui\" remote=\"couchbase-priv\" revision=\"93d26dbc120c4f9874abb65aafa4d815334273c6\" upstream=\"alice\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"dbfa1c0d73f0e49f6f04e390f03de8f9a6cee769\" />",
 "  <project name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"dcae66272b24600ae0005fa06b511cfae8914d3d\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" />",
 "  <project name=\"couchbase-cli\" revision=\"1333235b35a8faf8e68cc0a0327f9042232af1fb\" upstream=\"alice\" />",
 "  <project name=\"couchdb\" revision=\"8ccc45cdcc160f9896812965d61a29ddec6f69ba\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"4dc357bf919ec257cd87b05e7ea9f32de23f1b03\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"143858d76c865b10039436c3bfc723cbdf5c180f\" upstream=\"alice\" />",
 "  <project name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"f23ba3a5ee43012fcb4b92e1a2a405a92554f4f2\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"05067021a042a1b63e100a486afd7ebddab4c535\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" />",
 "  <project name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"6daf6338e56953bac41f3e5e5f3468ec3f31bc35\" upstream=\"alice\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"kv\" name=\"forestdb\" revision=\"caf0a806e2d1881c4b6558b06fae30adcff84d20\" upstream=\"vulcan\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"2a0e73f43451045f157640eec59ced72da18471f\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"5c0ac9ef0ba7a0356279cee7eadd8874c4a8f8c1\" upstream=\"alice\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" />",
 "  <project name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"47fc4e5e9153645da45af6a86a5bce95e63a0f9e\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d2f15a425a9c8e4d8447e5f5b89ce14845f7fa05\" upstream=\"vulcan\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" />",
 "  <project name=\"gocb\" path=\"godeps/src/github.com/couchbase/gocb\" revision=\"699b13a51af5dd4f80ff3deedf41bba60debad32\" upstream=\"refs/tags/v1.3.7\" />",
 "  <project name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"710456e087a6d497e87f41d0a9d98d6a75672186\" />",
 "  <project name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"a0d26c2d6f5de912499d35a5aba573006e5e036f\" upstream=\"refs/tags/v7.1.7\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"aecdbe5a5a91f0688df7bdf260ca962178c06828\" upstream=\"vulcan\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"e05ec3550789be1da2981787ef6444fef75e10bc\" upstream=\"vulcan\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"4809dd3aa2ab88013b1dfce15516d717a35cb6e8\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"f98adca8eb365032cab838ef4d99453931afa112\" upstream=\"vulcan\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"74af34a22201c501af7ac4116be5d662b309c38e\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"e4cb4e14e5db0ed0205270652c7626d07ca8cf78\" upstream=\"alice\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"c4ede32c575a231e71d82f5fee71887e14ef22b5\" upstream=\"alice\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"081e8b16b991bf706eb77f8243935c6fba31b895\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"3681c2a912330352991ecdd642f257efe5b85518\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"956632ec1bc3e28276d00ee2f22c3202f06efb12\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"abd3b58b422dbc2e9463a589d0f3d93441726e23\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project groups=\"kv\" name=\"moxi\" revision=\"cd8da46b9b953800d430c8b0aa4667790727ed6f\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"62685c2d7ca23c807425dca88b11a3e2323dab41\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"f3bef3551997be504612a2d05a8b324b3bfdfe1b\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"4494252972a2a00d8039d42211e6cfb5088295aa\" upstream=\"alice\" />",
 "  <project name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"96501c57bb0fd61c85cba6f63101aed2bcf41d38\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"937e735fe17ea2e9dcf522f25ec0b9d44e30128d\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"de77d527bd23012dc67794c81a7b1e76e46e3abd\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"8cf7d25a1dcb5434ea44983c0bed202fb9cc8351\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"655cdfa588ea190e901bc5590e65d5621688847c\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"01ccaa4589911755e95e3ef0270e2d52e6d965d0\" upstream=\"alice\" />",
 "  <project name=\"query-ui\" revision=\"15a6461f437fe810e411a6613ec7c143991cd1c6\" upstream=\"alice\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"fe09428be4c233d726797a1380f7438f4f71a31a\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"73353fe6dad8f3d67409feefb9b17f90f6de917b\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"4fba14c79f356ae48d6141c561bf9fd7ba33fabd\" upstream=\"refs/tags/v0.14.0\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"c30c3d4c250e68e81c57aa1e8ae91ffd21243cdb\" />",
 "  <project name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"9d4e42a20653790449273b3c85e67d6d8bae6e2e\" />",
 "  <project name=\"testrunner\" revision=\"3f6036c76c0e8d09921016d80fa6b7c3c28365cd\" upstream=\"alice\" />",
 "  <project name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"601048ad6acbab6cedd582db09b8c4839ff25b15\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"6dadf98e19803cc10ce9552bf4d9121cc2528965\" upstream=\"alice\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"0ceea4a37442f76199b9259840baf48d17af3c1a\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"62f7f8c3f9923bcccd6ed07c0bbfddc3280b3384\" upstream=\"alice\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-03-03T11:38:21.184+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{set_lock,{me,<0.139.0>},
          {{code_version,time_compat},<0.139.0>},
          {nodes,[nonode@nohost]},
          {retries,infinity},
          {times,1}}
[error_logger:info,2020-03-03T11:38:21.184+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{handle_set_lock,{{code_version,time_compat},<0.139.0>},<0.139.0>}
[error_logger:info,2020-03-03T11:38:21.184+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{set_lock,{me,<0.139.0>},
          {{code_version,time_compat},<0.139.0>},
          {nodes,[nonode@nohost]},
          {replies,[{nonode@nohost,true}]}}
[error_logger:info,2020-03-03T11:38:21.185+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{set_lock_true,{{code_version,time_compat},<0.139.0>}}
[error_logger:info,2020-03-03T11:38:21.185+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{del_lock,{me,<0.139.0>},
          {{code_version,time_compat},<0.139.0>},
          {nodes,[nonode@nohost]}}
[error_logger:info,2020-03-03T11:38:21.185+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{handle_del_lock,{pid,<0.139.0>},{id,{{code_version,time_compat},<0.139.0>}}}
[error_logger:info,2020-03-03T11:38:21.186+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]global_trace:
{remove_lock_1,{id,{code_version,time_compat}},{pid,<0.139.0>}}
[error_logger:info,2020-03-03T11:38:21.186+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.139.0>},
                       {name,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-03T11:38:21.194+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:read_address_config_from_path:159]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-03-03T11:38:21.194+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:read_address_config_from_path:159]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[error_logger:info,2020-03-03T11:38:21.206+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.143.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2020-03-03T11:38:21.206+05:30,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.142.0>},
                       {name,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-03-03T11:38:26.215+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:38:26.215+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:38:32.218+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:38:32.218+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:38:38.220+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:38:38.220+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:38:44.222+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:38:44.222+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:38:50.226+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:38:50.226+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:warn,2020-03-03T11:38:56.230+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:198]Could not resolve address `cb.local`: nxdomain
[ns_server:info,2020-03-03T11:38:56.230+05:30,nonode@nohost:dist_manager<0.141.0>:dist_manager:wait_for_address:205]Configured address `cb.local` seems to be invalid. Giving OS a chance to bring it up.
[ns_server:info,2020-04-02T20:08:14.603+05:30,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-04-02T20:08:14.615+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-04-02T20:08:14.615+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.615+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.615+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.615+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.615+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.615+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.615+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.615+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.616+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.616+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.616+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.616+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.616+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.616+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.616+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.616+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.616+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.616+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.616+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.616+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.616+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.616+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.616+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.616+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.616+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-04-02T20:08:14.616+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-04-02T20:08:14.620+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-04-02T20:08:14.620+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[error_logger:info,2020-04-02T20:08:14.621+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.168.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2020-04-02T20:08:14.621+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.167.0>},
                       {id,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:08:14.623+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@127.0.0.1'
[error_logger:info,2020-04-02T20:08:14.631+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.631+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.631+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:14.632+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.174.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.634+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.176.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:08:14.634+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2020-04-02T20:08:14.634+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.177.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:14.634+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.175.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:14.634+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:14.635+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.635+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.636+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.180.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:14.637+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-04-02T20:08:14.637+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-04-02T20:08:14.637+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2020-04-02T20:08:14.638+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-04-02T20:08:14.638+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.181.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.638+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.169.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-04-02T20:08:14.639+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-04-02T20:08:14.643+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-04-02T20:08:14.643+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2020-04-02T20:08:14.643+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:08:14.643+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T20:08:14.643+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1520182418.164364289.186385>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T20:08:14.643+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1520182418.164364289.186385>,
                                  inet_tcp_dist,<0.185.0>,
                                  #Ref<0.1520182418.164364289.186390>}
[ns_server:debug,2020-04-02T20:08:14.646+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-04-02T20:08:14.646+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": ok
[ns_server:info,2020-04-02T20:08:14.646+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-04-02T20:08:14.648+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-04-02T20:08:14.648+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.659+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:08:14.663+05:30,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,4,2},{20,8,14}}},
               {memory,
                   [{total,26501960},
                    {processes,9676440},
                    {processes_used,9671328},
                    {system,16825520},
                    {atom,388625},
                    {atom_used,364409},
                    {binary,134992},
                    {code,8250921},
                    {ets,1520352}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    error_handler,error_logger,application,file,file_server,
                    code,code_server,application_master,heart,
                    application_controller,file_io_server,kernel,supervisor,
                    gen_event,filename,erl_eval,erl_lint,gen_server,ets,gen,
                    lists,proc_lib,erl_parse,erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{os_mon,"CPO  CXC 138 46","2.4.4"},
                    {sasl,"SASL  CXC 138 11","3.1.2"},
                    {ns_server,"Couchbase server","6.5.0-4966-community"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ale,"Another Logger for Erlang","0.0.0"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,131},
               {node,'ns_1@127.0.0.1'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,auth,httpd_sup,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,kernel_safe_sup,tls_connection_sup,
                    sasl_sup,release_handler,lhttpc_sup,httpc_sup,
                    lhttpc_manager,alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    'sink-disk_xdcr',timer_server,'sink-disk_debug',
                    inet_gethost_native,ale_sup,'sink-disk_error',inet_db,
                    'sink-disk_default',ssl_pem_cache_dist,ale_dynamic_sup,
                    rex,global_group,net_sup,kernel_sup,ssl_connection_sup,
                    global_name_server,ssl_admin_sup,tftp_sup,ssl_sup,
                    root_sup,erts_code_purger,os_mon_sup,file_server_2,
                    error_logger,cpu_sup,erl_epmd,init,memsup,
                    erl_signal_server,disksup,ale,net_kernel,dist_manager,
                    ssl_pem_cache,ssl_manager,ssl_dist_admin_sup,
                    ssl_dist_connection_sup,ssl_dist_sup,user,
                    ssl_tls_dist_proxy,ssl_manager_dist,sasl_safe_sup,
                    ssl_listen_tracker_sup,inet_gethost_native_sup,
                    code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,0}]
[ns_server:info,2020-04-02T20:08:14.671+05:30,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"21e0ed4ef2e27d16585f31e4458a1db0546bbb05\" upstream=\"6.5.0\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"5716bf0df2d36db8ff45c6508a328a92b9457cbf\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4966\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"74c19969e8ef1b5309077a03885d00e273378f6c\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"956a2df5f2f2abb48054bc4ce56895ce9618d2ae\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-04-02T20:08:14.677+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.679+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.679+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:08:14.680+05:30,ns_1@127.0.0.1:ns_config_sup<0.192.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-04-02T20:08:14.680+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.680+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.194.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:08:14.702+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-04-02T20:08:14.703+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:info,2020-04-02T20:08:14.703+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1125]No dynamic config file found. Assuming we're brand new node
[ns_server:debug,2020-04-02T20:08:14.706+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[]]
[ns_server:info,2020-04-02T20:08:14.708+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',cbas_debug_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|-1]},
 {{node,'ns_1@127.0.0.1',cbas_parent_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9122]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9121]},
 {{node,'ns_1@127.0.0.1',cbas_replication_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9120]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9119]},
 {{node,'ns_1@127.0.0.1',cbas_messaging_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9118]},
 {{node,'ns_1@127.0.0.1',cbas_result_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9117]},
 {{node,'ns_1@127.0.0.1',cbas_data_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9116]},
 {{node,'ns_1@127.0.0.1',cbas_cluster_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9115]},
 {{node,'ns_1@127.0.0.1',cbas_console_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9114]},
 {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9113]},
 {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9112]},
 {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9111]},
 {{node,'ns_1@127.0.0.1',cbas_admin_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9110]},
 {{node,'ns_1@127.0.0.1',cbas_ssl_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',cbas_http_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   8095]},
 {{node,'ns_1@127.0.0.1',eventing_https_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',eventing_debug_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9140]},
 {{node,'ns_1@127.0.0.1',eventing_http_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   8096]},
 {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',fts_grpc_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9130]},
 {{node,'ns_1@127.0.0.1',fts_ssl_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',fts_http_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   8094]},
 {{node,'ns_1@127.0.0.1',indexer_https_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9105]},
 {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9104]},
 {{node,'ns_1@127.0.0.1',indexer_stinit_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9103]},
 {{node,'ns_1@127.0.0.1',indexer_http_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9102]},
 {{node,'ns_1@127.0.0.1',indexer_scan_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9101]},
 {{node,'ns_1@127.0.0.1',indexer_admin_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9100]},
 {{node,'ns_1@127.0.0.1',ssl_query_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',query_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   8093]},
 {{node,'ns_1@127.0.0.1',projector_ssl_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',projector_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',ssl_capi_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',capi_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   8092]},
 {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',xdcr_rest_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   9998]},
 {{node,'ns_1@127.0.0.1',ssl_rest_port},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',rest},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]},
   {port,8091},
   {port_meta,global}]},
 {rest,[{port,8091}]},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {drop_request_memory_threshold_mib,undefined},
 {{request_limit,capi},undefined},
 {{request_limit,rest},undefined},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {replication,[{enabled,true}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {{node,'ns_1@127.0.0.1',ns_log},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@127.0.0.1',port_servers},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}]},
 {{node,'ns_1@127.0.0.1',moxi},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]},
   {port,0}]},
 {secure_headers,[]},
 {buckets,[{configs,[]}]},
 {cbas_memory_quota,2174},
 {fts_memory_quota,512},
 {memory_quota,8886},
 {{node,'ns_1@127.0.0.1',memcached_config},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@127.0.0.1',memcached},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,undefined},
   {ssl_port,undefined},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@127.0.0.1',memcached_defaults},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,false},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {memcached,[]},
 {{node,'ns_1@127.0.0.1',audit},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {{node,'ns_1@127.0.0.1',isasl},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {remote_clusters,[]},
 {rest_creds,null},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":5,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {{node,'ns_1@127.0.0.1',membership},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   active]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
 {quorum_nodes,['ns_1@127.0.0.1']},
 {nodes_wanted,['ns_1@127.0.0.1']},
 {{node,'ns_1@127.0.0.1',compaction_daemon},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {max_bucket_count,30},
 {index_aware_rebalance_disabled,false},
 {{node,'ns_1@127.0.0.1',saslauthd_enabled},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',is_enterprise},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',config_version},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   {6,5}]},
 {{node,'ns_1@127.0.0.1',uuid},
  [{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
   <<"9245db3c028cb7987096449d46433aa1">>]}]
[error_logger:info,2020-04-02T20:08:14.711+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.195.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.712+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.201.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.712+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.202.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.713+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:14.714+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.204.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:08:14.714+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',erl_external_listeners} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2020-04-02T20:08:14.714+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',node_encryption} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|false]
[ns_server:debug,2020-04-02T20:08:14.714+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',address_family} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|inet]
[ns_server:debug,2020-04-02T20:08:14.714+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"9245db3c028cb7987096449d46433aa1">>} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}]
[error_logger:info,2020-04-02T20:08:14.715+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.207.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:14.730+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:14.732+05:30,ns_1@127.0.0.1:menelaus_barrier<0.211.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-04-02T20:08:14.733+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.733+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.736+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.213.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.737+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.215.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.737+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.214.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:14.744+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.217.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.745+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.220.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.749+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.222.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:14.750+05:30,ns_1@127.0.0.1:users_replicator<0.222.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-04-02T20:08:14.751+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.222.0>
[ns_server:debug,2020-04-02T20:08:14.752+05:30,ns_1@127.0.0.1:users_replicator<0.222.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.223.0>
[ns_server:debug,2020-04-02T20:08:14.753+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-04-02T20:08:14.753+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.223.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.753+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.221.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:08:14.754+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.225.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-04-02T20:08:14.754+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.225.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.759+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.228.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.759+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.219.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:14.761+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.232.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:14.761+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.233.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:08:14.769+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-04-02T20:08:14.770+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 16ms
[ns_server:debug,2020-04-02T20:08:14.771+05:30,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.236.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[error_logger:info,2020-04-02T20:08:14.771+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.231.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:14.771+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:08:14.771+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T20:08:14.771+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1520182418.164364292.185692>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T20:08:14.771+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1520182418.164364292.185692>,
                                  inet_tcp_dist,<0.239.0>,
                                  #Ref<0.1520182418.164364292.185696>}
[ns_server:debug,2020-04-02T20:08:14.771+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1520182418.164364292.185692>,
                               inet_tcp_dist,<0.239.0>,
                               #Ref<0.1520182418.164364292.185696>}
[ns_server:debug,2020-04-02T20:08:14.771+05:30,ns_1@127.0.0.1:<0.237.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-04-02T20:08:14.771+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.239.0>,shutdown}}
[error_logger:info,2020-04-02T20:08:14.771+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:08:14.772+05:30,ns_1@127.0.0.1:users_replicator<0.222.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2020-04-02T20:08:14.972+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:08:14.972+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T20:08:14.972+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1520182418.164364289.186431>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T20:08:14.972+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1520182418.164364289.186431>,
                                  inet_tcp_dist,<0.242.0>,
                                  #Ref<0.1520182418.164364289.186433>}
[ns_server:debug,2020-04-02T20:08:14.999+05:30,ns_1@127.0.0.1:<0.237.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-04-02T20:08:15.200+05:30,ns_1@127.0.0.1:<0.237.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-04-02T20:08:15.413+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.246.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.543+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.236.0>},
                       {name,wait_for_couchdb_node},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.555+05:30,ns_1@127.0.0.1:ns_server_nodes_sup<0.209.0>:ns_storage_conf:setup_db_and_ix_paths:64]Initialize db_and_ix_paths variable with [{db_path,
                                           "/opt/couchbase/var/lib/couchbase/data"},
                                          {index_path,
                                           "/opt/couchbase/var/lib/couchbase/data"}]
[ns_server:debug,2020-04-02T20:08:15.555+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"9245db3c028cb7987096449d46433aa1">>} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{2,63753057495}}]}]
[ns_server:debug,2020-04-02T20:08:15.555+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_dirs} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057495}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2020-04-02T20:08:15.555+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"9245db3c028cb7987096449d46433aa1">>} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{3,63753057495}}]}]
[ns_server:debug,2020-04-02T20:08:15.556+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_dir} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057495}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[error_logger:info,2020-04-02T20:08:15.558+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.250.0>},
                       {name,ns_disksup},
                       {mfargs,{ns_disksup,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.559+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.252.0>},
                       {name,diag_handler_worker},
                       {mfargs,{work_queue,start_link,[diag_handler_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:08:15.560+05:30,ns_1@127.0.0.1:ns_server_sup<0.249.0>:dir_size:start_link:39]Starting quick version of dir_size with program name: godu
[error_logger:info,2020-04-02T20:08:15.560+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.253.0>},
                       {name,dir_size},
                       {mfargs,{dir_size,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.567+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.254.0>},
                       {name,request_throttler},
                       {mfargs,{request_throttler,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-04-02T20:08:15.583+05:30,ns_1@127.0.0.1:ns_log<0.255.0>:ns_log:read_logs:91]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/ns_log" (perhaps it's first startup): {error,
                                                                                                 enoent}
[error_logger:info,2020-04-02T20:08:15.583+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.255.0>},
                       {name,ns_log},
                       {mfargs,{ns_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.584+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.256.0>},
                       {name,ns_crash_log_consumer},
                       {mfargs,{ns_log,start_link_crash_consumer,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[user:info,2020-04-02T20:08:15.589+05:30,ns_1@127.0.0.1:<0.256.0>:ns_log:crash_consumption_loop:69]Service 'ns_server' exited with status 3. Restarting. Messages:
[os_mon] memory supervisor port (memsup): Erlang has closed
[os_mon] cpu supervisor port (cpu_sup): Erlang has closed
Crap error:{badmatch,
            {error,
             {{shutdown,
               {failed_to_start_child,ns_server_cluster_sup,
                {shutdown,
                 {failed_to_start_child,ns_server_nodes_sup,
                  {shutdown,
                   {failed_to_start_child,wait_for_couchdb_node,
                    {abnormal,1}}}}}}},
              {ns_server,start,[normal,[]]}}}}
[{ns_bootstrap,start,0,[{file,"src/ns_bootstrap.erl"},{line,36}]},
 {child_erlang,do_child_start,1,[{file,"src/child_erlang.erl"},{line,113}]},
 {child_erlang,child_start,1,[{file,"src/child_erlang.erl"},{line,91}]},
 {init,start_em,1,[]},
 {init,do_boot,3,[]}]
[ns_server:debug,2020-04-02T20:08:15.589+05:30,ns_1@127.0.0.1:memcached_passwords<0.257.0>:memcached_cfg:init:62]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-04-02T20:08:15.590+05:30,ns_1@127.0.0.1:memcached_passwords<0.257.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[user:info,2020-04-02T20:08:15.591+05:30,ns_1@127.0.0.1:<0.256.0>:ns_log:crash_consumption_loop:69]Service 'ns_server' exited with status 3. Restarting. Messages:
[os_mon] memory supervisor port (memsup): Erlang has closed
[os_mon] cpu supervisor port (cpu_sup): Erlang has closed
Crap error:{badmatch,
            {error,
             {{shutdown,
               {failed_to_start_child,ns_server_cluster_sup,
                {shutdown,
                 {failed_to_start_child,ns_server_nodes_sup,
                  {shutdown,
                   {failed_to_start_child,wait_for_couchdb_node,
                    {abnormal,1}}}}}}},
              {ns_server,start,[normal,[]]}}}}
[{ns_bootstrap,start,0,[{file,"src/ns_bootstrap.erl"},{line,36}]},
 {child_erlang,do_child_start,1,[{file,"src/child_erlang.erl"},{line,113}]},
 {child_erlang,child_start,1,[{file,"src/child_erlang.erl"},{line,91}]},
 {init,start_em,1,[]},
 {init,do_boot,3,[]}]
[ns_server:info,2020-04-02T20:08:15.591+05:30,ns_1@127.0.0.1:ns_log<0.255.0>:ns_log:handle_cast:151]suppressing duplicate log ns_log:undefined([<<"Service 'ns_server' exited with status 3. Restarting. Messages:\n[os_mon] memory supervisor port (memsup): Erlang has closed\n[os_mon] cpu supervisor port (cpu_sup): Erlang has closed\nCrap error:{badmatch,\n            {error,\n             {{shutdown,\n               {failed_to_start_child,ns_server_cluster_sup,\n                {shutdown,\n                 {failed_to_start_child,ns_server_nodes_sup,\n                  {shutdown,\n                   {failed_to_start_child,wait_for_couchdb_node,\n                    {abnormal,1}}}}}}},\n              {ns_server,start,[normal,[]]}}}}\n[{ns_bootstrap,start,0,[{file,\"src/ns_bootstrap.erl\"},{line,36}]},\n {child_erlang,do_child_start,1,[{file,\"src/child_erlang.erl\"},{line,113}]},\n {child_erlang,child_start,1,[{file,\"src/child_erlang.erl\"},{line,91}]},\n {init,start_em,1,[]},\n {init,do_boot,3,[]}]">>]) because it's been seen 1 times in the past 0.001999 secs (last seen 0.001999 secs ago
[user:info,2020-04-02T20:08:15.593+05:30,ns_1@127.0.0.1:<0.256.0>:ns_log:crash_consumption_loop:69]Service 'ns_server' exited with status 3. Restarting. Messages:
[os_mon] memory supervisor port (memsup): Erlang has closed
[os_mon] cpu supervisor port (cpu_sup): Erlang has closed
Crap error:{badmatch,
            {error,
             {{shutdown,
               {failed_to_start_child,ns_server_cluster_sup,
                {shutdown,
                 {failed_to_start_child,ns_server_nodes_sup,
                  {shutdown,
                   {failed_to_start_child,wait_for_couchdb_node,
                    {abnormal,1}}}}}}},
              {ns_server,start,[normal,[]]}}}}
[{ns_bootstrap,start,0,[{file,"src/ns_bootstrap.erl"},{line,36}]},
 {child_erlang,do_child_start,1,[{file,"src/child_erlang.erl"},{line,113}]},
 {child_erlang,child_start,1,[{file,"src/child_erlang.erl"},{line,91}]},
 {init,start_em,1,[]},
 {init,do_boot,3,[]}]
[ns_server:info,2020-04-02T20:08:15.593+05:30,ns_1@127.0.0.1:ns_log<0.255.0>:ns_log:handle_cast:151]suppressing duplicate log ns_log:undefined([<<"Service 'ns_server' exited with status 3. Restarting. Messages:\n[os_mon] memory supervisor port (memsup): Erlang has closed\n[os_mon] cpu supervisor port (cpu_sup): Erlang has closed\nCrap error:{badmatch,\n            {error,\n             {{shutdown,\n               {failed_to_start_child,ns_server_cluster_sup,\n                {shutdown,\n                 {failed_to_start_child,ns_server_nodes_sup,\n                  {shutdown,\n                   {failed_to_start_child,wait_for_couchdb_node,\n                    {abnormal,1}}}}}}},\n              {ns_server,start,[normal,[]]}}}}\n[{ns_bootstrap,start,0,[{file,\"src/ns_bootstrap.erl\"},{line,36}]},\n {child_erlang,do_child_start,1,[{file,\"src/child_erlang.erl\"},{line,113}]},\n {child_erlang,child_start,1,[{file,\"src/child_erlang.erl\"},{line,91}]},\n {init,start_em,1,[]},\n {init,do_boot,3,[]}]">>]) because it's been seen 2 times in the past 0.004183 secs (last seen 0.002184 secs ago
[user:info,2020-04-02T20:08:15.596+05:30,ns_1@127.0.0.1:<0.256.0>:ns_log:crash_consumption_loop:69]Service 'ns_server' exited with status 3. Restarting. Messages:
[os_mon] cpu supervisor port (cpu_sup): Erlang has closed
[os_mon] memory supervisor port (memsup): Erlang has closed
Crap error:{badmatch,
            {error,
             {{shutdown,
               {failed_to_start_child,ns_server_cluster_sup,
                {shutdown,
                 {failed_to_start_child,ns_server_nodes_sup,
                  {shutdown,
                   {failed_to_start_child,wait_for_couchdb_node,
                    {abnormal,1}}}}}}},
              {ns_server,start,[normal,[]]}}}}
[{ns_bootstrap,start,0,[{file,"src/ns_bootstrap.erl"},{line,36}]},
 {child_erlang,do_child_start,1,[{file,"src/child_erlang.erl"},{line,113}]},
 {child_erlang,child_start,1,[{file,"src/child_erlang.erl"},{line,91}]},
 {init,start_em,1,[]},
 {init,do_boot,3,[]}]
[user:info,2020-04-02T20:08:15.597+05:30,ns_1@127.0.0.1:<0.256.0>:ns_log:crash_consumption_loop:69]Service 'ns_server' exited with status 3. Restarting. Messages:
[os_mon] memory supervisor port (memsup): Erlang has closed
[os_mon] cpu supervisor port (cpu_sup): Erlang has closed
Crap error:{badmatch,
            {error,
             {{shutdown,
               {failed_to_start_child,ns_server_cluster_sup,
                {shutdown,
                 {failed_to_start_child,ns_server_nodes_sup,
                  {shutdown,
                   {failed_to_start_child,wait_for_couchdb_node,
                    {abnormal,1}}}}}}},
              {ns_server,start,[normal,[]]}}}}
[{ns_bootstrap,start,0,[{file,"src/ns_bootstrap.erl"},{line,36}]},
 {child_erlang,do_child_start,1,[{file,"src/child_erlang.erl"},{line,113}]},
 {child_erlang,child_start,1,[{file,"src/child_erlang.erl"},{line,91}]},
 {init,start_em,1,[]},
 {init,do_boot,3,[]}]
[ns_server:info,2020-04-02T20:08:15.597+05:30,ns_1@127.0.0.1:ns_log<0.255.0>:ns_log:handle_cast:151]suppressing duplicate log ns_log:undefined([<<"Service 'ns_server' exited with status 3. Restarting. Messages:\n[os_mon] memory supervisor port (memsup): Erlang has closed\n[os_mon] cpu supervisor port (cpu_sup): Erlang has closed\nCrap error:{badmatch,\n            {error,\n             {{shutdown,\n               {failed_to_start_child,ns_server_cluster_sup,\n                {shutdown,\n                 {failed_to_start_child,ns_server_nodes_sup,\n                  {shutdown,\n                   {failed_to_start_child,wait_for_couchdb_node,\n                    {abnormal,1}}}}}}},\n              {ns_server,start,[normal,[]]}}}}\n[{ns_bootstrap,start,0,[{file,\"src/ns_bootstrap.erl\"},{line,36}]},\n {child_erlang,do_child_start,1,[{file,\"src/child_erlang.erl\"},{line,113}]},\n {child_erlang,child_start,1,[{file,\"src/child_erlang.erl\"},{line,91}]},\n {init,start_em,1,[]},\n {init,do_boot,3,[]}]">>]) because it's been seen 3 times in the past 0.008578 secs (last seen 0.004395 secs ago
[user:info,2020-04-02T20:08:15.599+05:30,ns_1@127.0.0.1:<0.256.0>:ns_log:crash_consumption_loop:69]Service 'ns_server' exited with status 3. Restarting. Messages:
[os_mon] memory supervisor port (memsup): Erlang has closed
[os_mon] cpu supervisor port (cpu_sup): Erlang has closed
Crap error:{badmatch,
            {error,
             {{shutdown,
               {failed_to_start_child,ns_server_cluster_sup,
                {shutdown,
                 {failed_to_start_child,ns_server_nodes_sup,
                  {shutdown,
                   {failed_to_start_child,wait_for_couchdb_node,
                    {abnormal,1}}}}}}},
              {ns_server,start,[normal,[]]}}}}
[{ns_bootstrap,start,0,[{file,"src/ns_bootstrap.erl"},{line,36}]},
 {child_erlang,do_child_start,1,[{file,"src/child_erlang.erl"},{line,113}]},
 {child_erlang,child_start,1,[{file,"src/child_erlang.erl"},{line,91}]},
 {init,start_em,1,[]},
 {init,do_boot,3,[]}]
[ns_server:info,2020-04-02T20:08:15.599+05:30,ns_1@127.0.0.1:ns_log<0.255.0>:ns_log:handle_cast:151]suppressing duplicate log ns_log:undefined([<<"Service 'ns_server' exited with status 3. Restarting. Messages:\n[os_mon] memory supervisor port (memsup): Erlang has closed\n[os_mon] cpu supervisor port (cpu_sup): Erlang has closed\nCrap error:{badmatch,\n            {error,\n             {{shutdown,\n               {failed_to_start_child,ns_server_cluster_sup,\n                {shutdown,\n                 {failed_to_start_child,ns_server_nodes_sup,\n                  {shutdown,\n                   {failed_to_start_child,wait_for_couchdb_node,\n                    {abnormal,1}}}}}}},\n              {ns_server,start,[normal,[]]}}}}\n[{ns_bootstrap,start,0,[{file,\"src/ns_bootstrap.erl\"},{line,36}]},\n {child_erlang,do_child_start,1,[{file,\"src/child_erlang.erl\"},{line,113}]},\n {child_erlang,child_start,1,[{file,\"src/child_erlang.erl\"},{line,91}]},\n {init,start_em,1,[]},\n {init,do_boot,3,[]}]">>]) because it's been seen 4 times in the past 0.010696 secs (last seen 0.002118 secs ago
[user:info,2020-04-02T20:08:15.605+05:30,ns_1@127.0.0.1:<0.256.0>:ns_log:crash_consumption_loop:69]Service 'ns_server' exited with status 3. Restarting. Messages:
[os_mon] memory supervisor port (memsup): Erlang has closed
[os_mon] cpu supervisor port (cpu_sup): Erlang has closed
Crap error:{badmatch,
            {error,
             {{shutdown,
               {failed_to_start_child,ns_server_cluster_sup,
                {shutdown,
                 {failed_to_start_child,ns_server_nodes_sup,
                  {shutdown,
                   {failed_to_start_child,wait_for_couchdb_node,
                    {abnormal,1}}}}}}},
              {ns_server,start,[normal,[]]}}}}
[{ns_bootstrap,start,0,[{file,"src/ns_bootstrap.erl"},{line,36}]},
 {child_erlang,do_child_start,1,[{file,"src/child_erlang.erl"},{line,113}]},
 {child_erlang,child_start,1,[{file,"src/child_erlang.erl"},{line,91}]},
 {init,start_em,1,[]},
 {init,do_boot,3,[]}]
[ns_server:info,2020-04-02T20:08:15.605+05:30,ns_1@127.0.0.1:ns_log<0.255.0>:ns_log:handle_cast:151]suppressing duplicate log ns_log:undefined([<<"Service 'ns_server' exited with status 3. Restarting. Messages:\n[os_mon] memory supervisor port (memsup): Erlang has closed\n[os_mon] cpu supervisor port (cpu_sup): Erlang has closed\nCrap error:{badmatch,\n            {error,\n             {{shutdown,\n               {failed_to_start_child,ns_server_cluster_sup,\n                {shutdown,\n                 {failed_to_start_child,ns_server_nodes_sup,\n                  {shutdown,\n                   {failed_to_start_child,wait_for_couchdb_node,\n                    {abnormal,1}}}}}}},\n              {ns_server,start,[normal,[]]}}}}\n[{ns_bootstrap,start,0,[{file,\"src/ns_bootstrap.erl\"},{line,36}]},\n {child_erlang,do_child_start,1,[{file,\"src/child_erlang.erl\"},{line,113}]},\n {child_erlang,child_start,1,[{file,\"src/child_erlang.erl\"},{line,91}]},\n {init,start_em,1,[]},\n {init,do_boot,3,[]}]">>]) because it's been seen 5 times in the past 0.016554 secs (last seen 0.005858 secs ago
[user:info,2020-04-02T20:08:15.607+05:30,ns_1@127.0.0.1:<0.256.0>:ns_log:crash_consumption_loop:69]Service 'ns_server' exited with status 3. Restarting. Messages:
[os_mon] memory supervisor port (memsup): Erlang has closed
[os_mon] cpu supervisor port (cpu_sup): Erlang has closed
Crap error:{badmatch,
            {error,
             {{shutdown,
               {failed_to_start_child,ns_server_cluster_sup,
                {shutdown,
                 {failed_to_start_child,ns_server_nodes_sup,
                  {shutdown,
                   {failed_to_start_child,wait_for_couchdb_node,
                    {abnormal,1}}}}}}},
              {ns_server,start,[normal,[]]}}}}
[{ns_bootstrap,start,0,[{file,"src/ns_bootstrap.erl"},{line,36}]},
 {child_erlang,do_child_start,1,[{file,"src/child_erlang.erl"},{line,113}]},
 {child_erlang,child_start,1,[{file,"src/child_erlang.erl"},{line,91}]},
 {init,start_em,1,[]},
 {init,do_boot,3,[]}]
[ns_server:info,2020-04-02T20:08:15.607+05:30,ns_1@127.0.0.1:ns_log<0.255.0>:ns_log:handle_cast:151]suppressing duplicate log ns_log:undefined([<<"Service 'ns_server' exited with status 3. Restarting. Messages:\n[os_mon] memory supervisor port (memsup): Erlang has closed\n[os_mon] cpu supervisor port (cpu_sup): Erlang has closed\nCrap error:{badmatch,\n            {error,\n             {{shutdown,\n               {failed_to_start_child,ns_server_cluster_sup,\n                {shutdown,\n                 {failed_to_start_child,ns_server_nodes_sup,\n                  {shutdown,\n                   {failed_to_start_child,wait_for_couchdb_node,\n                    {abnormal,1}}}}}}},\n              {ns_server,start,[normal,[]]}}}}\n[{ns_bootstrap,start,0,[{file,\"src/ns_bootstrap.erl\"},{line,36}]},\n {child_erlang,do_child_start,1,[{file,\"src/child_erlang.erl\"},{line,113}]},\n {child_erlang,child_start,1,[{file,\"src/child_erlang.erl\"},{line,91}]},\n {init,start_em,1,[]},\n {init,do_boot,3,[]}]">>]) because it's been seen 6 times in the past 0.01813 secs (last seen 0.001576 secs ago
[user:info,2020-04-02T20:08:15.609+05:30,ns_1@127.0.0.1:<0.256.0>:ns_log:crash_consumption_loop:69]Service 'ns_server' exited with status 1. Restarting. Messages:
[os_mon] memory supervisor port (memsup): Erlang has closed
[os_mon] cpu supervisor port (cpu_sup): Erlang has closed
[user:info,2020-04-02T20:08:15.610+05:30,ns_1@127.0.0.1:<0.256.0>:ns_log:crash_consumption_loop:69]Service 'ns_server' exited with status 1. Restarting. Messages:
[os_mon] memory supervisor port (memsup): Erlang has closed
[os_mon] cpu supervisor port (cpu_sup): Erlang has closed
[ns_server:info,2020-04-02T20:08:15.610+05:30,ns_1@127.0.0.1:ns_log<0.255.0>:ns_log:handle_cast:151]suppressing duplicate log ns_log:undefined([<<"Service 'ns_server' exited with status 1. Restarting. Messages:\n[os_mon] memory supervisor port (memsup): Erlang has closed\n[os_mon] cpu supervisor port (cpu_sup): Erlang has closed">>]) because it's been seen 1 times in the past 0.001275 secs (last seen 0.001275 secs ago
[user:info,2020-04-02T20:08:15.612+05:30,ns_1@127.0.0.1:<0.256.0>:ns_log:crash_consumption_loop:69]Service 'ns_server' exited with status 1. Restarting. Messages:
[os_mon] cpu supervisor port (cpu_sup): Erlang has closed
[os_mon] memory supervisor port (memsup): Erlang has closed
[ns_server:info,2020-04-02T20:08:15.614+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.231.0>:ns_port_server:log:224]ns_couchdb<0.231.0>: Apache CouchDB  (LogLevel=info) is starting.
ns_couchdb<0.231.0>: Apache CouchDB has started. Time to relax.
ns_couchdb<0.231.0>: 26323: Booted. Waiting for shutdown request
ns_couchdb<0.231.0>: working as port

[ns_server:debug,2020-04-02T20:08:15.622+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:302]Suspended by process <0.257.0>
[ns_server:debug,2020-04-02T20:08:15.622+05:30,ns_1@127.0.0.1:memcached_passwords<0.257.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-04-02T20:08:15.622+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:309]Released by process <0.257.0>
[ns_server:debug,2020-04-02T20:08:15.622+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[error_logger:info,2020-04-02T20:08:15.622+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.257.0>},
                       {name,memcached_passwords},
                       {mfargs,{memcached_passwords,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.624+05:30,ns_1@127.0.0.1:memcached_permissions<0.260.0>:memcached_cfg:init:62]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:warn,2020-04-02T20:08:15.626+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-04-02T20:08:15.627+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:93]Refresh of [isasl] failed. Retry in 1000 ms.
[ns_server:debug,2020-04-02T20:08:15.628+05:30,ns_1@127.0.0.1:memcached_permissions<0.260.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2020-04-02T20:08:15.633+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:302]Suspended by process <0.260.0>
[ns_server:debug,2020-04-02T20:08:15.633+05:30,ns_1@127.0.0.1:memcached_permissions<0.260.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-04-02T20:08:15.633+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:309]Released by process <0.260.0>
[ns_server:debug,2020-04-02T20:08:15.633+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[error_logger:info,2020-04-02T20:08:15.633+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.260.0>},
                       {name,memcached_permissions},
                       {mfargs,{memcached_permissions,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-04-02T20:08:15.633+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-04-02T20:08:15.633+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2020-04-02T20:08:15.634+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.263.0>},
                       {name,ns_email_alert},
                       {mfargs,{ns_email_alert,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.635+05:30,ns_1@127.0.0.1:ns_node_disco<0.266.0>:ns_node_disco:init:128]Initting ns_node_disco with []
[ns_server:debug,2020-04-02T20:08:15.635+05:30,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[error_logger:info,2020-04-02T20:08:15.635+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.265.0>},
                       {id,ns_node_disco_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_node_disco_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.635+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"9245db3c028cb7987096449d46433aa1">>} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{4,63753057495}}]}]
[user:info,2020-04-02T20:08:15.635+05:30,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_init:84]Initial otp cookie generated: {sanitized,
                                  <<"LfF0PbfjEuzg2v35gqT8vk4/IfHDbKW9Uvg+GpJ7Cbg=">>}
[ns_server:debug,2020-04-02T20:08:15.635+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
otp ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057495}}]},
 {cookie,{sanitized,<<"LfF0PbfjEuzg2v35gqT8vk4/IfHDbKW9Uvg+GpJ7Cbg=">>}}]
[ns_server:debug,2020-04-02T20:08:15.635+05:30,ns_1@127.0.0.1:<0.267.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"LfF0PbfjEuzg2v35gqT8vk4/IfHDbKW9Uvg+GpJ7Cbg=">>}
[ns_server:debug,2020-04-02T20:08:15.636+05:30,ns_1@127.0.0.1:<0.267.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"LfF0PbfjEuzg2v35gqT8vk4/IfHDbKW9Uvg+GpJ7Cbg=">>}
[error_logger:info,2020-04-02T20:08:15.636+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.266.0>},
                       {id,ns_node_disco},
                       {mfargs,{ns_node_disco,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.637+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.269.0>},
                       {id,ns_node_disco_log},
                       {mfargs,{ns_node_disco_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.638+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.270.0>},
                       {id,ns_node_disco_conf_events},
                       {mfargs,{ns_node_disco_conf_events,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.639+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:init:71]init pulling
[error_logger:info,2020-04-02T20:08:15.639+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.271.0>},
                       {id,ns_config_rep_merger},
                       {mfargs,{ns_config_rep,start_link_merger,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.639+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:init:73]init pushing
[ns_server:debug,2020-04-02T20:08:15.641+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:init:77]init reannouncing
[ns_server:debug,2020-04-02T20:08:15.641+05:30,ns_1@127.0.0.1:ns_config_events<0.193.0>:ns_node_disco_conf_events:handle_event:50]ns_node_disco_conf_events config on otp
[ns_server:debug,2020-04-02T20:08:15.641+05:30,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2020-04-02T20:08:15.641+05:30,ns_1@127.0.0.1:ns_config_events<0.193.0>:ns_node_disco_conf_events:handle_event:44]ns_node_disco_conf_events config on nodes_wanted
[ns_server:debug,2020-04-02T20:08:15.641+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.225.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from undefined to {undefined,
                                                                             {0,
                                                                              1090696716},
                                                                             {0,
                                                                              1090696716},
                                                                             false,
                                                                             []}
[ns_server:debug,2020-04-02T20:08:15.641+05:30,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2020-04-02T20:08:15.641+05:30,ns_1@127.0.0.1:<0.279.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"LfF0PbfjEuzg2v35gqT8vk4/IfHDbKW9Uvg+GpJ7Cbg=">>}
[ns_server:debug,2020-04-02T20:08:15.642+05:30,ns_1@127.0.0.1:memcached_passwords<0.257.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-04-02T20:08:15.642+05:30,ns_1@127.0.0.1:<0.279.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"LfF0PbfjEuzg2v35gqT8vk4/IfHDbKW9Uvg+GpJ7Cbg=">>}
[ns_server:debug,2020-04-02T20:08:15.642+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
otp ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057495}}]},
 {cookie,{sanitized,<<"LfF0PbfjEuzg2v35gqT8vk4/IfHDbKW9Uvg+GpJ7Cbg=">>}}]
[ns_server:debug,2020-04-02T20:08:15.641+05:30,ns_1@127.0.0.1:<0.278.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"LfF0PbfjEuzg2v35gqT8vk4/IfHDbKW9Uvg+GpJ7Cbg=">>}
[ns_server:debug,2020-04-02T20:08:15.642+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_dir} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057495}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2020-04-02T20:08:15.642+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_dirs} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057495}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2020-04-02T20:08:15.642+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',erl_external_listeners} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2020-04-02T20:08:15.642+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',node_encryption} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|false]
[ns_server:debug,2020-04-02T20:08:15.642+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',address_family} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|inet]
[ns_server:debug,2020-04-02T20:08:15.642+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
alert_limits ->
[{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]
[ns_server:debug,2020-04-02T20:08:15.642+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
audit ->
[{auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2020-04-02T20:08:15.643+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]
[ns_server:debug,2020-04-02T20:08:15.643+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_reprovision_cfg ->
[{enabled,true},{max_nodes,1},{count,0}]
[ns_server:debug,2020-04-02T20:08:15.643+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
autocompaction ->
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-04-02T20:08:15.643+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
buckets ->
[[],{configs,[]}]
[ns_server:debug,2020-04-02T20:08:15.643+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
cbas_memory_quota ->
2174
[ns_server:debug,2020-04-02T20:08:15.643+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
drop_request_memory_threshold_mib ->
undefined
[ns_server:debug,2020-04-02T20:08:15.643+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
email_alerts ->
[{recipients,["root@localhost"]},
 {sender,"couchbase@localhost"},
 {enabled,false},
 {email_server,[{user,[]},
                {pass,"*****"},
                {host,"localhost"},
                {port,25},
                {encrypt,false}]},
 {alerts,[auto_failover_node,auto_failover_maximum_reached,
          auto_failover_other_nodes_down,auto_failover_cluster_too_small,
          auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
          ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
          ep_clock_cas_drift_threshold_exceeded,communication_issue]}]
[ns_server:debug,2020-04-02T20:08:15.642+05:30,ns_1@127.0.0.1:<0.278.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"LfF0PbfjEuzg2v35gqT8vk4/IfHDbKW9Uvg+GpJ7Cbg=">>}
[ns_server:debug,2020-04-02T20:08:15.643+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
fts_memory_quota ->
512
[ns_server:debug,2020-04-02T20:08:15.643+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
index_aware_rebalance_disabled ->
false
[ns_server:debug,2020-04-02T20:08:15.643+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
log_redaction_default_cfg ->
[{redact_level,none}]
[ns_server:debug,2020-04-02T20:08:15.643+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
max_bucket_count ->
30
[ns_server:debug,2020-04-02T20:08:15.643+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
memcached ->
[]
[ns_server:debug,2020-04-02T20:08:15.643+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
memory_quota ->
8886
[ns_server:debug,2020-04-02T20:08:15.643+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
nodes_wanted ->
['ns_1@127.0.0.1']
[ns_server:debug,2020-04-02T20:08:15.643+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
password_policy ->
[{min_length,6},{must_present,[]}]
[ns_server:debug,2020-04-02T20:08:15.643+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
quorum_nodes ->
['ns_1@127.0.0.1']
[ns_server:debug,2020-04-02T20:08:15.643+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
remote_clusters ->
[]
[ns_server:debug,2020-04-02T20:08:15.643+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
replication ->
[{enabled,true}]
[ns_server:debug,2020-04-02T20:08:15.644+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
rest ->
[{port,8091}]
[ns_server:debug,2020-04-02T20:08:15.644+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
rest_creds ->
null
[ns_server:debug,2020-04-02T20:08:15.644+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
secure_headers ->
[]
[ns_server:debug,2020-04-02T20:08:15.644+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
server_groups ->
[[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]
[ns_server:debug,2020-04-02T20:08:15.644+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
set_view_update_daemon ->
[{update_interval,5000},
 {update_min_changes,5000},
 {replica_update_min_changes,5000}]
[ns_server:debug,2020-04-02T20:08:15.644+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_indexers} ->
4
[ns_server:debug,2020-04-02T20:08:15.644+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_replica_indexers} ->
2
[ns_server:debug,2020-04-02T20:08:15.644+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
<<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"in"...>>
[ns_server:debug,2020-04-02T20:08:15.644+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{request_limit,capi} ->
undefined
[ns_server:debug,2020-04-02T20:08:15.644+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{request_limit,rest} ->
undefined
[ns_server:debug,2020-04-02T20:08:15.644+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',audit} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}]
[ns_server:debug,2020-04-02T20:08:15.644+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',capi_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|8092]
[ns_server:debug,2020-04-02T20:08:15.644+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_admin_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9110]
[ns_server:debug,2020-04-02T20:08:15.644+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_client_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9113]
[ns_server:debug,2020-04-02T20:08:15.644+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9112]
[ns_server:debug,2020-04-02T20:08:15.644+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_http_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9111]
[ns_server:debug,2020-04-02T20:08:15.644+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cluster_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9115]
[ns_server:debug,2020-04-02T20:08:15.644+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_console_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9114]
[ns_server:debug,2020-04-02T20:08:15.644+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_data_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9116]
[ns_server:debug,2020-04-02T20:08:15.645+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_debug_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|-1]
[ns_server:debug,2020-04-02T20:08:15.645+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_http_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|8095]
[ns_server:debug,2020-04-02T20:08:15.645+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_messaging_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9118]
[ns_server:debug,2020-04-02T20:08:15.645+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9119]
[ns_server:debug,2020-04-02T20:08:15.645+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9121]
[ns_server:debug,2020-04-02T20:08:15.645+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_parent_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9122]
[ns_server:debug,2020-04-02T20:08:15.645+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_replication_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9120]
[ns_server:debug,2020-04-02T20:08:15.645+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_result_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9117]
[ns_server:debug,2020-04-02T20:08:15.645+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_ssl_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:08:15.645+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',compaction_daemon} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2020-04-02T20:08:15.645+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',config_version} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|{6,5}]
[ns_server:debug,2020-04-02T20:08:15.645+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_debug_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9140]
[ns_server:debug,2020-04-02T20:08:15.645+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_http_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|8096]
[ns_server:debug,2020-04-02T20:08:15.645+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_https_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:08:15.645+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_grpc_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9130]
[ns_server:debug,2020-04-02T20:08:15.645+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:08:15.645+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_http_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|8094]
[ns_server:debug,2020-04-02T20:08:15.646+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_ssl_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:08:15.646+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_admin_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9100]
[ns_server:debug,2020-04-02T20:08:15.646+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_http_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9102]
[ns_server:debug,2020-04-02T20:08:15.646+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_https_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:08:15.646+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_scan_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9101]
[ns_server:debug,2020-04-02T20:08:15.646+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stcatchup_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9104]
[ns_server:debug,2020-04-02T20:08:15.646+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stinit_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9103]
[ns_server:debug,2020-04-02T20:08:15.646+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stmaint_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9105]
[ns_server:debug,2020-04-02T20:08:15.646+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',is_enterprise} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|false]
[ns_server:debug,2020-04-02T20:08:15.646+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',isasl} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2020-04-02T20:08:15.646+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',membership} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
 active]
[ns_server:debug,2020-04-02T20:08:15.647+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,undefined},
 {ssl_port,undefined},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_sleeptime,19},
 {log_rotation_period,39003}]
[ns_server:debug,2020-04-02T20:08:15.647+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([alert_limits,audit,auto_failover_cfg,
                               auto_reprovision_cfg,autocompaction,buckets,
                               cbas_memory_quota,
                               drop_request_memory_threshold_mib,email_alerts,
                               fts_memory_quota,
                               index_aware_rebalance_disabled,
                               log_redaction_default_cfg,max_bucket_count,
                               memcached,memory_quota,nodes_wanted,otp,
                               password_policy,quorum_nodes,remote_clusters,
                               replication,rest,rest_creds,secure_headers,
                               server_groups,set_view_update_daemon,
                               {couchdb,max_parallel_indexers},
                               {couchdb,max_parallel_replica_indexers},
                               {local_changes_count,
                                   <<"9245db3c028cb7987096449d46433aa1">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {request_limit,capi},
                               {request_limit,rest},
                               {node,'ns_1@127.0.0.1',address_family},
                               {node,'ns_1@127.0.0.1',audit},
                               {node,'ns_1@127.0.0.1',capi_port},
                               {node,'ns_1@127.0.0.1',cbas_admin_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_client_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_http_port},
                               {node,'ns_1@127.0.0.1',cbas_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_console_port},
                               {node,'ns_1@127.0.0.1',cbas_data_port},
                               {node,'ns_1@127.0.0.1',cbas_debug_port},
                               {node,'ns_1@127.0.0.1',cbas_dirs},
                               {node,'ns_1@127.0.0.1',cbas_http_port},
                               {node,'ns_1@127.0.0.1',cbas_messaging_port},
                               {node,'ns_1@127.0.0.1',
                                   cbas_metadata_callback_port},
                               {node,'ns_1@127.0.0.1',cbas_metadata_port},
                               {node,'ns_1@127.0.0.1',cbas_parent_port},
                               {node,'ns_1@127.0.0.1',cbas_replication_port},
                               {node,'ns_1@127.0.0.1',cbas_result_port},
                               {node,'ns_1@127.0.0.1',cbas_ssl_port},
                               {node,'ns_1@127.0.0.1',compaction_daemon},
                               {node,'ns_1@127.0.0.1',config_version},
                               {node,'ns_1@127.0.0.1',erl_external_listeners},
                               {node,'ns_1@127.0.0.1',eventing_debug_port},
                               {node,'ns_1@127.0.0.1',eventing_dir},
                               {node,'ns_1@127.0.0.1',eventing_http_port},
                               {node,'ns_1@127.0.0.1',eventing_https_port},
                               {node,'ns_1@127.0.0.1',fts_grpc_port},
                               {node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
                               {node,'ns_1@127.0.0.1',fts_http_port},
                               {node,'ns_1@127.0.0.1',fts_ssl_port},
                               {node,'ns_1@127.0.0.1',indexer_admin_port}]..)
[ns_server:debug,2020-04-02T20:08:15.647+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_config} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
 {[{interfaces,
    {memcached_config_mgr,omit_missing_mcd_ports,
     [{[{host,<<"*">>},
        {port,port},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_port},
        {system,true},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,ssl_port},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_ssl_port},
        {system,true},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
   {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
   {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {opentracing,
    {[{enabled,opentracing_enabled},
      {module,{"~s",[opentracing_module]}},
      {config,{"~s",[opentracing_config]}}]}},
   {admin,{"~s",[admin_user]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},
      {cyclesize,log_cyclesize},
      {sleeptime,log_sleeptime}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}}]}]
[ns_server:debug,2020-04-02T20:08:15.648+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:08:15.648+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_defaults} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {opentracing_enabled,false},
 {opentracing_module,[]},
 {opentracing_config,[]},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {tracing_enabled,false},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>}]
[ns_server:debug,2020-04-02T20:08:15.648+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',moxi} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]},
 {port,0}]
[error_logger:info,2020-04-02T20:08:15.648+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.272.0>},
                       {id,ns_config_rep},
                       {mfargs,{ns_config_rep,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.648+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.264.0>},
                       {name,ns_node_disco_sup},
                       {mfargs,{ns_node_disco_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:08:15.648+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ns_log} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2020-04-02T20:08:15.648+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',port_servers} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}]
[ns_server:debug,2020-04-02T20:08:15.648+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',projector_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9999]
[ns_server:debug,2020-04-02T20:08:15.648+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',projector_ssl_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:08:15.648+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',query_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|8093]
[ns_server:debug,2020-04-02T20:08:15.648+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',rest} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2020-04-02T20:08:15.648+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',saslauthd_enabled} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|true]
[ns_server:debug,2020-04-02T20:08:15.649+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_capi_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:08:15.649+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_query_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:08:15.649+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_rest_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:08:15.649+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',uuid} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|
 <<"9245db3c028cb7987096449d46433aa1">>]
[ns_server:debug,2020-04-02T20:08:15.649+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',xdcr_rest_port} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|9998]
[ns_server:debug,2020-04-02T20:08:15.649+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057494}}]}|false]
[ns_server:debug,2020-04-02T20:08:15.649+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"9245db3c028cb7987096449d46433aa1">>} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{4,63753057495}}]}]
[error_logger:info,2020-04-02T20:08:15.656+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.284.0>},
                       {name,vbucket_map_mirror},
                       {mfargs,{vbucket_map_mirror,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.658+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.286.0>},
                       {name,bucket_info_cache},
                       {mfargs,{bucket_info_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.658+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.289.0>},
                       {name,ns_tick_event},
                       {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.659+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.290.0>},
                       {name,buckets_events},
                       {mfargs,
                           {gen_event,start_link,[{local,buckets_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.659+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.291.0>},
                       {name,ns_stats_event},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_stats_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.660+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.292.0>},
                       {name,samples_loader_tasks},
                       {mfargs,{samples_loader_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.664+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.294.0>},
                       {id,ns_heart},
                       {mfargs,{ns_heart,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.664+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.297.0>},
                       {id,ns_heart_slow_updater},
                       {mfargs,{ns_heart,start_link_slow_updater,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.664+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.293.0>},
                       {name,ns_heart_sup},
                       {mfargs,{ns_heart_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:15.666+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.301.0>},
                       {id,ns_doctor_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_doctor_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.668+05:30,ns_1@127.0.0.1:ns_heart<0.294.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]},
                 {ns_heart,handle_info,2,
                           [{file,"src/ns_heart.erl"},{line,118}]}]}}

[ns_server:debug,2020-04-02T20:08:15.668+05:30,ns_1@127.0.0.1:ns_heart<0.294.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]}]}}

[ns_server:debug,2020-04-02T20:08:15.670+05:30,ns_1@127.0.0.1:<0.299.0>:restartable:start_child:98]Started child process <0.300.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2020-04-02T20:08:15.670+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.302.0>},
                       {id,ns_doctor},
                       {mfargs,{ns_doctor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.671+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.299.0>},
                       {name,ns_doctor_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_doctor_sup,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:15.671+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.305.0>},
                       {name,master_activity_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.672+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:302]Suspended by process <0.257.0>
[ns_server:debug,2020-04-02T20:08:15.672+05:30,ns_1@127.0.0.1:memcached_passwords<0.257.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-04-02T20:08:15.672+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:309]Released by process <0.257.0>
[ns_server:debug,2020-04-02T20:08:15.673+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:warn,2020-04-02T20:08:15.674+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-04-02T20:08:15.674+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2020-04-02T20:08:15.674+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.307.0>},
                       {name,xdcr_ckpt_store},
                       {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.674+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.308.0>},
                       {name,metakv_worker},
                       {mfargs,{work_queue,start_link,[metakv_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.674+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.309.0>},
                       {name,index_events},
                       {mfargs,{gen_event,start_link,[{local,index_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.674+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.310.0>},
                       {name,index_settings_manager},
                       {mfargs,{index_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.676+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.312.0>},
                       {name,query_settings_manager},
                       {mfargs,{query_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.677+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.314.0>},
                       {name,eventing_settings_manager},
                       {mfargs,{eventing_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.677+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.316.0>},
                       {name,audit_events},
                       {mfargs,{gen_event,start_link,[{local,audit_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.682+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.319.0>},
                       {id,menelaus_ui_auth},
                       {mfargs,{menelaus_ui_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.682+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.321.0>},
                       {id,scram_sha},
                       {mfargs,{scram_sha,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.688+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.322.0>},
                       {id,menelaus_local_auth},
                       {mfargs,{menelaus_local_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.689+05:30,ns_1@127.0.0.1:ns_heart<0.294.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2020-04-02T20:08:15.696+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.323.0>},
                       {id,menelaus_web_cache},
                       {mfargs,{menelaus_web_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.697+05:30,ns_1@127.0.0.1:ns_heart<0.294.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2020-04-02T20:08:15.698+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.324.0>},
                       {id,menelaus_stats_gatherer},
                       {mfargs,{menelaus_stats_gatherer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.698+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.325.0>},
                       {id,json_rpc_events},
                       {mfargs,
                           {gen_event,start_link,[{local,json_rpc_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:08:15.707+05:30,ns_1@127.0.0.1:<0.332.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-04-02T20:08:15.707+05:30,ns_1@127.0.0.1:<0.332.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:debug,2020-04-02T20:08:15.707+05:30,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.297.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]},
                 {proc_lib,init_p_do_apply,3,
                           [{file,"proc_lib.erl"},{line,247}]}]}}

[ns_server:debug,2020-04-02T20:08:15.707+05:30,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.297.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]}]}}

[ns_server:info,2020-04-02T20:08:15.707+05:30,ns_1@127.0.0.1:<0.332.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-04-02T20:08:15.708+05:30,ns_1@127.0.0.1:<0.332.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2020-04-02T20:08:15.708+05:30,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.297.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-04-02T20:08:15.708+05:30,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.297.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[ns_server:debug,2020-04-02T20:08:15.723+05:30,ns_1@127.0.0.1:<0.326.0>:restartable:start_child:98]Started child process <0.332.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2020-04-02T20:08:15.723+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.332.0>,menelaus_web}
             started: [{pid,<0.354.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                           {menelaus_web,http_server,
                               [[{ip,"0.0.0.0"},{name,menelaus_web_ipv4}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.723+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.326.0>},
                       {id,menelaus_web},
                       {mfargs,
                           {restartable,start_link,
                               [{menelaus_web,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:15.724+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.371.0>},
                       {id,menelaus_event},
                       {mfargs,{menelaus_event,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.725+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.372.0>},
                       {id,hot_keys_keeper},
                       {mfargs,{hot_keys_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.728+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.373.0>},
                       {id,menelaus_web_alerts_srv},
                       {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.731+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.374.0>},
                       {id,menelaus_cbauth},
                       {mfargs,{menelaus_cbauth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[user:info,2020-04-02T20:08:15.731+05:30,ns_1@127.0.0.1:ns_server_sup<0.249.0>:menelaus_sup:start_link:48]Couchbase Server has started on web port 8091 on node 'ns_1@127.0.0.1'. Version: "6.5.0-4966-community".
[error_logger:info,2020-04-02T20:08:15.731+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.317.0>},
                       {name,menelaus},
                       {mfargs,{menelaus_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:15.732+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.380.0>},
                       {name,ns_ports_setup},
                       {mfargs,{ns_ports_setup,start,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.735+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.384.0>},
                       {id,service_agent_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_agent_children_sup},
                                service_agent_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:15.735+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.385.0>},
                       {id,service_agent_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<service_agent_sup.0.107373856>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.735+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.383.0>},
                       {name,service_agent_sup},
                       {mfargs,{service_agent_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:08:15.739+05:30,ns_1@127.0.0.1:ns_ports_setup<0.380.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,goxdcr]
[error_logger:info,2020-04-02T20:08:15.742+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.387.0>},
                       {name,ns_memcached_sockets_pool},
                       {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.746+05:30,ns_1@127.0.0.1:memcached_auth_server<0.388.0>:memcached_auth_server:reconnect:233]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2020-04-02T20:08:15.746+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.388.0>},
                       {name,memcached_auth_server},
                       {mfargs,{memcached_auth_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.747+05:30,ns_1@127.0.0.1:ns_audit_cfg<0.390.0>:ns_audit_cfg:write_audit_json:259]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      1},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {disabled,
                                                                                      []},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2020-04-02T20:08:15.761+05:30,ns_1@127.0.0.1:ns_audit_cfg<0.390.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[error_logger:info,2020-04-02T20:08:15.761+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.390.0>},
                       {name,ns_audit_cfg},
                       {mfargs,{ns_audit_cfg,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-04-02T20:08:15.761+05:30,ns_1@127.0.0.1:<0.394.0>:ns_memcached:connect:1104]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[error_logger:info,2020-04-02T20:08:15.765+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.395.0>},
                       {name,ns_audit},
                       {mfargs,{ns_audit,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.766+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.396.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.765+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:init:49]waiting for completion of initial ns_ports_setup round
[ns_server:info,2020-04-02T20:08:15.787+05:30,ns_1@127.0.0.1:<0.397.0>:ns_memcached_log_rotator:init:42]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2020-04-02T20:08:15.787+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.397.0>},
                       {name,ns_memcached_log_rotator},
                       {mfargs,{ns_memcached_log_rotator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.791+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.398.0>},
                       {name,testconditions_store},
                       {mfargs,{simple_store,start_link,[testconditions]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.795+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.399.0>},
                       {name,terse_cluster_info_uploader},
                       {mfargs,{terse_cluster_info_uploader,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.803+05:30,ns_1@127.0.0.1:terse_cluster_info_uploader<0.399.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":4,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"kv\":11210,\"capi\":8092,\"projector\":9999},\"thisNode\":true}]}">>
[ns_server:warn,2020-04-02T20:08:15.805+05:30,ns_1@127.0.0.1:<0.403.0>:ns_memcached:connect:1104]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[error_logger:info,2020-04-02T20:08:15.806+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.404.0>},
                       {id,ns_bucket_sup},
                       {mfargs,{ns_bucket_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:15.808+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.405.0>},
                       {id,ns_bucket_worker},
                       {mfargs,{ns_bucket_worker,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.808+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.401.0>},
                       {name,ns_bucket_worker_sup},
                       {mfargs,{ns_bucket_worker_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:15.811+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.407.0>},
                       {name,system_stats_collector},
                       {mfargs,{system_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.813+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.411.0>},
                       {name,{stats_archiver,"@system"}},
                       {mfargs,{stats_archiver,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.814+05:30,ns_1@127.0.0.1:ns_ports_setup<0.380.0>:ns_ports_setup:set_children:85]Monitor ns_child_ports_sup <12938.107.0>
[ns_server:debug,2020-04-02T20:08:15.814+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:init:51]ns_ports_setup seems to be ready
[error_logger:info,2020-04-02T20:08:15.816+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.414.0>},
                       {name,{stats_reader,"@system"}},
                       {mfargs,{stats_reader,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.817+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.415.0>},
                       {name,{stats_archiver,"@system-processes"}},
                       {mfargs,
                           {stats_archiver,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.817+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.417.0>},
                       {name,{stats_reader,"@system-processes"}},
                       {mfargs,
                           {stats_reader,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.817+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.418.0>},
                       {name,{stats_archiver,"@query"}},
                       {mfargs,{stats_archiver,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.818+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.420.0>},
                       {name,{stats_reader,"@query"}},
                       {mfargs,{stats_reader,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.819+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:find_port_pid_loop:137]Found memcached port <12938.114.0>
[error_logger:info,2020-04-02T20:08:15.821+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.422.0>},
                       {name,query_stats_collector},
                       {mfargs,{query_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.821+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.424.0>},
                       {name,{stats_archiver,"@global"}},
                       {mfargs,{stats_archiver,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.821+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.426.0>},
                       {name,{stats_reader,"@global"}},
                       {mfargs,{stats_reader,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.829+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.427.0>},
                       {name,global_stats_collector},
                       {mfargs,{global_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.831+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.430.0>},
                       {name,goxdcr_status_keeper},
                       {mfargs,{goxdcr_status_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.832+05:30,ns_1@127.0.0.1:goxdcr_status_keeper<0.430.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-04-02T20:08:15.832+05:30,ns_1@127.0.0.1:goxdcr_status_keeper<0.430.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2020-04-02T20:08:15.835+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.434.0>},
                       {id,service_stats_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_stats_children_sup},
                                services_stats_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:15.839+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.436.0>},
                       {id,service_status_keeper_worker},
                       {mfargs,
                           {work_queue,start_link,
                               [service_status_keeper_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.841+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:init:82]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[ns_server:debug,2020-04-02T20:08:15.841+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:init:86]activated memcached port server
[error_logger:info,2020-04-02T20:08:15.843+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.437.0>},
                       {id,service_status_keeper_index},
                       {mfargs,{service_index,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.844+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.440.0>},
                       {id,service_status_keeper_fts},
                       {mfargs,{service_fts,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.847+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.443.0>},
                       {id,service_status_keeper_eventing},
                       {mfargs,{service_eventing,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.848+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.435.0>},
                       {id,service_status_keeper_sup},
                       {mfargs,{service_status_keeper_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:15.848+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.446.0>},
                       {id,service_stats_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<services_stats_sup.0.108537742>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.848+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.433.0>},
                       {name,services_stats_sup},
                       {mfargs,{services_stats_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:08:15.858+05:30,ns_1@127.0.0.1:<0.451.0>:new_concurrency_throttle:init:115]init concurrent throttle process, pid: <0.451.0>, type: kv_throttle# of available token: 1
[ns_server:debug,2020-04-02T20:08:15.864+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:08:15.864+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[error_logger:info,2020-04-02T20:08:15.864+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.448.0>},
                       {name,compaction_daemon},
                       {mfargs,{compaction_daemon,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.864+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:08:15.864+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:08:15.864+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:08:15.864+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2020-04-02T20:08:15.865+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,cluster_logs_sup}
             started: [{pid,<0.453.0>},
                       {id,ets_holder},
                       {mfargs,
                           {cluster_logs_collection_task,
                               start_link_ets_holder,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.865+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.452.0>},
                       {name,cluster_logs_sup},
                       {mfargs,{cluster_logs_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:15.865+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.454.0>},
                       {name,leader_events},
                       {mfargs,{gen_event,start_link,[{local,leader_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.873+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.458.0>},
                       {id,leader_activities},
                       {mfargs,{leader_activities,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.875+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.459.0>},
                       {id,leader_lease_agent},
                       {mfargs,{leader_lease_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.875+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.457.0>},
                       {id,leader_leases_sup},
                       {mfargs,{leader_leases_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:15.877+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.461.0>},
                       {id,leader_registry_server},
                       {mfargs,{leader_registry_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.879+05:30,ns_1@127.0.0.1:leader_registry_sup<0.460.0>:mb_master:check_master_takeover_needed:283]Sending master node question to the following nodes: []
[ns_server:debug,2020-04-02T20:08:15.879+05:30,ns_1@127.0.0.1:leader_registry_sup<0.460.0>:mb_master:check_master_takeover_needed:285]Got replies: []
[ns_server:debug,2020-04-02T20:08:15.879+05:30,ns_1@127.0.0.1:leader_registry_sup<0.460.0>:mb_master:check_master_takeover_needed:291]Was unable to discover master, not going to force mastership takeover
[user:info,2020-04-02T20:08:15.881+05:30,ns_1@127.0.0.1:mb_master<0.464.0>:mb_master:init:103]I'm the only node, so I'm the master.
[ns_server:debug,2020-04-02T20:08:15.881+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_new_leader:241]New leader is 'ns_1@127.0.0.1'. Invalidating name cache.
[ns_server:debug,2020-04-02T20:08:15.892+05:30,ns_1@127.0.0.1:mb_master<0.464.0>:master_activity_events:submit_cast:82]Failed to send master activity event: {error,badarg}
[ns_server:debug,2020-04-02T20:08:15.893+05:30,ns_1@127.0.0.1:leader_lease_acquirer<0.467.0>:leader_utils:wait_cluster_is_55:54]Delaying start since cluster is not fully upgraded to 5.5 yet.
[error_logger:info,2020-04-02T20:08:15.893+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.467.0>},
                       {id,leader_lease_acquirer},
                       {mfargs,{leader_lease_acquirer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.895+05:30,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.469.0>:leader_utils:wait_cluster_is_55:54]Delaying start since cluster is not fully upgraded to 5.5 yet.
[error_logger:info,2020-04-02T20:08:15.895+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.469.0>},
                       {id,leader_quorum_nodes_manager},
                       {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:08:15.897+05:30,ns_1@127.0.0.1:mb_master_sup<0.466.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.471.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-04-02T20:08:15.897+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.471.0>},
                       {id,ns_tick},
                       {mfargs,{ns_tick,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.899+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.473.0>},
                       {id,compat_mode_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,compat_mode_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.910+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[5,0]}]

[ns_server:info,2020-04-02T20:08:15.911+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [5,1]
[ns_server:debug,2020-04-02T20:08:15.911+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[5,1]},
 {set,client_cert_auth,[{state,"disable"},{prefixes,[]}]},
 {set,buckets,[{configs,[]}]}]

[ns_server:info,2020-04-02T20:08:15.912+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [5,5]
[ns_server:debug,2020-04-02T20:08:15.912+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[5,5]},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]}]},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120}">>},
 {set,{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
 {set,buckets,[{configs,[]}]},
 {delete,{rbac_upgrade,[5,5]}},
 {set,audit,
      [{enabled,[]},
       {disabled_users,[]},
       {auditd_enabled,false},
       {rotate_interval,86400},
       {rotate_size,20971520},
       {disabled,[]},
       {sync,[]},
       {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {set,quorum_nodes,['ns_1@127.0.0.1']},
 {set,scramsha_fallback_salt,
      <<53,116,70,173,103,129,158,204,100,121,119,133>>}]

[ns_server:info,2020-04-02T20:08:15.912+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [6,0]
[ns_server:debug,2020-04-02T20:08:15.912+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[6,0]}]

[ns_server:info,2020-04-02T20:08:15.915+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [6,5]
[ns_server:debug,2020-04-02T20:08:15.918+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[6,5]},
 {set,audit_decriptors,
      [{8243,
        [{name,<<"mutate document">>},
         {description,<<"Document was mutated via the REST API">>},
         {enabled,true},
         {module,ns_server}]},
       {8255,
        [{name,<<"read document">>},
         {description,<<"Document was read via the REST API">>},
         {enabled,false},
         {module,ns_server}]},
       {8257,
        [{name,<<"alert email sent">>},
         {description,<<"An alert email was successfully sent">>},
         {enabled,true},
         {module,ns_server}]},
       {20480,
        [{name,<<"opened DCP connection">>},
         {description,<<"opened DCP connection">>},
         {enabled,true},
         {module,memcached}]},
       {20482,
        [{name,<<"external memcached bucket flush">>},
         {description,<<"External user flushed the content of a memcached bucket">>},
         {enabled,true},
         {module,memcached}]},
       {20483,
        [{name,<<"invalid packet">>},
         {description,<<"Rejected an invalid packet">>},
         {enabled,true},
         {module,memcached}]},
       {20485,
        [{name,<<"authentication succeeded">>},
         {description,<<"Authentication to the cluster succeeded">>},
         {enabled,false},
         {module,memcached}]},
       {20488,
        [{name,<<"document read">>},
         {description,<<"Document was read">>},
         {enabled,false},
         {module,memcached}]},
       {20489,
        [{name,<<"document locked">>},
         {description,<<"Document was locked">>},
         {enabled,false},
         {module,memcached}]},
       {20490,
        [{name,<<"document modify">>},
         {description,<<"Document was modified">>},
         {enabled,false},
         {module,memcached}]},
       {20491,
        [{name,<<"document delete">>},
         {description,<<"Document was deleted">>},
         {enabled,false},
         {module,memcached}]},
       {20492,
        [{name,<<"select bucket">>},
         {description,<<"The specified bucket was selected">>},
         {enabled,true},
         {module,memcached}]},
       {28672,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28673,
        [{name,<<"EXPLAIN statement">>},
         {description,<<"A N1QL EXPLAIN statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28674,
        [{name,<<"PREPARE statement">>},
         {description,<<"A N1QL PREPARE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28675,
        [{name,<<"INFER statement">>},
         {description,<<"A N1QL INFER statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28676,
        [{name,<<"INSERT statement">>},
         {description,<<"A N1QL INSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28677,
        [{name,<<"UPSERT statement">>},
         {description,<<"A N1QL UPSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28678,
        [{name,<<"DELETE statement">>},
         {description,<<"A N1QL DELETE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28679,
        [{name,<<"UPDATE statement">>},
         {description,<<"A N1QL UPDATE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28680,
        [{name,<<"MERGE statement">>},
         {description,<<"A N1QL MERGE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28681,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28682,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28683,
        [{name,<<"ALTER INDEX statement">>},
         {description,<<"A N1QL ALTER INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28684,
        [{name,<<"BUILD INDEX statement">>},
         {description,<<"A N1QL BUILD INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28685,
        [{name,<<"GRANT ROLE statement">>},
         {description,<<"A N1QL GRANT ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28686,
        [{name,<<"REVOKE ROLE statement">>},
         {description,<<"A N1QL REVOKE ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28687,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An unrecognized statement was received by the N1QL query engine">>},
         {enabled,false},
         {module,n1ql}]},
       {28688,
        [{name,<<"CREATE PRIMARY INDEX statement">>},
         {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28689,
        [{name,<<"/admin/stats API request">>},
         {description,<<"An HTTP request was made to the API at /admin/stats.">>},
         {enabled,false},
         {module,n1ql}]},
       {28690,
        [{name,<<"/admin/vitals API request">>},
         {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
         {enabled,false},
         {module,n1ql}]},
       {28691,
        [{name,<<"/admin/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28692,
        [{name,<<"/admin/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28693,
        [{name,<<"/admin/indexes/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28694,
        [{name,<<"/admin/indexes/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28695,
        [{name,<<"/admin/indexes/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28697,
        [{name,<<"/admin/ping API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ping.">>},
         {enabled,false},
         {module,n1ql}]},
       {28698,
        [{name,<<"/admin/config API request">>},
         {description,<<"An HTTP request was made to the API at /admin/config.">>},
         {enabled,false},
         {module,n1ql}]},
       {28699,
        [{name,<<"/admin/ssl_cert API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
         {enabled,false},
         {module,n1ql}]},
       {28700,
        [{name,<<"/admin/settings API request">>},
         {description,<<"An HTTP request was made to the API at /admin/settings.">>},
         {enabled,false},
         {module,n1ql}]},
       {28701,
        [{name,<<"/admin/clusters API request">>},
         {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
         {enabled,false},
         {module,n1ql}]},
       {28702,
        [{name,<<"/admin/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28704,
        [{name,<<"/admin/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28705,
        [{name,<<"/admin/indexes/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {40960,
        [{name,<<"Create Design Doc">>},
         {description,<<"Design Doc is Created">>},
         {enabled,true},
         {module,view_engine}]},
       {40961,
        [{name,<<"Delete Design Doc">>},
         {description,<<"Design Doc is Deleted">>},
         {enabled,true},
         {module,view_engine}]},
       {40962,
        [{name,<<"Query DDoc Meta Data">>},
         {description,<<"Design Doc Meta Data Query Request">>},
         {enabled,true},
         {module,view_engine}]},
       {40963,
        [{name,<<"View Query">>},
         {description,<<"View Query Request">>},
         {enabled,false},
         {module,view_engine}]},
       {40964,
        [{name,<<"Update Design Doc">>},
         {description,<<"Design Doc is Updated">>},
         {enabled,true},
         {module,view_engine}]}]},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]},
       {can_abort_rebalance,false}]},
 {set,max_bucket_count,30},
 {set,retry_rebalance,
      [{enabled,false},{after_time_period,300},{max_attempts,1}]},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_size\":5120}">>}]

[ns_server:debug,2020-04-02T20:08:15.920+05:30,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.469.0>:leader_utils:wait_cluster_is_55_loop:78]Cluster upgraded to 5.5. Starting.
[ns_server:debug,2020-04-02T20:08:15.920+05:30,ns_1@127.0.0.1:leader_lease_acquirer<0.467.0>:leader_utils:wait_cluster_is_55_loop:78]Cluster upgraded to 5.5. Starting.
[ns_server:debug,2020-04-02T20:08:15.920+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.225.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from {undefined,
                                                                {0,1090696716},
                                                                {0,1090696716},
                                                                false,[]} to {[6,
                                                                               5],
                                                                              {0,
                                                                               1090696716},
                                                                              {0,
                                                                               1090696716},
                                                                              false,
                                                                              []}
[ns_server:debug,2020-04-02T20:08:15.920+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([audit,audit_decriptors,auto_failover_cfg,
                               buckets,client_cert_auth,
                               cluster_compat_version,max_bucket_count,
                               quorum_nodes,retry_rebalance,
                               scramsha_fallback_salt,
                               {local_changes_count,
                                   <<"9245db3c028cb7987096449d46433aa1">>},
                               {metakv,<<"/eventing/settings/config">>},
                               {metakv,<<"/query/settings/config">>}]..)
[ns_server:debug,2020-04-02T20:08:15.921+05:30,ns_1@127.0.0.1:memcached_permissions<0.260.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2020-04-02T20:08:15.921+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
retry_rebalance ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057495}}]},
 {enabled,false},
 {after_time_period,300},
 {max_attempts,1}]
[ns_server:debug,2020-04-02T20:08:15.921+05:30,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.469.0>:leader_quorum_nodes_manager:pull_config:114]Attempting to pull config from nodes:
[]
[ns_server:debug,2020-04-02T20:08:15.921+05:30,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.469.0>:leader_quorum_nodes_manager:pull_config:119]Pulled config successfully.
[ns_server:debug,2020-04-02T20:08:15.921+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:handle_call:122]Got full synchronization request from 'ns_1@127.0.0.1'
[ns_server:debug,2020-04-02T20:08:15.921+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:handle_call:128]Fully synchronized config in 16 us
[user:warn,2020-04-02T20:08:15.921+05:30,ns_1@127.0.0.1:compat_mode_manager<0.474.0>:compat_mode_manager:handle_consider_switching_compat_mode:49]Changed cluster compat mode from undefined to [6,5]
[ns_server:debug,2020-04-02T20:08:15.922+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:302]Suspended by process <0.260.0>
[ns_server:debug,2020-04-02T20:08:15.922+05:30,ns_1@127.0.0.1:memcached_permissions<0.260.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-04-02T20:08:15.922+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:309]Released by process <0.260.0>
[error_logger:info,2020-04-02T20:08:15.922+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.474.0>},
                       {id,compat_mode_manager},
                       {mfargs,{compat_mode_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.923+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
audit_decriptors ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057495}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {40960,
  [{name,<<"Create Design Doc">>},
   {description,<<"Design Doc is Created">>},
   {enabled,true},
   {module,view_engine}]},
 {40961,
  [{name,<<"Delete Design Doc">>},
   {description,<<"Design Doc is Deleted">>},
   {enabled,true},
   {module,view_engine}]},
 {40962,
  [{name,<<"Query DDoc Meta Data">>},
   {description,<<"Design Doc Meta Data Query Request">>},
   {enabled,true},
   {module,view_engine}]},
 {40963,
  [{name,<<"View Query">>},
   {description,<<"View Query Request">>},
   {enabled,false},
   {module,view_engine}]},
 {40964,
  [{name,<<"Update Design Doc">>},
   {description,<<"Design Doc is Updated">>},
   {enabled,true},
   {module,view_engine}]}]
[ns_server:debug,2020-04-02T20:08:15.923+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
scramsha_fallback_salt ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057495}}]}|
 <<53,116,70,173,103,129,158,204,100,121,119,133>>]
[ns_server:debug,2020-04-02T20:08:15.923+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/eventing/settings/config">>} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057495}}]}|
 <<"{\"ram_quota\":256}">>]
[ns_server:debug,2020-04-02T20:08:15.923+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{2,63753057495}}]}|
 <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_si"...>>]
[ns_server:debug,2020-04-02T20:08:15.923+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
client_cert_auth ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057495}}]},
 {state,"disable"},
 {prefixes,[]}]
[ns_server:debug,2020-04-02T20:08:15.923+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
cluster_compat_version ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{5,63753057495}}]},6,5]
[ns_server:debug,2020-04-02T20:08:15.923+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
audit ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057495}}]},
 {enabled,[]},
 {disabled_users,[]},
 {auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2020-04-02T20:08:15.923+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{2,63753057495}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,false}]
[ns_server:debug,2020-04-02T20:08:15.923+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
buckets ->
[[{<<"9245db3c028cb7987096449d46433aa1">>,{2,63753057495}}],{configs,[]}]
[ns_server:debug,2020-04-02T20:08:15.923+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
max_bucket_count ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057495}}]}|30]
[ns_server:debug,2020-04-02T20:08:15.923+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
quorum_nodes ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{1,63753057495}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-04-02T20:08:15.923+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"9245db3c028cb7987096449d46433aa1">>} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{5,63753057495}}]}]
[ns_server:debug,2020-04-02T20:08:15.924+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2020-04-02T20:08:15.933+05:30,ns_1@127.0.0.1:leader_lease_agent<0.459.0>:leader_lease_agent:do_handle_acquire_lease:149]Granting lease to {lease_holder,<<"55657736f3f51d00175de8a7fc1f666f">>,
                                'ns_1@127.0.0.1'} for 15000ms
[error_logger:info,2020-04-02T20:08:15.934+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.497.0>},
                       {id,ns_janitor_server},
                       {mfargs,{ns_janitor_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:08:15.935+05:30,ns_1@127.0.0.1:<0.493.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:302]Acquired lease from node 'ns_1@127.0.0.1' (lease uuid: <<"55657736f3f51d00175de8a7fc1f666f">>)
[ns_server:info,2020-04-02T20:08:15.936+05:30,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.494.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.498.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-04-02T20:08:15.937+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.498.0>},
                       {id,auto_reprovision},
                       {mfargs,{auto_reprovision,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.938+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:apply_changed_memcached_config:179]New memcached config is hot-reloadable.
[ns_server:debug,2020-04-02T20:08:15.938+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:do_read_current_memcached_config:287]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2020-04-02T20:08:15.938+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:89]Refresh of [rbac,isasl] succeeded
[ns_server:info,2020-04-02T20:08:15.939+05:30,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.494.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.499.0> on 'ns_1@127.0.0.1'

[ns_server:info,2020-04-02T20:08:15.939+05:30,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.494.0>:misc:start_singleton:857]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.500.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-04-02T20:08:15.939+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.499.0>},
                       {id,auto_rebalance},
                       {mfargs,{auto_rebalance,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.939+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.500.0>},
                       {id,ns_orchestrator},
                       {mfargs,{ns_orchestrator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.939+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.494.0>},
                       {id,ns_orchestrator_child_sup},
                       {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:08:15.941+05:30,ns_1@127.0.0.1:<0.502.0>:auto_failover:init:185]init auto_failover.
[user:info,2020-04-02T20:08:15.941+05:30,ns_1@127.0.0.1:<0.502.0>:auto_failover:handle_call:216]Enabled auto-failover with timeout 120 and max count 1
[user:info,2020-04-02T20:08:15.943+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:hot_reload_config:248]Hot-reloaded memcached.json for config change of the following keys: [<<"client_cert_auth">>,
                                                                      <<"datatype_snappy">>,
                                                                      <<"scramsha_fallback_salt">>]
[ns_server:info,2020-04-02T20:08:15.944+05:30,ns_1@127.0.0.1:ns_orchestrator_sup<0.472.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.502.0> on 'ns_1@127.0.0.1'

[ns_server:debug,2020-04-02T20:08:15.944+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"9245db3c028cb7987096449d46433aa1">>} ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{6,63753057495}}]}]
[error_logger:info,2020-04-02T20:08:15.944+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.502.0>},
                       {id,auto_failover},
                       {mfargs,{auto_failover,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.944+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"9245db3c028cb7987096449d46433aa1">>,{2,63753057495}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,false}]
[ns_server:info,2020-04-02T20:08:15.944+05:30,ns_1@127.0.0.1:mb_master_sup<0.466.0>:misc:start_singleton:857]start_singleton(work_queue, start_link, [{via,leader_registry,collections}]): started as <0.507.0> on 'ns_1@127.0.0.1'

[ns_server:debug,2020-04-02T20:08:15.944+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([auto_failover_cfg,
                               {local_changes_count,
                                   <<"9245db3c028cb7987096449d46433aa1">>}]..)
[error_logger:info,2020-04-02T20:08:15.944+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.472.0>},
                       {id,ns_orchestrator_sup},
                       {mfargs,{ns_orchestrator_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:08:15.944+05:30,ns_1@127.0.0.1:<0.455.0>:restartable:start_child:98]Started child process <0.456.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2020-04-02T20:08:15.944+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.507.0>},
                       {id,collections},
                       {mfargs,{collections,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.944+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.464.0>},
                       {id,mb_master},
                       {mfargs,{mb_master,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:15.945+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.460.0>},
                       {id,leader_registry_sup},
                       {mfargs,{leader_registry_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:15.945+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.455.0>},
                       {name,leader_services_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{leader_services_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:15.946+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.512.0>},
                       {name,ns_tick_agent},
                       {mfargs,{ns_tick_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.946+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.514.0>},
                       {name,master_activity_events_ingress},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events_ingress}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.946+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.515.0>},
                       {name,master_activity_events_timestamper},
                       {mfargs,
                           {master_activity_events,start_link_timestamper,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.947+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.516.0>},
                       {name,master_activity_events_pids_watcher},
                       {mfargs,
                           {master_activity_events_pids_watcher,start_link,
                               []}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.948+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.517.0>},
                       {name,master_activity_events_keeper},
                       {mfargs,{master_activity_events_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.951+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.520.0>},
                       {id,ns_server_monitor},
                       {mfargs,{ns_server_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.951+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.522.0>},
                       {id,service_monitor_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_monitor_children_sup},
                                health_monitor_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:15.951+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.523.0>},
                       {id,service_monitor_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<health_monitor_sup.0.112499759>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.953+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.529.0>},
                       {id,node_monitor},
                       {mfargs,{node_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.954+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.535.0>},
                       {id,node_status_analyzer},
                       {mfargs,{node_status_analyzer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:08:15.954+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.519.0>},
                       {name,health_monitor_sup},
                       {mfargs,{health_monitor_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:15.955+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.537.0>},
                       {name,rebalance_agent},
                       {mfargs,{rebalance_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.957+05:30,ns_1@127.0.0.1:ns_server_nodes_sup<0.209.0>:one_shot_barrier:notify:27]Notifying on barrier menelaus_barrier
[ns_server:debug,2020-04-02T20:08:15.957+05:30,ns_1@127.0.0.1:menelaus_barrier<0.211.0>:one_shot_barrier:barrier_body:62]Barrier menelaus_barrier got notification from <0.209.0>
[error_logger:info,2020-04-02T20:08:15.957+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.538.0>},
                       {name,ns_rebalance_report_manager},
                       {mfargs,{ns_rebalance_report_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.957+05:30,ns_1@127.0.0.1:ns_server_nodes_sup<0.209.0>:one_shot_barrier:notify:32]Successfuly notified on barrier menelaus_barrier
[error_logger:info,2020-04-02T20:08:15.957+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.249.0>},
                       {name,ns_server_sup},
                       {mfargs,{ns_server_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:08:15.957+05:30,ns_1@127.0.0.1:<0.208.0>:restartable:start_child:98]Started child process <0.209.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2020-04-02T20:08:15.957+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.208.0>},
                       {id,ns_server_nodes_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_server_nodes_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:08:15.958+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.225.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@",admin}
[error_logger:info,2020-04-02T20:08:15.959+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.540.0>},
                       {id,remote_api},
                       {mfargs,{remote_api,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:08:15.959+05:30,ns_1@127.0.0.1:<0.5.0>:child_erlang:child_loop:130]26251: Entered child_loop
[error_logger:info,2020-04-02T20:08:15.960+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.187.0>},
                       {id,ns_server_cluster_sup},
                       {mfargs,{ns_server_cluster_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:08:15.960+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
         application: ns_server
          started_at: 'ns_1@127.0.0.1'

[ns_server:debug,2020-04-02T20:08:15.965+05:30,ns_1@127.0.0.1:json_rpc_connection-goxdcr-cbauth<0.541.0>:json_rpc_connection:init:73]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.541.0>
[ns_server:debug,2020-04-02T20:08:15.966+05:30,ns_1@127.0.0.1:json_rpc_connection-saslauthd-saslauthd-port<0.542.0>:json_rpc_connection:init:73]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.542.0>
[ns_server:debug,2020-04-02T20:08:15.966+05:30,ns_1@127.0.0.1:menelaus_cbauth<0.374.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"goxdcr-cbauth",<0.541.0>} started
[ns_server:debug,2020-04-02T20:08:15.967+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.225.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:debug,2020-04-02T20:08:16.779+05:30,ns_1@127.0.0.1:ns_audit_cfg<0.390.0>:ns_audit_cfg:write_audit_json:259]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "48537283"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2020-04-02T20:08:16.791+05:30,ns_1@127.0.0.1:ns_audit_cfg<0.390.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[ns_server:debug,2020-04-02T20:08:16.809+05:30,ns_1@127.0.0.1:terse_cluster_info_uploader<0.399.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":6,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"kv\":11210,\"capi\":8092,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-04-02T20:08:16.943+05:30,ns_1@127.0.0.1:<0.502.0>:auto_failover_logic:log_master_activity:177]Transitioned node {'ns_1@127.0.0.1',<<"9245db3c028cb7987096449d46433aa1">>} state new -> up
[ns_server:debug,2020-04-02T20:08:45.865+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:08:45.865+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:08:45.865+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:08:45.866+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:09:15.866+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:09:15.866+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:09:15.866+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:09:15.866+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:09:29.745+05:30,ns_1@127.0.0.1:ldap_auth_cache<0.217.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-04-02T20:09:45.867+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:09:45.867+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:09:45.867+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:09:45.867+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:10:15.868+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:10:15.868+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:10:15.868+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:10:15.868+05:30,ns_1@127.0.0.1:compaction_daemon<0.448.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:10:27.511+05:30,ns_1@127.0.0.1:<0.605.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.370.0>} exited with reason normal
[ns_server:debug,2020-04-02T20:10:27.511+05:30,ns_1@127.0.0.1:<0.588.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.567.0>} exited with reason normal
[ns_server:debug,2020-04-02T20:10:27.511+05:30,ns_1@127.0.0.1:<0.568.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.360.0>} exited with reason normal
[ns_server:debug,2020-04-02T20:10:27.511+05:30,ns_1@127.0.0.1:<0.548.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.359.0>} exited with reason normal
[ns_server:debug,2020-04-02T20:10:27.511+05:30,ns_1@127.0.0.1:json_rpc_connection-goxdcr-cbauth<0.541.0>:json_rpc_connection:handle_info:129]Socket closed
[ns_server:debug,2020-04-02T20:10:27.511+05:30,ns_1@127.0.0.1:menelaus_cbauth<0.374.0>:menelaus_cbauth:handle_info:142]Observed json rpc process {"goxdcr-cbauth",<0.541.0>} died with reason shutdown
[user:debug,2020-04-02T20:10:27.512+05:30,ns_1@127.0.0.1:<0.256.0>:ns_log:crash_consumption_loop:69]Service 'goxdcr' exited with status 0. Restarting. Messages:
2020-04-02T20:10:15.982+05:30 INFO GOXDCR.ReplMgr: Mem stats = {"Alloc":1732952,"TotalAlloc":1732952,"Sys":72153336,"Lookups":0,"Mallocs":17195,"Frees":644,"HeapAlloc":1732952,"HeapSys":66322432,"HeapIdle":63053824,"HeapInuse":3268608,"HeapReleased":0,"HeapObjects":16551,"StackInuse":786432,"StackSys":786432,"MSpanInuse":50768,"MSpanSys":65536,"MCacheInuse":6912,"MCacheSys":16384,"BuckHashSys":1444257,"GCSys":2234368,"OtherSys":1283927,"NextGC":4473924,"LastGC":0,"PauseTotalNs":0,"PauseNs":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"PauseEnd":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"NumGC":0,"NumForcedGC":0,"GCCPUFraction":0,"EnableGC":true,"DebugGC":false,"BySize":[{"Size":0,"Mallocs":0,"Frees":0},{"Size":8,"Mallocs":337,"Frees":0},{"Size":16,"Mallocs":3769,"Frees":0},{"Size":32,"Mallocs":5339,"Frees":0},{"Size":48,"Mallocs":2627,"Frees":0},{"Size":64,"Mallocs":662,"Frees":0},{"Size":80,"Mallocs":600,"Frees":0},{"Size":96,"Mallocs":656,"Frees":0},{"Size":112,"Mallocs":964,"Frees":0},{"Size":128,"Mallocs":162,"Frees":0},{"Size":144,"Mallocs":72,"Frees":0},{"Size":160,"Mallocs":132,"Frees":0},{"Size":176,"Mallocs":36,"Frees":0},{"Size":192,"Mallocs":10,"Frees":0},{"Size":208,"Mallocs":185,"Frees":0},{"Size":224,"Mallocs":78,"Frees":0},{"Size":240,"Mallocs":2,"Frees":0},{"Size":256,"Mallocs":122,"Frees":0},{"Size":288,"Mallocs":77,"Frees":0},{"Size":320,"Mallocs":42,"Frees":0},{"Size":352,"Mallocs":269,"Frees":0},{"Size":384,"Mallocs":92,"Frees":0},{"Size":416,"Mallocs":27,"Frees":0},{"Size":448,"Mallocs":2,"Frees":0},{"Size":480,"Mallocs":2,"Frees":0},{"Size":512,"Mallocs":25,"Frees":0},{"Size":576,"Mallocs":20,"Frees":0},{"Size":640,"Mallocs":23,"Frees":0},{"Size":704,"Mallocs":21,"Frees":0},{"Size":768,"Mallocs":3,"Frees":0},{"Size":896,"Mallocs":31,"Frees":0},{"Size":1024,"Mallocs":7,"Frees":0},{"Size":1152,"Mallocs":20,"Frees":0},{"Size":1280,"Mallocs":12,"Frees":0},{"Size":1408,"Mallocs":4,"Frees":0},{"Size":1536,"Mallocs":3,"Frees":0},{"Size":1792,"Mallocs":10,"Frees":0},{"Size":2048,"Mallocs":23,"Frees":0},{"Size":2304,"Mallocs":10,"Frees":0},{"Size":2688,"Mallocs":11,"Frees":0},{"Size":3072,"Mallocs":0,"Frees":0},{"Size":3200,"Mallocs":1,"Frees":0},{"Size":3456,"Mallocs":1,"Frees":0},{"Size":4096,"Mallocs":26,"Frees":0},{"Size":4864,"Mallocs":3,"Frees":0},{"Size":5376,"Mallocs":4,"Frees":0},{"Size":6144,"Mallocs":4,"Frees":0},{"Size":6528,"Mallocs":4,"Frees":0},{"Size":6784,"Mallocs":1,"Frees":0},{"Size":6912,"Mallocs":0,"Frees":0},{"Size":8192,"Mallocs":4,"Frees":0},{"Size":9472,"Mallocs":5,"Frees":0},{"Size":9728,"Mallocs":0,"Frees":0},{"Size":10240,"Mallocs":0,"Frees":0},{"Size":10880,"Mallocs":2,"Frees":0},{"Size":12288,"Mallocs":0,"Frees":0},{"Size":13568,"Mallocs":0,"Frees":0},{"Size":14336,"Mallocs":0,"Frees":0},{"Size":16384,"Mallocs":2,"Frees":0},{"Size":18432,"Mallocs":3,"Frees":0},{"Size":19072,"Mallocs":0,"Frees":0}]}
2020-04-02T20:10:25.980+05:30 INFO GOXDCR.ResourceMgr: Resource Manager State = overallTP: 0 highTP: 0 highExist: false lowExist: false backlogExist: false maxTP: 0 highTPNeeded: 0 highTokens: 0 maxTokens: 0 lowTPLimit: 0 calibration: None dcpAction: Reset processCpu: 0 idleCpu: 91
2020-04-02T20:10:25.980+05:30 INFO GOXDCR.ResourceMgr: backlogCount=0, noBacklogCount=130 extraQuota=false cpuNotMaxedCount=0 throughputDropCount=0
2020-04-02T20:10:25.980+05:30 INFO GOXDCR.ResourceMgr: DcpPriorityMap=map[]
ongoingReplMap=map[]

[ns_server:debug,2020-04-02T20:10:27.513+05:30,ns_1@127.0.0.1:json_rpc_connection-saslauthd-saslauthd-port<0.542.0>:json_rpc_connection:handle_info:129]Socket closed
[user:debug,2020-04-02T20:10:27.513+05:30,ns_1@127.0.0.1:<0.256.0>:ns_log:crash_consumption_loop:69]Service 'saslauthd_port' exited with status 0. Restarting. Messages:
2020/04/02 20:10:27 Got EOL. Exiting
[user:debug,2020-04-02T20:10:27.515+05:30,ns_1@127.0.0.1:<0.256.0>:ns_log:crash_consumption_loop:69]Service 'memcached' exited with status 0. Restarting. Messages:
EOL on stdin.  Initiating shutdown
[ns_server:debug,2020-04-02T20:10:27.516+05:30,ns_1@127.0.0.1:<0.421.0>:remote_monitors:monitor_loop:129]Monitored remote process <12938.114.0> went down with: shutdown
[ns_server:debug,2020-04-02T20:10:27.516+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:handle_info:163]Got DOWN with reason: shutdown from memcached port server: <12938.114.0>. Shutting down
[ns_server:debug,2020-04-02T20:10:27.516+05:30,ns_1@127.0.0.1:<0.413.0>:remote_monitors:monitor_loop:129]Monitored remote process <12938.107.0> went down with: shutdown
[ns_server:debug,2020-04-02T20:10:27.516+05:30,ns_1@127.0.0.1:ns_ports_setup<0.380.0>:ns_ports_setup:children_loop_continue:121]ns_child_ports_sup <12938.107.0> died on babysitter node with shutdown. Restart.
[ns_server:debug,2020-04-02T20:10:27.516+05:30,ns_1@127.0.0.1:<0.429.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.396.0>} exited with reason {shutdown,
                                                                                {memcached_port_server_down,
                                                                                 <12938.114.0>,
                                                                                 shutdown}}
[ns_server:debug,2020-04-02T20:10:27.516+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.3248.0>:memcached_config_mgr:init:49]waiting for completion of initial ns_ports_setup round
[error_logger:error,2020-04-02T20:10:27.516+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {shutdown,
                     {memcached_port_server_down,<12938.114.0>,shutdown}}
     Offender:   [{pid,<0.396.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2020-04-02T20:10:27.516+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.3251.0>:memcached_config_mgr:init:49]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2020-04-02T20:10:27.516+05:30,ns_1@127.0.0.1:<0.382.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.380.0>} exited with reason {{child_ports_sup_died,
                                                                                    <12938.107.0>,
                                                                                    shutdown},
                                                                                   [{ns_ports_setup,
                                                                                     children_loop_continue,
                                                                                     3,
                                                                                     [{file,
                                                                                       "src/ns_ports_setup.erl"},
                                                                                      {line,
                                                                                       122}]},
                                                                                    {proc_lib,
                                                                                     wake_up,
                                                                                     3,
                                                                                     [{file,
                                                                                       "proc_lib.erl"},
                                                                                      {line,
                                                                                       257}]}]}
[ns_server:debug,2020-04-02T20:10:27.516+05:30,ns_1@127.0.0.1:<0.381.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.380.0>} exited with reason {{child_ports_sup_died,
                                                                                 <12938.107.0>,
                                                                                 shutdown},
                                                                                [{ns_ports_setup,
                                                                                  children_loop_continue,
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/ns_ports_setup.erl"},
                                                                                   {line,
                                                                                    122}]},
                                                                                 {proc_lib,
                                                                                  wake_up,
                                                                                  3,
                                                                                  [{file,
                                                                                    "proc_lib.erl"},
                                                                                   {line,
                                                                                    257}]}]}
[ns_server:debug,2020-04-02T20:10:27.516+05:30,ns_1@127.0.0.1:<0.5.0>:child_erlang:child_loop:134]26251: Got EOL
[ns_server:info,2020-04-02T20:10:27.516+05:30,ns_1@127.0.0.1:<0.5.0>:ns_bootstrap:stop:40]Initiated server shutdown
[ns_server:debug,2020-04-02T20:10:27.518+05:30,ns_1@127.0.0.1:<0.539.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.538.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.518+05:30,ns_1@127.0.0.1:<0.536.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.535.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.518+05:30,ns_1@127.0.0.1:<0.530.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.529.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.518+05:30,ns_1@127.0.0.1:<0.524.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.523.0>} exited with reason shutdown
[error_logger:error,2020-04-02T20:10:27.518+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_ports_setup:setup_body_tramp/0
    pid: <0.380.0>
    registered_name: ns_ports_setup
    exception error: {child_ports_sup_died,<12938.107.0>,shutdown}
      in function  ns_ports_setup:children_loop_continue/3 (src/ns_ports_setup.erl, line 122)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.208.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.381.0>,<0.382.0>,<0.249.0>]
    dictionary: [{'ns_ports_setup-indexer-available',
                      "/opt/couchbase/bin/indexer"},
                  {'ns_ports_setup-eventing-producer-available',
                      "/opt/couchbase/bin/eventing-producer"},
                  {'ns_ports_setup-saslauthd-port-available',
                      "/opt/couchbase/bin/saslauthd-port"},
                  {'ns_ports_setup-cbas-available',"/opt/couchbase/bin/cbas"},
                  {'ns_ports_setup-cbft-available',"/opt/couchbase/bin/cbft"},
                  {'ns_ports_setup-cbq-engine-available',
                      "/opt/couchbase/bin/cbq-engine"},
                  {'ns_ports_setup-goxdcr-available',
                      "/opt/couchbase/bin/goxdcr"},
                  {'ns_ports_setup-projector-available',
                      "/opt/couchbase/bin/projector"}]
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 11749
  neighbours:

[ns_server:debug,2020-04-02T20:10:27.518+05:30,ns_1@127.0.0.1:ns_ports_setup<0.3249.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,goxdcr]
[ns_server:debug,2020-04-02T20:10:27.518+05:30,ns_1@127.0.0.1:<0.518.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {master_activity_events,<0.517.0>} exited with reason killed
[ns_server:debug,2020-04-02T20:10:27.518+05:30,ns_1@127.0.0.1:<0.521.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.520.0>} exited with reason shutdown
[ns_server:info,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:mb_master<0.464.0>:mb_master:terminate:327]Synchronously shutting down child mb_master_sup
[ns_server:debug,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:<0.513.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {leader_events,<0.512.0>} exited with reason shutdown
[error_logger:error,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.3248.0>
    registered_name: memcached_config_mgr
    exception exit: {noproc,{gen_server,call,[ns_ports_setup,sync,infinity]}}
      in function  gen_server:call/3 (gen_server.erl, line 214)
      in call from memcached_config_mgr:init/1 (src/memcached_config_mgr.erl, line 50)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.208.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.249.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 987
    stack_size: 27
    reductions: 1517
  neighbours:

[ns_server:info,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_down:253]Process <0.507.0> registered as 'collections' terminated.
[ns_server:debug,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:<0.503.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {compat_mode_events,<0.502.0>} exited with reason shutdown
[ns_server:info,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_down:253]Process <0.502.0> registered as 'auto_failover' terminated.
[error_logger:info,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.3248.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_down:253]Process <0.500.0> registered as 'ns_orchestrator' terminated.
[ns_server:info,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_down:253]Process <0.499.0> registered as 'auto_rebalance' terminated.
[ns_server:info,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_down:253]Process <0.498.0> registered as 'auto_reprovision' terminated.
[error_logger:error,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{child_ports_sup_died,<12938.107.0>,shutdown},
                  [{ns_ports_setup,children_loop_continue,3,
                                   [{file,"src/ns_ports_setup.erl"},
                                    {line,122}]},
                   {proc_lib,wake_up,3,[{file,"proc_lib.erl"},{line,257}]}]}
     Offender:   [{pid,<0.380.0>},
                  {name,ns_ports_setup},
                  {mfargs,{ns_ports_setup,start,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,brutal_kill},
                  {child_type,worker}]


[ns_server:debug,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:leader_activities<0.458.0>:leader_activities:handle_internal_process_down:511]Process {quorum_nodes_manager,<0.469.0>} terminated with reason shutdown
[ns_server:info,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_down:253]Process <0.471.0> registered as 'ns_tick' terminated.
[ns_server:debug,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:<0.487.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.469.0>} exited with reason shutdown
[error_logger:info,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.3249.0>},
                       {name,ns_ports_setup},
                       {mfargs,{ns_ports_setup,start,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:leader_activities<0.458.0>:leader_activities:handle_internal_process_down:511]Process {acquirer,<0.467.0>} terminated with reason shutdown
[error_logger:error,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {noproc,{gen_server,call,[ns_ports_setup,sync,infinity]}}
     Offender:   [{pid,<0.3248.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:<0.478.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.467.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:leader_lease_agent<0.459.0>:leader_lease_agent:handle_abolish_lease:255]Received abolish lease request from {lease_holder,
                                     <<"55657736f3f51d00175de8a7fc1f666f">>,
                                     'ns_1@127.0.0.1'} when lease is {lease,
                                                                      {lease_holder,
                                                                       <<"55657736f3f51d00175de8a7fc1f666f">>,
                                                                       'ns_1@127.0.0.1'},
                                                                      -576460619863759500,
                                                                      -576460604863759500,
                                                                      {timer,
                                                                       #Ref<0.1520182418.164364292.197201>,
                                                                       {lease_expired,
                                                                        {lease_holder,
                                                                         <<"55657736f3f51d00175de8a7fc1f666f">>,
                                                                         'ns_1@127.0.0.1'}}},
                                                                      active}
[error_logger:info,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.3251.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:leader_lease_agent<0.459.0>:leader_lease_agent:handle_abolish_lease:260]Expiring abolished lease
[error_logger:info,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]Initiated server shutdown
[ns_server:debug,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_new_leader:241]New leader is undefined. Invalidating name cache.
[ns_server:debug,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:<0.465.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.464.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:<0.462.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {leader_events,<0.461.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:leader_activities<0.458.0>:leader_activities:handle_internal_process_down:511]Process {agent,<0.459.0>} terminated with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:<0.455.0>:restartable:shutdown_child:120]Successfully terminated process <0.456.0>
[ns_server:debug,2020-04-02T20:10:27.520+05:30,ns_1@127.0.0.1:<0.449.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.448.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.520+05:30,ns_1@127.0.0.1:<0.447.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.446.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.520+05:30,ns_1@127.0.0.1:<0.445.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.443.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.520+05:30,ns_1@127.0.0.1:<0.444.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.443.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.520+05:30,ns_1@127.0.0.1:<0.438.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.437.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.520+05:30,ns_1@127.0.0.1:<0.442.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.440.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.519+05:30,ns_1@127.0.0.1:ns_ports_setup<0.3249.0>:misc:delaying_crash:1608]Delaying crash exit:{noproc,
                     {gen_server,call,
                      [{ns_ports_manager,'babysitter_of_ns_1@cb.local'},
                       {set_dynamic_children,
                        [{memcached,"/opt/couchbase/bin/memcached",
                          ["-C",
                           "/opt/couchbase/var/lib/couchbase/config/memcached.json"],
                          [{env,
                            [{"EVENT_NOSELECT","1"},
                             {"MEMCACHED_TOP_KEYS","5"},
                             {"CBSASL_PWFILE",
                              "/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
                           use_stdio,stderr_to_stdout,exit_status,
                           port_server_dont_start,stream]},
                         {saslauthd_port,"/opt/couchbase/bin/saslauthd-port",
                          [],
                          [use_stdio,exit_status,stderr_to_stdout,
                           {env,
                            [{"GOTRACEBACK","single"},
                             {"CBAUTH_REVRPC_URL",
                              "http://%40:3024d106b34044209b1fdd48b264cc55@127.0.0.1:8091/saslauthd"}]}]},
                         {goxdcr,"/opt/couchbase/bin/goxdcr",
                          ["-sourceKVAdminPort=8091","-xdcrRestPort=9998",
                           "-isEnterprise=false","-ipv6=false"],
                          [via_goport,exit_status,stderr_to_stdout,
                           {env,
                            [{"GOTRACEBACK","single"},
                             {"CBAUTH_REVRPC_URL",
                              "http://%40:3024d106b34044209b1fdd48b264cc55@127.0.0.1:8091/goxdcr"}]},
                           {log,"goxdcr.log"}]}]},
                       infinity]}} by 1000ms
Stacktrace: [{gen_server,call,3,[{file,"gen_server.erl"},{line,214}]},
             {ns_ports_setup,set_children,2,
                             [{file,"src/ns_ports_setup.erl"},{line,81}]},
             {ns_ports_setup,set_children_and_loop,3,
                             [{file,"src/ns_ports_setup.erl"},{line,97}]},
             {misc,delaying_crash,2,[{file,"src/misc.erl"},{line,1605}]},
             {proc_lib,init_p_do_apply,3,[{file,"proc_lib.erl"},{line,247}]}]
[ns_server:debug,2020-04-02T20:10:27.520+05:30,ns_1@127.0.0.1:<0.439.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.437.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.520+05:30,ns_1@127.0.0.1:<0.441.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.440.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.520+05:30,ns_1@127.0.0.1:<0.428.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_tick_event,<0.427.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.531+05:30,ns_1@127.0.0.1:<0.425.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_stats_event,<0.424.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.531+05:30,ns_1@127.0.0.1:<0.423.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_tick_event,<0.422.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.544+05:30,ns_1@127.0.0.1:<0.419.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_stats_event,<0.418.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.579+05:30,ns_1@127.0.0.1:<0.416.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_stats_event,<0.415.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.603+05:30,ns_1@127.0.0.1:<0.412.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_stats_event,<0.411.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.603+05:30,ns_1@127.0.0.1:<0.406.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.405.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.603+05:30,ns_1@127.0.0.1:<0.409.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ale_stats_events,<0.407.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.603+05:30,ns_1@127.0.0.1:<0.410.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_tick_event,<0.407.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.604+05:30,ns_1@127.0.0.1:<0.400.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {bucket_info_cache_invalidations,<0.399.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.604+05:30,ns_1@127.0.0.1:<0.391.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.390.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.604+05:30,ns_1@127.0.0.1:<0.386.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.385.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.604+05:30,ns_1@127.0.0.1:<0.389.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.388.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.604+05:30,ns_1@127.0.0.1:<0.3252.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.3249.0>} exited with reason killed
[ns_server:debug,2020-04-02T20:10:27.604+05:30,ns_1@127.0.0.1:<0.3250.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.3249.0>} exited with reason killed
[ns_server:debug,2020-04-02T20:10:27.604+05:30,ns_1@127.0.0.1:<0.376.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.374.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.604+05:30,ns_1@127.0.0.1:<0.379.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ssl_service_events,<0.374.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.604+05:30,ns_1@127.0.0.1:<0.375.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {json_rpc_events,<0.374.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.605+05:30,ns_1@127.0.0.1:<0.377.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.374.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.604+05:30,ns_1@127.0.0.1:<0.378.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.374.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.606+05:30,ns_1@127.0.0.1:<0.326.0>:restartable:shutdown_child:120]Successfully terminated process <0.332.0>
[ns_server:debug,2020-04-02T20:10:27.606+05:30,ns_1@127.0.0.1:<0.320.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.319.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.607+05:30,ns_1@127.0.0.1:<0.313.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.312.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.607+05:30,ns_1@127.0.0.1:<0.315.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.314.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.607+05:30,ns_1@127.0.0.1:<0.311.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.310.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.607+05:30,ns_1@127.0.0.1:<0.299.0>:restartable:shutdown_child:120]Successfully terminated process <0.300.0>
[ns_server:debug,2020-04-02T20:10:27.607+05:30,ns_1@127.0.0.1:<0.303.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.302.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.608+05:30,ns_1@127.0.0.1:<0.295.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {buckets_events,<0.294.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.608+05:30,ns_1@127.0.0.1:<0.285.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.284.0>} exited with reason killed
[ns_server:debug,2020-04-02T20:10:27.608+05:30,ns_1@127.0.0.1:<0.288.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.286.0>} exited with reason killed
[ns_server:debug,2020-04-02T20:10:27.608+05:30,ns_1@127.0.0.1:<0.273.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events_local,<0.272.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.609+05:30,ns_1@127.0.0.1:<0.261.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.260.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.609+05:30,ns_1@127.0.0.1:<0.262.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.260.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.609+05:30,ns_1@127.0.0.1:<0.258.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.257.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.609+05:30,ns_1@127.0.0.1:<0.259.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.257.0>} exited with reason shutdown
[error_logger:error,2020-04-02T20:10:27.609+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: gen_event:init_it/6
    pid: <0.287.0>
    registered_name: bucket_info_cache_invalidations
    exception exit: killed
      in function  gen_event:terminate_server/4 (gen_event.erl, line 351)
    ancestors: [bucket_info_cache,ns_server_sup,ns_server_nodes_sup,
                  <0.208.0>,ns_server_cluster_sup,root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: []
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 280
  neighbours:

[ns_server:debug,2020-04-02T20:10:27.611+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.231.0>:ns_port_server:terminate:196]Shutting down port ns_couchdb
[ns_server:debug,2020-04-02T20:10:27.611+05:30,ns_1@127.0.0.1:<0.247.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.236.0> died with shutdown. Exiting
[ns_server:debug,2020-04-02T20:10:27.611+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.231.0>:ns_port_server:port_shutdown:297]Shutdown command: "shutdown"
[ns_server:debug,2020-04-02T20:10:27.621+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1520182418.164364289.186431>,
                               inet_tcp_dist,<0.242.0>,
                               #Ref<0.1520182418.164364289.186433>}
[error_logger:info,2020-04-02T20:10:27.621+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.242.0>,connection_closed}}
[ns_server:debug,2020-04-02T20:10:27.621+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.231.0>:ns_port_server:terminate:199]ns_couchdb has exited
[ns_server:info,2020-04-02T20:10:27.621+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.231.0>:ns_port_server:log:224]ns_couchdb<0.231.0>: 26323: got shutdown request. Exiting
ns_couchdb<0.231.0>: [os_mon] cpu supervisor port (cpu_sup): Erlang has closed
ns_couchdb<0.231.0>: [os_mon] memory supervisor port (memsup): Erlang has closed

[error_logger:info,2020-04-02T20:10:27.621+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:10:27.621+05:30,ns_1@127.0.0.1:<0.230.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.228.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.621+05:30,ns_1@127.0.0.1:<0.229.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.228.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.621+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T20:10:27.621+05:30,ns_1@127.0.0.1:<0.227.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.225.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.621+05:30,ns_1@127.0.0.1:<0.226.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.225.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.621+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1520182418.164364291.191397>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T20:10:27.622+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1520182418.164364291.191397>,
                                  inet_tcp_dist,<0.3256.0>,
                                  #Ref<0.1520182418.164364291.191398>}
[ns_server:debug,2020-04-02T20:10:27.622+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1520182418.164364291.191397>,
                               inet_tcp_dist,<0.3256.0>,
                               #Ref<0.1520182418.164364291.191398>}
[error_logger:info,2020-04-02T20:10:27.622+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.3256.0>,shutdown}}
[error_logger:info,2020-04-02T20:10:27.622+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:10:27.622+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T20:10:27.623+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:10:27.623+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1520182418.164364290.190229>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T20:10:27.623+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1520182418.164364290.190229>,
                                  inet_tcp_dist,<0.3258.0>,
                                  #Ref<0.1520182418.164364291.191400>}
[ns_server:debug,2020-04-02T20:10:27.623+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1520182418.164364290.190229>,
                               inet_tcp_dist,<0.3258.0>,
                               #Ref<0.1520182418.164364291.191400>}
[error_logger:info,2020-04-02T20:10:27.623+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.3258.0>,shutdown}}
[ns_server:debug,2020-04-02T20:10:27.623+05:30,ns_1@127.0.0.1:<0.218.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.217.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.623+05:30,ns_1@127.0.0.1:<0.208.0>:restartable:shutdown_child:120]Successfully terminated process <0.209.0>
[error_logger:info,2020-04-02T20:10:27.623+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:10:27.624+05:30,ns_1@127.0.0.1:<0.203.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.202.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:10:27.624+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:wait_saver:866]Done waiting for saver.
[ns_server:info,2020-04-02T20:10:27.625+05:30,ns_1@127.0.0.1:<0.5.0>:ns_bootstrap:stop:44]Successfully stopped ns_server
[error_logger:info,2020-04-02T20:10:27.625+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
         application: ns_server
              exited: stopped
                type: permanent

[ns_server:info,2020-04-02T20:11:11.568+05:30,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.580+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.581+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.581+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.581+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.581+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-04-02T20:11:11.581+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-04-02T20:11:11.585+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-04-02T20:11:11.585+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[error_logger:info,2020-04-02T20:11:11.586+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.168.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2020-04-02T20:11:11.586+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.167.0>},
                       {id,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:11:11.588+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@127.0.0.1'
[error_logger:info,2020-04-02T20:11:11.598+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.598+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.599+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:11.600+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.174.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:11.601+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2020-04-02T20:11:11.601+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.176.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:11.602+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.177.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:11.602+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.175.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:11.602+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:11.603+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.603+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.604+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.180.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:11.605+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-04-02T20:11:11.605+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-04-02T20:11:11.605+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2020-04-02T20:11:11.606+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-04-02T20:11:11.606+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.181.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.606+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.169.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-04-02T20:11:11.607+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-04-02T20:11:11.612+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-04-02T20:11:11.612+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2020-04-02T20:11:11.612+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:11:11.612+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T20:11:11.612+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1274859421.433324035.24032>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T20:11:11.612+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1274859421.433324035.24032>,
                                  inet_tcp_dist,<0.185.0>,
                                  #Ref<0.1274859421.433324035.24034>}
[ns_server:debug,2020-04-02T20:11:11.615+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-04-02T20:11:11.615+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": ok
[ns_server:info,2020-04-02T20:11:11.616+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-04-02T20:11:11.617+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-04-02T20:11:11.617+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.621+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:11:11.622+05:30,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,4,2},{20,11,11}}},
               {memory,
                   [{total,26685880},
                    {processes,9853136},
                    {processes_used,9835744},
                    {system,16832744},
                    {atom,388625},
                    {atom_used,364409},
                    {binary,139544},
                    {code,8250921},
                    {ets,1520352}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    error_handler,application,application_master,file,kernel,
                    file_server,heart,file_io_server,application_controller,
                    error_logger,gen_server,proc_lib,code_server,erl_eval,
                    lists,filename,supervisor,gen,erl_parse,ets,code,erl_lint,
                    gen_event,erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{os_mon,"CPO  CXC 138 46","2.4.4"},
                    {sasl,"SASL  CXC 138 11","3.1.2"},
                    {ns_server,"Couchbase server","6.5.0-4966-community"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ale,"Another Logger for Erlang","0.0.0"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,131},
               {node,'ns_1@127.0.0.1'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,auth,httpd_sup,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    release_handler,lhttpc_sup,httpc_sup,lhttpc_manager,
                    alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc',kernel_safe_sup,'sink-disk_metakv',
                    inets_sup,'sink-disk_access_int','sink-disk_access',
                    standard_error,'sink-disk_reports',ale_stats_events,
                    'sink-disk_stats','sink-disk_xdcr',timer_server,
                    'sink-disk_debug',inet_gethost_native,ale_sup,
                    'sink-disk_error',inet_db,'sink-disk_default',
                    ssl_pem_cache_dist,ale_dynamic_sup,rex,global_group,
                    net_sup,kernel_sup,ssl_connection_sup,global_name_server,
                    ssl_admin_sup,tftp_sup,ssl_sup,root_sup,erts_code_purger,
                    os_mon_sup,file_server_2,error_logger,cpu_sup,erl_epmd,
                    init,memsup,erl_signal_server,disksup,ale,net_kernel,
                    dist_manager,ssl_pem_cache,ssl_manager,ssl_dist_admin_sup,
                    ssl_dist_connection_sup,ssl_dist_sup,user,
                    ssl_tls_dist_proxy,ssl_manager_dist,sasl_safe_sup,
                    ssl_listen_tracker_sup,inet_gethost_native_sup,
                    code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,0}]
[ns_server:info,2020-04-02T20:11:11.625+05:30,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"21e0ed4ef2e27d16585f31e4458a1db0546bbb05\" upstream=\"6.5.0\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"5716bf0df2d36db8ff45c6508a328a92b9457cbf\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4966\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"74c19969e8ef1b5309077a03885d00e273378f6c\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"956a2df5f2f2abb48054bc4ce56895ce9618d2ae\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-04-02T20:11:11.627+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.627+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.627+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:11:11.628+05:30,ns_1@127.0.0.1:ns_config_sup<0.192.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-04-02T20:11:11.628+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.628+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.194.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:11:11.643+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-04-02T20:11:11.643+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:info,2020-04-02T20:11:11.644+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1125]No dynamic config file found. Assuming we're brand new node
[ns_server:debug,2020-04-02T20:11:11.645+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[]]
[ns_server:info,2020-04-02T20:11:11.647+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',cbas_debug_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|-1]},
 {{node,'ns_1@127.0.0.1',cbas_parent_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9122]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9121]},
 {{node,'ns_1@127.0.0.1',cbas_replication_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9120]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9119]},
 {{node,'ns_1@127.0.0.1',cbas_messaging_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9118]},
 {{node,'ns_1@127.0.0.1',cbas_result_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9117]},
 {{node,'ns_1@127.0.0.1',cbas_data_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9116]},
 {{node,'ns_1@127.0.0.1',cbas_cluster_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9115]},
 {{node,'ns_1@127.0.0.1',cbas_console_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9114]},
 {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9113]},
 {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9112]},
 {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9111]},
 {{node,'ns_1@127.0.0.1',cbas_admin_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9110]},
 {{node,'ns_1@127.0.0.1',cbas_ssl_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',cbas_http_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   8095]},
 {{node,'ns_1@127.0.0.1',eventing_https_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',eventing_debug_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9140]},
 {{node,'ns_1@127.0.0.1',eventing_http_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   8096]},
 {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',fts_grpc_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9130]},
 {{node,'ns_1@127.0.0.1',fts_ssl_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',fts_http_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   8094]},
 {{node,'ns_1@127.0.0.1',indexer_https_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9105]},
 {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9104]},
 {{node,'ns_1@127.0.0.1',indexer_stinit_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9103]},
 {{node,'ns_1@127.0.0.1',indexer_http_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9102]},
 {{node,'ns_1@127.0.0.1',indexer_scan_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9101]},
 {{node,'ns_1@127.0.0.1',indexer_admin_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9100]},
 {{node,'ns_1@127.0.0.1',ssl_query_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',query_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   8093]},
 {{node,'ns_1@127.0.0.1',projector_ssl_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',projector_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',ssl_capi_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',capi_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   8092]},
 {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',xdcr_rest_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9998]},
 {{node,'ns_1@127.0.0.1',ssl_rest_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',rest},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
   {port,8091},
   {port_meta,global}]},
 {rest,[{port,8091}]},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {drop_request_memory_threshold_mib,undefined},
 {{request_limit,capi},undefined},
 {{request_limit,rest},undefined},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {replication,[{enabled,true}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {{node,'ns_1@127.0.0.1',ns_log},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@127.0.0.1',port_servers},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}]},
 {{node,'ns_1@127.0.0.1',moxi},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
   {port,0}]},
 {secure_headers,[]},
 {buckets,[{configs,[]}]},
 {cbas_memory_quota,2174},
 {fts_memory_quota,512},
 {memory_quota,8886},
 {{node,'ns_1@127.0.0.1',memcached_config},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@127.0.0.1',memcached},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,undefined},
   {ssl_port,undefined},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@127.0.0.1',memcached_defaults},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,false},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {memcached,[]},
 {{node,'ns_1@127.0.0.1',audit},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {{node,'ns_1@127.0.0.1',isasl},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {remote_clusters,[]},
 {rest_creds,null},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":5,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {{node,'ns_1@127.0.0.1',membership},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   active]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
 {quorum_nodes,['ns_1@127.0.0.1']},
 {nodes_wanted,['ns_1@127.0.0.1']},
 {{node,'ns_1@127.0.0.1',compaction_daemon},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {max_bucket_count,30},
 {index_aware_rebalance_disabled,false},
 {{node,'ns_1@127.0.0.1',saslauthd_enabled},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',is_enterprise},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',config_version},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   {6,5}]},
 {{node,'ns_1@127.0.0.1',uuid},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   <<"dce74b57ae3924cb616de84cba56b09d">>]}]
[error_logger:info,2020-04-02T20:11:11.650+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.195.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.650+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.201.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.651+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.202.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.651+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:11:11.652+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',erl_external_listeners} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
 {inet,false},
 {inet6,false}]
[error_logger:info,2020-04-02T20:11:11.652+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.204.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:11:11.652+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',node_encryption} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|false]
[ns_server:debug,2020-04-02T20:11:11.652+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',address_family} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|inet]
[ns_server:debug,2020-04-02T20:11:11.652+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"dce74b57ae3924cb616de84cba56b09d">>} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}]
[error_logger:info,2020-04-02T20:11:11.653+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.207.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:11.657+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:11.657+05:30,ns_1@127.0.0.1:menelaus_barrier<0.211.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-04-02T20:11:11.657+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.657+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.659+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.213.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.659+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.215.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.659+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.214.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:11.663+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.217.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.664+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.220.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.667+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.222.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:11.668+05:30,ns_1@127.0.0.1:users_replicator<0.222.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-04-02T20:11:11.668+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.222.0>
[ns_server:debug,2020-04-02T20:11:11.669+05:30,ns_1@127.0.0.1:users_replicator<0.222.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.223.0>
[ns_server:debug,2020-04-02T20:11:11.669+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-04-02T20:11:11.669+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.223.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.669+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.221.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:11:11.674+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.225.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-04-02T20:11:11.674+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.225.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.676+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.228.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.676+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.219.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:11.676+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.231.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:11.676+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.232.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:11:11.683+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-04-02T20:11:11.684+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 14ms
[ns_server:debug,2020-04-02T20:11:11.685+05:30,ns_1@127.0.0.1:users_replicator<0.222.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[ns_server:debug,2020-04-02T20:11:11.686+05:30,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.236.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[error_logger:info,2020-04-02T20:11:11.686+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.235.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:11.686+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:11:11.686+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T20:11:11.686+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1274859421.433324034.23646>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T20:11:11.686+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1274859421.433324034.23646>,
                                  inet_tcp_dist,<0.239.0>,
                                  #Ref<0.1274859421.433324034.23650>}
[ns_server:debug,2020-04-02T20:11:11.686+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1274859421.433324034.23646>,
                               inet_tcp_dist,<0.239.0>,
                               #Ref<0.1274859421.433324034.23650>}
[error_logger:info,2020-04-02T20:11:11.686+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.239.0>,shutdown}}
[ns_server:debug,2020-04-02T20:11:11.686+05:30,ns_1@127.0.0.1:<0.237.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-04-02T20:11:11.686+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:11:11.887+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T20:11:11.887+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:11:11.887+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1274859421.433324033.23845>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T20:11:11.887+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1274859421.433324033.23845>,
                                  inet_tcp_dist,<0.242.0>,
                                  #Ref<0.1274859421.433324033.23849>}
[ns_server:debug,2020-04-02T20:11:11.919+05:30,ns_1@127.0.0.1:<0.237.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-04-02T20:11:12.120+05:30,ns_1@127.0.0.1:<0.237.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-04-02T20:11:12.355+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.246.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.443+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.236.0>},
                       {name,wait_for_couchdb_node},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.448+05:30,ns_1@127.0.0.1:ns_server_nodes_sup<0.209.0>:ns_storage_conf:setup_db_and_ix_paths:64]Initialize db_and_ix_paths variable with [{db_path,
                                           "/opt/couchbase/var/lib/couchbase/data"},
                                          {index_path,
                                           "/opt/couchbase/var/lib/couchbase/data"}]
[ns_server:debug,2020-04-02T20:11:12.448+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"dce74b57ae3924cb616de84cba56b09d">>} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{2,63753057672}}]}]
[ns_server:debug,2020-04-02T20:11:12.448+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_dirs} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2020-04-02T20:11:12.448+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"dce74b57ae3924cb616de84cba56b09d">>} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{3,63753057672}}]}]
[ns_server:debug,2020-04-02T20:11:12.448+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_dir} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[error_logger:info,2020-04-02T20:11:12.450+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.250.0>},
                       {name,ns_disksup},
                       {mfargs,{ns_disksup,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.451+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.251.0>},
                       {name,diag_handler_worker},
                       {mfargs,{work_queue,start_link,[diag_handler_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:11:12.452+05:30,ns_1@127.0.0.1:ns_server_sup<0.249.0>:dir_size:start_link:39]Starting quick version of dir_size with program name: godu
[error_logger:info,2020-04-02T20:11:12.452+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.253.0>},
                       {name,dir_size},
                       {mfargs,{dir_size,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.453+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.254.0>},
                       {name,request_throttler},
                       {mfargs,{request_throttler,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.455+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.255.0>},
                       {name,ns_log},
                       {mfargs,{ns_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.455+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.256.0>},
                       {name,ns_crash_log_consumer},
                       {mfargs,{ns_log,start_link_crash_consumer,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.461+05:30,ns_1@127.0.0.1:memcached_passwords<0.257.0>:memcached_cfg:init:62]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-04-02T20:11:12.462+05:30,ns_1@127.0.0.1:memcached_passwords<0.257.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-04-02T20:11:12.489+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:302]Suspended by process <0.257.0>
[ns_server:debug,2020-04-02T20:11:12.489+05:30,ns_1@127.0.0.1:memcached_passwords<0.257.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-04-02T20:11:12.489+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:309]Released by process <0.257.0>
[ns_server:debug,2020-04-02T20:11:12.489+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[error_logger:info,2020-04-02T20:11:12.489+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.257.0>},
                       {name,memcached_passwords},
                       {mfargs,{memcached_passwords,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.490+05:30,ns_1@127.0.0.1:memcached_permissions<0.260.0>:memcached_cfg:init:62]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:warn,2020-04-02T20:11:12.492+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-04-02T20:11:12.493+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:93]Refresh of [isasl] failed. Retry in 1000 ms.
[ns_server:debug,2020-04-02T20:11:12.494+05:30,ns_1@127.0.0.1:memcached_permissions<0.260.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2020-04-02T20:11:12.494+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:302]Suspended by process <0.260.0>
[ns_server:debug,2020-04-02T20:11:12.494+05:30,ns_1@127.0.0.1:memcached_permissions<0.260.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-04-02T20:11:12.494+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:309]Released by process <0.260.0>
[ns_server:debug,2020-04-02T20:11:12.494+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[error_logger:info,2020-04-02T20:11:12.494+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.260.0>},
                       {name,memcached_permissions},
                       {mfargs,{memcached_permissions,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-04-02T20:11:12.495+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-04-02T20:11:12.495+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2020-04-02T20:11:12.495+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.263.0>},
                       {name,ns_email_alert},
                       {mfargs,{ns_email_alert,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.496+05:30,ns_1@127.0.0.1:ns_node_disco<0.266.0>:ns_node_disco:init:128]Initting ns_node_disco with []
[ns_server:debug,2020-04-02T20:11:12.496+05:30,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[error_logger:info,2020-04-02T20:11:12.496+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.265.0>},
                       {id,ns_node_disco_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_node_disco_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.496+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"dce74b57ae3924cb616de84cba56b09d">>} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{4,63753057672}}]}]
[user:info,2020-04-02T20:11:12.496+05:30,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_init:84]Initial otp cookie generated: {sanitized,
                                  <<"ft9dkn2C+310OM1xuvfJf5sksTOKIEIFn5LWJOhgEaU=">>}
[ns_server:debug,2020-04-02T20:11:12.496+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
otp ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
 {cookie,{sanitized,<<"ft9dkn2C+310OM1xuvfJf5sksTOKIEIFn5LWJOhgEaU=">>}}]
[ns_server:debug,2020-04-02T20:11:12.496+05:30,ns_1@127.0.0.1:<0.267.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"ft9dkn2C+310OM1xuvfJf5sksTOKIEIFn5LWJOhgEaU=">>}
[ns_server:debug,2020-04-02T20:11:12.497+05:30,ns_1@127.0.0.1:<0.267.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"ft9dkn2C+310OM1xuvfJf5sksTOKIEIFn5LWJOhgEaU=">>}
[error_logger:info,2020-04-02T20:11:12.497+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.266.0>},
                       {id,ns_node_disco},
                       {mfargs,{ns_node_disco,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.498+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.269.0>},
                       {id,ns_node_disco_log},
                       {mfargs,{ns_node_disco_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.498+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.270.0>},
                       {id,ns_node_disco_conf_events},
                       {mfargs,{ns_node_disco_conf_events,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.503+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:init:71]init pulling
[error_logger:info,2020-04-02T20:11:12.503+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.271.0>},
                       {id,ns_config_rep_merger},
                       {mfargs,{ns_config_rep,start_link_merger,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.503+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:init:73]init pushing
[ns_server:debug,2020-04-02T20:11:12.505+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:init:77]init reannouncing
[ns_server:debug,2020-04-02T20:11:12.505+05:30,ns_1@127.0.0.1:ns_config_events<0.193.0>:ns_node_disco_conf_events:handle_event:50]ns_node_disco_conf_events config on otp
[ns_server:debug,2020-04-02T20:11:12.505+05:30,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2020-04-02T20:11:12.505+05:30,ns_1@127.0.0.1:ns_config_events<0.193.0>:ns_node_disco_conf_events:handle_event:44]ns_node_disco_conf_events config on nodes_wanted
[ns_server:debug,2020-04-02T20:11:12.505+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.225.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from undefined to {undefined,
                                                                             {0,
                                                                              3617707697},
                                                                             {0,
                                                                              3617707697},
                                                                             false,
                                                                             []}
[ns_server:debug,2020-04-02T20:11:12.505+05:30,ns_1@127.0.0.1:<0.278.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"ft9dkn2C+310OM1xuvfJf5sksTOKIEIFn5LWJOhgEaU=">>}
[ns_server:debug,2020-04-02T20:11:12.505+05:30,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2020-04-02T20:11:12.505+05:30,ns_1@127.0.0.1:<0.278.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"ft9dkn2C+310OM1xuvfJf5sksTOKIEIFn5LWJOhgEaU=">>}
[ns_server:debug,2020-04-02T20:11:12.505+05:30,ns_1@127.0.0.1:memcached_passwords<0.257.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-04-02T20:11:12.505+05:30,ns_1@127.0.0.1:<0.279.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"ft9dkn2C+310OM1xuvfJf5sksTOKIEIFn5LWJOhgEaU=">>}
[ns_server:debug,2020-04-02T20:11:12.505+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
otp ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
 {cookie,{sanitized,<<"ft9dkn2C+310OM1xuvfJf5sksTOKIEIFn5LWJOhgEaU=">>}}]
[ns_server:debug,2020-04-02T20:11:12.505+05:30,ns_1@127.0.0.1:<0.279.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"ft9dkn2C+310OM1xuvfJf5sksTOKIEIFn5LWJOhgEaU=">>}
[ns_server:debug,2020-04-02T20:11:12.505+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_dir} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2020-04-02T20:11:12.505+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_dirs} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2020-04-02T20:11:12.506+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',erl_external_listeners} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2020-04-02T20:11:12.506+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',node_encryption} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|false]
[ns_server:debug,2020-04-02T20:11:12.506+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',address_family} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|inet]
[ns_server:debug,2020-04-02T20:11:12.506+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
alert_limits ->
[{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]
[ns_server:debug,2020-04-02T20:11:12.506+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
audit ->
[{auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2020-04-02T20:11:12.506+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]
[ns_server:debug,2020-04-02T20:11:12.506+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_reprovision_cfg ->
[{enabled,true},{max_nodes,1},{count,0}]
[ns_server:debug,2020-04-02T20:11:12.506+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
autocompaction ->
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-04-02T20:11:12.506+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
buckets ->
[[],{configs,[]}]
[ns_server:debug,2020-04-02T20:11:12.506+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
cbas_memory_quota ->
2174
[ns_server:debug,2020-04-02T20:11:12.506+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
drop_request_memory_threshold_mib ->
undefined
[ns_server:debug,2020-04-02T20:11:12.506+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
email_alerts ->
[{recipients,["root@localhost"]},
 {sender,"couchbase@localhost"},
 {enabled,false},
 {email_server,[{user,[]},
                {pass,"*****"},
                {host,"localhost"},
                {port,25},
                {encrypt,false}]},
 {alerts,[auto_failover_node,auto_failover_maximum_reached,
          auto_failover_other_nodes_down,auto_failover_cluster_too_small,
          auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
          ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
          ep_clock_cas_drift_threshold_exceeded,communication_issue]}]
[ns_server:debug,2020-04-02T20:11:12.506+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
fts_memory_quota ->
512
[ns_server:debug,2020-04-02T20:11:12.507+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
index_aware_rebalance_disabled ->
false
[ns_server:debug,2020-04-02T20:11:12.507+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
log_redaction_default_cfg ->
[{redact_level,none}]
[ns_server:debug,2020-04-02T20:11:12.507+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
max_bucket_count ->
30
[ns_server:debug,2020-04-02T20:11:12.507+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
memcached ->
[]
[ns_server:debug,2020-04-02T20:11:12.507+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
memory_quota ->
8886
[ns_server:debug,2020-04-02T20:11:12.507+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
nodes_wanted ->
['ns_1@127.0.0.1']
[ns_server:debug,2020-04-02T20:11:12.507+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
password_policy ->
[{min_length,6},{must_present,[]}]
[ns_server:debug,2020-04-02T20:11:12.507+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
quorum_nodes ->
['ns_1@127.0.0.1']
[ns_server:debug,2020-04-02T20:11:12.507+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
remote_clusters ->
[]
[ns_server:debug,2020-04-02T20:11:12.507+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
replication ->
[{enabled,true}]
[ns_server:debug,2020-04-02T20:11:12.507+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
rest ->
[{port,8091}]
[ns_server:debug,2020-04-02T20:11:12.507+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
rest_creds ->
null
[ns_server:debug,2020-04-02T20:11:12.508+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
secure_headers ->
[]
[ns_server:debug,2020-04-02T20:11:12.508+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
server_groups ->
[[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]
[ns_server:debug,2020-04-02T20:11:12.508+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
set_view_update_daemon ->
[{update_interval,5000},
 {update_min_changes,5000},
 {replica_update_min_changes,5000}]
[ns_server:debug,2020-04-02T20:11:12.508+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_indexers} ->
4
[ns_server:debug,2020-04-02T20:11:12.508+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_replica_indexers} ->
2
[ns_server:debug,2020-04-02T20:11:12.508+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
<<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"in"...>>
[ns_server:debug,2020-04-02T20:11:12.508+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{request_limit,capi} ->
undefined
[ns_server:debug,2020-04-02T20:11:12.508+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{request_limit,rest} ->
undefined
[ns_server:debug,2020-04-02T20:11:12.508+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',audit} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}]
[ns_server:debug,2020-04-02T20:11:12.508+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',capi_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|8092]
[ns_server:debug,2020-04-02T20:11:12.509+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_admin_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9110]
[ns_server:debug,2020-04-02T20:11:12.509+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_client_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9113]
[ns_server:debug,2020-04-02T20:11:12.509+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9112]
[ns_server:debug,2020-04-02T20:11:12.509+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_http_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9111]
[ns_server:debug,2020-04-02T20:11:12.509+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cluster_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9115]
[ns_server:debug,2020-04-02T20:11:12.509+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_console_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9114]
[ns_server:debug,2020-04-02T20:11:12.509+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_data_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9116]
[ns_server:debug,2020-04-02T20:11:12.509+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_debug_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|-1]
[ns_server:debug,2020-04-02T20:11:12.510+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_http_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|8095]
[ns_server:debug,2020-04-02T20:11:12.510+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_messaging_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9118]
[ns_server:debug,2020-04-02T20:11:12.510+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9119]
[ns_server:debug,2020-04-02T20:11:12.510+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9121]
[ns_server:debug,2020-04-02T20:11:12.510+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_parent_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9122]
[ns_server:debug,2020-04-02T20:11:12.510+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_replication_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9120]
[ns_server:debug,2020-04-02T20:11:12.510+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_result_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9117]
[ns_server:debug,2020-04-02T20:11:12.510+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_ssl_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:11:12.510+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',compaction_daemon} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2020-04-02T20:11:12.510+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',config_version} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|{6,5}]
[ns_server:debug,2020-04-02T20:11:12.510+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_debug_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9140]
[ns_server:debug,2020-04-02T20:11:12.510+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_http_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|8096]
[ns_server:debug,2020-04-02T20:11:12.511+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_https_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:11:12.511+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_grpc_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9130]
[ns_server:debug,2020-04-02T20:11:12.511+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:11:12.511+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_http_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|8094]
[ns_server:debug,2020-04-02T20:11:12.511+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_ssl_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:11:12.511+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_admin_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9100]
[ns_server:debug,2020-04-02T20:11:12.511+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_http_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9102]
[ns_server:debug,2020-04-02T20:11:12.511+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_https_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:11:12.511+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_scan_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9101]
[ns_server:debug,2020-04-02T20:11:12.511+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stcatchup_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9104]
[ns_server:debug,2020-04-02T20:11:12.511+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stinit_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9103]
[ns_server:debug,2020-04-02T20:11:12.511+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stmaint_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9105]
[ns_server:debug,2020-04-02T20:11:12.511+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',is_enterprise} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|false]
[ns_server:debug,2020-04-02T20:11:12.511+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',isasl} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2020-04-02T20:11:12.511+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',membership} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 active]
[ns_server:debug,2020-04-02T20:11:12.512+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,undefined},
 {ssl_port,undefined},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_sleeptime,19},
 {log_rotation_period,39003}]
[error_logger:info,2020-04-02T20:11:12.512+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.272.0>},
                       {id,ns_config_rep},
                       {mfargs,{ns_config_rep,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.512+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.264.0>},
                       {name,ns_node_disco_sup},
                       {mfargs,{ns_node_disco_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:11:12.512+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([alert_limits,audit,auto_failover_cfg,
                               auto_reprovision_cfg,autocompaction,buckets,
                               cbas_memory_quota,
                               drop_request_memory_threshold_mib,email_alerts,
                               fts_memory_quota,
                               index_aware_rebalance_disabled,
                               log_redaction_default_cfg,max_bucket_count,
                               memcached,memory_quota,nodes_wanted,otp,
                               password_policy,quorum_nodes,remote_clusters,
                               replication,rest,rest_creds,secure_headers,
                               server_groups,set_view_update_daemon,
                               {couchdb,max_parallel_indexers},
                               {couchdb,max_parallel_replica_indexers},
                               {local_changes_count,
                                   <<"dce74b57ae3924cb616de84cba56b09d">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {request_limit,capi},
                               {request_limit,rest},
                               {node,'ns_1@127.0.0.1',address_family},
                               {node,'ns_1@127.0.0.1',audit},
                               {node,'ns_1@127.0.0.1',capi_port},
                               {node,'ns_1@127.0.0.1',cbas_admin_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_client_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_http_port},
                               {node,'ns_1@127.0.0.1',cbas_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_console_port},
                               {node,'ns_1@127.0.0.1',cbas_data_port},
                               {node,'ns_1@127.0.0.1',cbas_debug_port},
                               {node,'ns_1@127.0.0.1',cbas_dirs},
                               {node,'ns_1@127.0.0.1',cbas_http_port},
                               {node,'ns_1@127.0.0.1',cbas_messaging_port},
                               {node,'ns_1@127.0.0.1',
                                   cbas_metadata_callback_port},
                               {node,'ns_1@127.0.0.1',cbas_metadata_port},
                               {node,'ns_1@127.0.0.1',cbas_parent_port},
                               {node,'ns_1@127.0.0.1',cbas_replication_port},
                               {node,'ns_1@127.0.0.1',cbas_result_port},
                               {node,'ns_1@127.0.0.1',cbas_ssl_port},
                               {node,'ns_1@127.0.0.1',compaction_daemon},
                               {node,'ns_1@127.0.0.1',config_version},
                               {node,'ns_1@127.0.0.1',erl_external_listeners},
                               {node,'ns_1@127.0.0.1',eventing_debug_port},
                               {node,'ns_1@127.0.0.1',eventing_dir},
                               {node,'ns_1@127.0.0.1',eventing_http_port},
                               {node,'ns_1@127.0.0.1',eventing_https_port},
                               {node,'ns_1@127.0.0.1',fts_grpc_port},
                               {node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
                               {node,'ns_1@127.0.0.1',fts_http_port},
                               {node,'ns_1@127.0.0.1',fts_ssl_port},
                               {node,'ns_1@127.0.0.1',indexer_admin_port}]..)
[ns_server:debug,2020-04-02T20:11:12.512+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_config} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 {[{interfaces,
    {memcached_config_mgr,omit_missing_mcd_ports,
     [{[{host,<<"*">>},
        {port,port},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_port},
        {system,true},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,ssl_port},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_ssl_port},
        {system,true},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
   {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
   {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {opentracing,
    {[{enabled,opentracing_enabled},
      {module,{"~s",[opentracing_module]}},
      {config,{"~s",[opentracing_config]}}]}},
   {admin,{"~s",[admin_user]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},
      {cyclesize,log_cyclesize},
      {sleeptime,log_sleeptime}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}}]}]
[ns_server:debug,2020-04-02T20:11:12.513+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:11:12.513+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_defaults} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {opentracing_enabled,false},
 {opentracing_module,[]},
 {opentracing_config,[]},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {tracing_enabled,false},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>}]
[ns_server:debug,2020-04-02T20:11:12.513+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',moxi} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
 {port,0}]
[ns_server:debug,2020-04-02T20:11:12.513+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ns_log} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2020-04-02T20:11:12.513+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',port_servers} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}]
[ns_server:debug,2020-04-02T20:11:12.513+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',projector_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9999]
[ns_server:debug,2020-04-02T20:11:12.513+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',projector_ssl_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:11:12.513+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',query_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|8093]
[ns_server:debug,2020-04-02T20:11:12.513+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',rest} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2020-04-02T20:11:12.513+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',saslauthd_enabled} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|true]
[ns_server:debug,2020-04-02T20:11:12.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_capi_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:11:12.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_query_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:11:12.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_rest_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:11:12.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',uuid} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 <<"dce74b57ae3924cb616de84cba56b09d">>]
[ns_server:debug,2020-04-02T20:11:12.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',xdcr_rest_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9998]
[ns_server:debug,2020-04-02T20:11:12.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|false]
[ns_server:debug,2020-04-02T20:11:12.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"dce74b57ae3924cb616de84cba56b09d">>} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{4,63753057672}}]}]
[error_logger:info,2020-04-02T20:11:12.515+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.284.0>},
                       {name,vbucket_map_mirror},
                       {mfargs,{vbucket_map_mirror,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.518+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.286.0>},
                       {name,bucket_info_cache},
                       {mfargs,{bucket_info_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.518+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.289.0>},
                       {name,ns_tick_event},
                       {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.518+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.290.0>},
                       {name,buckets_events},
                       {mfargs,
                           {gen_event,start_link,[{local,buckets_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.518+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.291.0>},
                       {name,ns_stats_event},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_stats_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.520+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.292.0>},
                       {name,samples_loader_tasks},
                       {mfargs,{samples_loader_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.524+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.294.0>},
                       {id,ns_heart},
                       {mfargs,{ns_heart,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.524+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.297.0>},
                       {id,ns_heart_slow_updater},
                       {mfargs,{ns_heart,start_link_slow_updater,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.524+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.293.0>},
                       {name,ns_heart_sup},
                       {mfargs,{ns_heart_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:12.525+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.301.0>},
                       {id,ns_doctor_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_doctor_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.527+05:30,ns_1@127.0.0.1:ns_heart<0.294.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]},
                 {ns_heart,handle_info,2,
                           [{file,"src/ns_heart.erl"},{line,118}]}]}}

[ns_server:debug,2020-04-02T20:11:12.527+05:30,ns_1@127.0.0.1:ns_heart<0.294.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]}]}}

[ns_server:debug,2020-04-02T20:11:12.530+05:30,ns_1@127.0.0.1:<0.299.0>:restartable:start_child:98]Started child process <0.300.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2020-04-02T20:11:12.530+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.302.0>},
                       {id,ns_doctor},
                       {mfargs,{ns_doctor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.530+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.299.0>},
                       {name,ns_doctor_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_doctor_sup,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:12.530+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.305.0>},
                       {name,master_activity_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.533+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.307.0>},
                       {name,xdcr_ckpt_store},
                       {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.533+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.308.0>},
                       {name,metakv_worker},
                       {mfargs,{work_queue,start_link,[metakv_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.533+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.309.0>},
                       {name,index_events},
                       {mfargs,{gen_event,start_link,[{local,index_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.533+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.310.0>},
                       {name,index_settings_manager},
                       {mfargs,{index_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.537+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.312.0>},
                       {name,query_settings_manager},
                       {mfargs,{query_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.538+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:302]Suspended by process <0.257.0>
[ns_server:debug,2020-04-02T20:11:12.538+05:30,ns_1@127.0.0.1:memcached_passwords<0.257.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-04-02T20:11:12.538+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:309]Released by process <0.257.0>
[ns_server:debug,2020-04-02T20:11:12.538+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[error_logger:info,2020-04-02T20:11:12.538+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.314.0>},
                       {name,eventing_settings_manager},
                       {mfargs,{eventing_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.538+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.316.0>},
                       {name,audit_events},
                       {mfargs,{gen_event,start_link,[{local,audit_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:warn,2020-04-02T20:11:12.539+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-04-02T20:11:12.539+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2020-04-02T20:11:12.543+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.319.0>},
                       {id,menelaus_ui_auth},
                       {mfargs,{menelaus_ui_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.543+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.321.0>},
                       {id,scram_sha},
                       {mfargs,{scram_sha,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.545+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.322.0>},
                       {id,menelaus_local_auth},
                       {mfargs,{menelaus_local_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.547+05:30,ns_1@127.0.0.1:ns_heart<0.294.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2020-04-02T20:11:12.548+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.323.0>},
                       {id,menelaus_web_cache},
                       {mfargs,{menelaus_web_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.549+05:30,ns_1@127.0.0.1:ns_heart<0.294.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2020-04-02T20:11:12.550+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.329.0>},
                       {id,menelaus_stats_gatherer},
                       {mfargs,{menelaus_stats_gatherer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.550+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.330.0>},
                       {id,json_rpc_events},
                       {mfargs,
                           {gen_event,start_link,[{local,json_rpc_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.554+05:30,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.297.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]},
                 {proc_lib,init_p_do_apply,3,
                           [{file,"proc_lib.erl"},{line,247}]}]}}

[ns_server:debug,2020-04-02T20:11:12.555+05:30,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.297.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]}]}}

[ns_server:debug,2020-04-02T20:11:12.555+05:30,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.297.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-04-02T20:11:12.555+05:30,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.297.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[ns_server:info,2020-04-02T20:11:12.556+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.235.0>:ns_port_server:log:224]ns_couchdb<0.235.0>: Apache CouchDB  (LogLevel=info) is starting.
ns_couchdb<0.235.0>: Apache CouchDB has started. Time to relax.
ns_couchdb<0.235.0>: 29906: Booted. Waiting for shutdown request
ns_couchdb<0.235.0>: working as port

[ns_server:info,2020-04-02T20:11:12.558+05:30,ns_1@127.0.0.1:<0.334.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-04-02T20:11:12.558+05:30,ns_1@127.0.0.1:<0.334.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-04-02T20:11:12.558+05:30,ns_1@127.0.0.1:<0.334.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-04-02T20:11:12.558+05:30,ns_1@127.0.0.1:<0.334.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2020-04-02T20:11:12.566+05:30,ns_1@127.0.0.1:<0.331.0>:restartable:start_child:98]Started child process <0.334.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2020-04-02T20:11:12.566+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.334.0>,menelaus_web}
             started: [{pid,<0.354.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                           {menelaus_web,http_server,
                               [[{ip,"0.0.0.0"},{name,menelaus_web_ipv4}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.566+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.331.0>},
                       {id,menelaus_web},
                       {mfargs,
                           {restartable,start_link,
                               [{menelaus_web,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:12.567+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.371.0>},
                       {id,menelaus_event},
                       {mfargs,{menelaus_event,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.567+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.372.0>},
                       {id,hot_keys_keeper},
                       {mfargs,{hot_keys_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.569+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.373.0>},
                       {id,menelaus_web_alerts_srv},
                       {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.570+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.374.0>},
                       {id,menelaus_cbauth},
                       {mfargs,{menelaus_cbauth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[user:info,2020-04-02T20:11:12.570+05:30,ns_1@127.0.0.1:ns_server_sup<0.249.0>:menelaus_sup:start_link:48]Couchbase Server has started on web port 8091 on node 'ns_1@127.0.0.1'. Version: "6.5.0-4966-community".
[error_logger:info,2020-04-02T20:11:12.570+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.317.0>},
                       {name,menelaus},
                       {mfargs,{menelaus_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:12.570+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.380.0>},
                       {name,ns_ports_setup},
                       {mfargs,{ns_ports_setup,start,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.571+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.384.0>},
                       {id,service_agent_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_agent_children_sup},
                                service_agent_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:12.571+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.385.0>},
                       {id,service_agent_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<service_agent_sup.0.107373856>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.572+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.383.0>},
                       {name,service_agent_sup},
                       {mfargs,{service_agent_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:11:12.573+05:30,ns_1@127.0.0.1:ns_ports_setup<0.380.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,goxdcr]
[error_logger:info,2020-04-02T20:11:12.575+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.387.0>},
                       {name,ns_memcached_sockets_pool},
                       {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.585+05:30,ns_1@127.0.0.1:memcached_auth_server<0.388.0>:memcached_auth_server:reconnect:233]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2020-04-02T20:11:12.585+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.388.0>},
                       {name,memcached_auth_server},
                       {mfargs,{memcached_auth_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.585+05:30,ns_1@127.0.0.1:ns_audit_cfg<0.391.0>:ns_audit_cfg:write_audit_json:259]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      1},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {disabled,
                                                                                      []},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[error_logger:info,2020-04-02T20:11:12.588+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.391.0>},
                       {name,ns_audit_cfg},
                       {mfargs,{ns_audit_cfg,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.588+05:30,ns_1@127.0.0.1:ns_audit_cfg<0.391.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[ns_server:warn,2020-04-02T20:11:12.590+05:30,ns_1@127.0.0.1:<0.394.0>:ns_memcached:connect:1104]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[error_logger:info,2020-04-02T20:11:12.592+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.395.0>},
                       {name,ns_audit},
                       {mfargs,{ns_audit,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.592+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:init:49]waiting for completion of initial ns_ports_setup round
[error_logger:info,2020-04-02T20:11:12.592+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.396.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:11:12.593+05:30,ns_1@127.0.0.1:<0.397.0>:ns_memcached_log_rotator:init:42]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2020-04-02T20:11:12.593+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.397.0>},
                       {name,ns_memcached_log_rotator},
                       {mfargs,{ns_memcached_log_rotator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.593+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.398.0>},
                       {name,testconditions_store},
                       {mfargs,{simple_store,start_link,[testconditions]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.594+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.399.0>},
                       {name,terse_cluster_info_uploader},
                       {mfargs,{terse_cluster_info_uploader,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.597+05:30,ns_1@127.0.0.1:terse_cluster_info_uploader<0.399.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":4,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"kv\":11210,\"capi\":8092,\"projector\":9999},\"thisNode\":true}]}">>
[ns_server:warn,2020-04-02T20:11:12.597+05:30,ns_1@127.0.0.1:<0.403.0>:ns_memcached:connect:1104]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[error_logger:info,2020-04-02T20:11:12.598+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.404.0>},
                       {id,ns_bucket_sup},
                       {mfargs,{ns_bucket_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:12.599+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.405.0>},
                       {id,ns_bucket_worker},
                       {mfargs,{ns_bucket_worker,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.599+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.401.0>},
                       {name,ns_bucket_worker_sup},
                       {mfargs,{ns_bucket_worker_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:12.600+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.407.0>},
                       {name,system_stats_collector},
                       {mfargs,{system_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.601+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.411.0>},
                       {name,{stats_archiver,"@system"}},
                       {mfargs,{stats_archiver,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.603+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.413.0>},
                       {name,{stats_reader,"@system"}},
                       {mfargs,{stats_reader,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.603+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.414.0>},
                       {name,{stats_archiver,"@system-processes"}},
                       {mfargs,
                           {stats_archiver,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.603+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.416.0>},
                       {name,{stats_reader,"@system-processes"}},
                       {mfargs,
                           {stats_reader,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.604+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.417.0>},
                       {name,{stats_archiver,"@query"}},
                       {mfargs,{stats_archiver,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.604+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.419.0>},
                       {name,{stats_reader,"@query"}},
                       {mfargs,{stats_reader,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.608+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.420.0>},
                       {name,query_stats_collector},
                       {mfargs,{query_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.608+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.422.0>},
                       {name,{stats_archiver,"@global"}},
                       {mfargs,{stats_archiver,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.608+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.424.0>},
                       {name,{stats_reader,"@global"}},
                       {mfargs,{stats_reader,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.610+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.425.0>},
                       {name,global_stats_collector},
                       {mfargs,{global_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.611+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.427.0>},
                       {name,goxdcr_status_keeper},
                       {mfargs,{goxdcr_status_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.611+05:30,ns_1@127.0.0.1:goxdcr_status_keeper<0.427.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-04-02T20:11:12.612+05:30,ns_1@127.0.0.1:goxdcr_status_keeper<0.427.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2020-04-02T20:11:12.612+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.431.0>},
                       {id,service_stats_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_stats_children_sup},
                                services_stats_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:12.613+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.433.0>},
                       {id,service_status_keeper_worker},
                       {mfargs,
                           {work_queue,start_link,
                               [service_status_keeper_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.615+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.434.0>},
                       {id,service_status_keeper_index},
                       {mfargs,{service_index,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.616+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.437.0>},
                       {id,service_status_keeper_fts},
                       {mfargs,{service_fts,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.619+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.440.0>},
                       {id,service_status_keeper_eventing},
                       {mfargs,{service_eventing,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.620+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.432.0>},
                       {id,service_status_keeper_sup},
                       {mfargs,{service_status_keeper_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:12.620+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.443.0>},
                       {id,service_stats_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<services_stats_sup.0.108537742>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.620+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.430.0>},
                       {name,services_stats_sup},
                       {mfargs,{services_stats_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:11:12.620+05:30,ns_1@127.0.0.1:ns_ports_setup<0.380.0>:ns_ports_setup:set_children:85]Monitor ns_child_ports_sup <12938.107.0>
[ns_server:debug,2020-04-02T20:11:12.620+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:init:51]ns_ports_setup seems to be ready
[ns_server:debug,2020-04-02T20:11:12.625+05:30,ns_1@127.0.0.1:<0.449.0>:new_concurrency_throttle:init:115]init concurrent throttle process, pid: <0.449.0>, type: kv_throttle# of available token: 1
[ns_server:debug,2020-04-02T20:11:12.627+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:11:12.627+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:11:12.627+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:11:12.627+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[error_logger:info,2020-04-02T20:11:12.627+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.446.0>},
                       {name,compaction_daemon},
                       {mfargs,{compaction_daemon,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.627+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:11:12.628+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 3600s
[ns_server:debug,2020-04-02T20:11:12.629+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:find_port_pid_loop:137]Found memcached port <12938.114.0>
[error_logger:info,2020-04-02T20:11:12.629+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,cluster_logs_sup}
             started: [{pid,<0.452.0>},
                       {id,ets_holder},
                       {mfargs,
                           {cluster_logs_collection_task,
                               start_link_ets_holder,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.629+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.451.0>},
                       {name,cluster_logs_sup},
                       {mfargs,{cluster_logs_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:12.630+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.453.0>},
                       {name,leader_events},
                       {mfargs,{gen_event,start_link,[{local,leader_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.636+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:init:82]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[error_logger:info,2020-04-02T20:11:12.636+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.458.0>},
                       {id,leader_activities},
                       {mfargs,{leader_activities,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.637+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:init:86]activated memcached port server
[error_logger:info,2020-04-02T20:11:12.638+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.459.0>},
                       {id,leader_lease_agent},
                       {mfargs,{leader_lease_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.639+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.457.0>},
                       {id,leader_leases_sup},
                       {mfargs,{leader_leases_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:12.647+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.461.0>},
                       {id,leader_registry_server},
                       {mfargs,{leader_registry_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.649+05:30,ns_1@127.0.0.1:leader_registry_sup<0.460.0>:mb_master:check_master_takeover_needed:283]Sending master node question to the following nodes: []
[ns_server:debug,2020-04-02T20:11:12.649+05:30,ns_1@127.0.0.1:leader_registry_sup<0.460.0>:mb_master:check_master_takeover_needed:285]Got replies: []
[ns_server:debug,2020-04-02T20:11:12.649+05:30,ns_1@127.0.0.1:leader_registry_sup<0.460.0>:mb_master:check_master_takeover_needed:291]Was unable to discover master, not going to force mastership takeover
[user:info,2020-04-02T20:11:12.651+05:30,ns_1@127.0.0.1:mb_master<0.464.0>:mb_master:init:103]I'm the only node, so I'm the master.
[ns_server:debug,2020-04-02T20:11:12.651+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_new_leader:241]New leader is 'ns_1@127.0.0.1'. Invalidating name cache.
[ns_server:debug,2020-04-02T20:11:12.655+05:30,ns_1@127.0.0.1:mb_master<0.464.0>:master_activity_events:submit_cast:82]Failed to send master activity event: {error,badarg}
[ns_server:debug,2020-04-02T20:11:12.656+05:30,ns_1@127.0.0.1:leader_lease_acquirer<0.467.0>:leader_utils:wait_cluster_is_55:54]Delaying start since cluster is not fully upgraded to 5.5 yet.
[error_logger:info,2020-04-02T20:11:12.656+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.467.0>},
                       {id,leader_lease_acquirer},
                       {mfargs,{leader_lease_acquirer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.657+05:30,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.469.0>:leader_utils:wait_cluster_is_55:54]Delaying start since cluster is not fully upgraded to 5.5 yet.
[error_logger:info,2020-04-02T20:11:12.657+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.469.0>},
                       {id,leader_quorum_nodes_manager},
                       {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:11:12.659+05:30,ns_1@127.0.0.1:mb_master_sup<0.466.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.471.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-04-02T20:11:12.659+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.471.0>},
                       {id,ns_tick},
                       {mfargs,{ns_tick,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.660+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.473.0>},
                       {id,compat_mode_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,compat_mode_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.661+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[5,0]}]

[ns_server:info,2020-04-02T20:11:12.661+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [5,1]
[ns_server:debug,2020-04-02T20:11:12.661+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[5,1]},
 {set,client_cert_auth,[{state,"disable"},{prefixes,[]}]},
 {set,buckets,[{configs,[]}]}]

[ns_server:info,2020-04-02T20:11:12.662+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [5,5]
[ns_server:debug,2020-04-02T20:11:12.663+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[5,5]},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]}]},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120}">>},
 {set,{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
 {set,buckets,[{configs,[]}]},
 {delete,{rbac_upgrade,[5,5]}},
 {set,audit,
      [{enabled,[]},
       {disabled_users,[]},
       {auditd_enabled,false},
       {rotate_interval,86400},
       {rotate_size,20971520},
       {disabled,[]},
       {sync,[]},
       {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {set,quorum_nodes,['ns_1@127.0.0.1']},
 {set,scramsha_fallback_salt,<<112,253,85,12,194,135,218,207,1,194,207,171>>}]

[ns_server:info,2020-04-02T20:11:12.663+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [6,0]
[ns_server:debug,2020-04-02T20:11:12.663+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[6,0]}]

[ns_server:info,2020-04-02T20:11:12.665+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [6,5]
[ns_server:debug,2020-04-02T20:11:12.668+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[6,5]},
 {set,audit_decriptors,
      [{8243,
        [{name,<<"mutate document">>},
         {description,<<"Document was mutated via the REST API">>},
         {enabled,true},
         {module,ns_server}]},
       {8255,
        [{name,<<"read document">>},
         {description,<<"Document was read via the REST API">>},
         {enabled,false},
         {module,ns_server}]},
       {8257,
        [{name,<<"alert email sent">>},
         {description,<<"An alert email was successfully sent">>},
         {enabled,true},
         {module,ns_server}]},
       {20480,
        [{name,<<"opened DCP connection">>},
         {description,<<"opened DCP connection">>},
         {enabled,true},
         {module,memcached}]},
       {20482,
        [{name,<<"external memcached bucket flush">>},
         {description,<<"External user flushed the content of a memcached bucket">>},
         {enabled,true},
         {module,memcached}]},
       {20483,
        [{name,<<"invalid packet">>},
         {description,<<"Rejected an invalid packet">>},
         {enabled,true},
         {module,memcached}]},
       {20485,
        [{name,<<"authentication succeeded">>},
         {description,<<"Authentication to the cluster succeeded">>},
         {enabled,false},
         {module,memcached}]},
       {20488,
        [{name,<<"document read">>},
         {description,<<"Document was read">>},
         {enabled,false},
         {module,memcached}]},
       {20489,
        [{name,<<"document locked">>},
         {description,<<"Document was locked">>},
         {enabled,false},
         {module,memcached}]},
       {20490,
        [{name,<<"document modify">>},
         {description,<<"Document was modified">>},
         {enabled,false},
         {module,memcached}]},
       {20491,
        [{name,<<"document delete">>},
         {description,<<"Document was deleted">>},
         {enabled,false},
         {module,memcached}]},
       {20492,
        [{name,<<"select bucket">>},
         {description,<<"The specified bucket was selected">>},
         {enabled,true},
         {module,memcached}]},
       {28672,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28673,
        [{name,<<"EXPLAIN statement">>},
         {description,<<"A N1QL EXPLAIN statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28674,
        [{name,<<"PREPARE statement">>},
         {description,<<"A N1QL PREPARE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28675,
        [{name,<<"INFER statement">>},
         {description,<<"A N1QL INFER statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28676,
        [{name,<<"INSERT statement">>},
         {description,<<"A N1QL INSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28677,
        [{name,<<"UPSERT statement">>},
         {description,<<"A N1QL UPSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28678,
        [{name,<<"DELETE statement">>},
         {description,<<"A N1QL DELETE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28679,
        [{name,<<"UPDATE statement">>},
         {description,<<"A N1QL UPDATE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28680,
        [{name,<<"MERGE statement">>},
         {description,<<"A N1QL MERGE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28681,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28682,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28683,
        [{name,<<"ALTER INDEX statement">>},
         {description,<<"A N1QL ALTER INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28684,
        [{name,<<"BUILD INDEX statement">>},
         {description,<<"A N1QL BUILD INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28685,
        [{name,<<"GRANT ROLE statement">>},
         {description,<<"A N1QL GRANT ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28686,
        [{name,<<"REVOKE ROLE statement">>},
         {description,<<"A N1QL REVOKE ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28687,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An unrecognized statement was received by the N1QL query engine">>},
         {enabled,false},
         {module,n1ql}]},
       {28688,
        [{name,<<"CREATE PRIMARY INDEX statement">>},
         {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28689,
        [{name,<<"/admin/stats API request">>},
         {description,<<"An HTTP request was made to the API at /admin/stats.">>},
         {enabled,false},
         {module,n1ql}]},
       {28690,
        [{name,<<"/admin/vitals API request">>},
         {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
         {enabled,false},
         {module,n1ql}]},
       {28691,
        [{name,<<"/admin/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28692,
        [{name,<<"/admin/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28693,
        [{name,<<"/admin/indexes/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28694,
        [{name,<<"/admin/indexes/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28695,
        [{name,<<"/admin/indexes/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28697,
        [{name,<<"/admin/ping API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ping.">>},
         {enabled,false},
         {module,n1ql}]},
       {28698,
        [{name,<<"/admin/config API request">>},
         {description,<<"An HTTP request was made to the API at /admin/config.">>},
         {enabled,false},
         {module,n1ql}]},
       {28699,
        [{name,<<"/admin/ssl_cert API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
         {enabled,false},
         {module,n1ql}]},
       {28700,
        [{name,<<"/admin/settings API request">>},
         {description,<<"An HTTP request was made to the API at /admin/settings.">>},
         {enabled,false},
         {module,n1ql}]},
       {28701,
        [{name,<<"/admin/clusters API request">>},
         {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
         {enabled,false},
         {module,n1ql}]},
       {28702,
        [{name,<<"/admin/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28704,
        [{name,<<"/admin/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28705,
        [{name,<<"/admin/indexes/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {40960,
        [{name,<<"Create Design Doc">>},
         {description,<<"Design Doc is Created">>},
         {enabled,true},
         {module,view_engine}]},
       {40961,
        [{name,<<"Delete Design Doc">>},
         {description,<<"Design Doc is Deleted">>},
         {enabled,true},
         {module,view_engine}]},
       {40962,
        [{name,<<"Query DDoc Meta Data">>},
         {description,<<"Design Doc Meta Data Query Request">>},
         {enabled,true},
         {module,view_engine}]},
       {40963,
        [{name,<<"View Query">>},
         {description,<<"View Query Request">>},
         {enabled,false},
         {module,view_engine}]},
       {40964,
        [{name,<<"Update Design Doc">>},
         {description,<<"Design Doc is Updated">>},
         {enabled,true},
         {module,view_engine}]}]},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]},
       {can_abort_rebalance,false}]},
 {set,max_bucket_count,30},
 {set,retry_rebalance,
      [{enabled,false},{after_time_period,300},{max_attempts,1}]},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_size\":5120}">>}]

[ns_server:debug,2020-04-02T20:11:12.670+05:30,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.469.0>:leader_utils:wait_cluster_is_55_loop:78]Cluster upgraded to 5.5. Starting.
[ns_server:debug,2020-04-02T20:11:12.670+05:30,ns_1@127.0.0.1:leader_lease_acquirer<0.467.0>:leader_utils:wait_cluster_is_55_loop:78]Cluster upgraded to 5.5. Starting.
[ns_server:debug,2020-04-02T20:11:12.670+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.225.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from {undefined,
                                                                {0,3617707697},
                                                                {0,3617707697},
                                                                false,[]} to {[6,
                                                                               5],
                                                                              {0,
                                                                               3617707697},
                                                                              {0,
                                                                               3617707697},
                                                                              false,
                                                                              []}
[ns_server:debug,2020-04-02T20:11:12.670+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([audit,audit_decriptors,auto_failover_cfg,
                               buckets,client_cert_auth,
                               cluster_compat_version,max_bucket_count,
                               quorum_nodes,retry_rebalance,
                               scramsha_fallback_salt,
                               {local_changes_count,
                                   <<"dce74b57ae3924cb616de84cba56b09d">>},
                               {metakv,<<"/eventing/settings/config">>},
                               {metakv,<<"/query/settings/config">>}]..)
[ns_server:debug,2020-04-02T20:11:12.670+05:30,ns_1@127.0.0.1:memcached_permissions<0.260.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2020-04-02T20:11:12.670+05:30,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.469.0>:leader_quorum_nodes_manager:pull_config:114]Attempting to pull config from nodes:
[]
[ns_server:debug,2020-04-02T20:11:12.671+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:302]Suspended by process <0.260.0>
[ns_server:debug,2020-04-02T20:11:12.671+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
retry_rebalance ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
 {enabled,false},
 {after_time_period,300},
 {max_attempts,1}]
[ns_server:debug,2020-04-02T20:11:12.671+05:30,ns_1@127.0.0.1:memcached_permissions<0.260.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-04-02T20:11:12.671+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:309]Released by process <0.260.0>
[ns_server:debug,2020-04-02T20:11:12.672+05:30,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.469.0>:leader_quorum_nodes_manager:pull_config:119]Pulled config successfully.
[ns_server:debug,2020-04-02T20:11:12.672+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:handle_call:122]Got full synchronization request from 'ns_1@127.0.0.1'
[ns_server:debug,2020-04-02T20:11:12.672+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:handle_call:128]Fully synchronized config in 15 us
[user:warn,2020-04-02T20:11:12.672+05:30,ns_1@127.0.0.1:compat_mode_manager<0.474.0>:compat_mode_manager:handle_consider_switching_compat_mode:49]Changed cluster compat mode from undefined to [6,5]
[error_logger:info,2020-04-02T20:11:12.672+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.474.0>},
                       {id,compat_mode_manager},
                       {mfargs,{compat_mode_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.673+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
audit_decriptors ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {40960,
  [{name,<<"Create Design Doc">>},
   {description,<<"Design Doc is Created">>},
   {enabled,true},
   {module,view_engine}]},
 {40961,
  [{name,<<"Delete Design Doc">>},
   {description,<<"Design Doc is Deleted">>},
   {enabled,true},
   {module,view_engine}]},
 {40962,
  [{name,<<"Query DDoc Meta Data">>},
   {description,<<"Design Doc Meta Data Query Request">>},
   {enabled,true},
   {module,view_engine}]},
 {40963,
  [{name,<<"View Query">>},
   {description,<<"View Query Request">>},
   {enabled,false},
   {module,view_engine}]},
 {40964,
  [{name,<<"Update Design Doc">>},
   {description,<<"Design Doc is Updated">>},
   {enabled,true},
   {module,view_engine}]}]
[ns_server:debug,2020-04-02T20:11:12.673+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
scramsha_fallback_salt ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]}|
 <<112,253,85,12,194,135,218,207,1,194,207,171>>]
[ns_server:debug,2020-04-02T20:11:12.673+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/eventing/settings/config">>} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]}|
 <<"{\"ram_quota\":256}">>]
[ns_server:debug,2020-04-02T20:11:12.673+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{2,63753057672}}]}|
 <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_si"...>>]
[ns_server:debug,2020-04-02T20:11:12.673+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
client_cert_auth ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
 {state,"disable"},
 {prefixes,[]}]
[ns_server:debug,2020-04-02T20:11:12.673+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
cluster_compat_version ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{5,63753057672}}]},6,5]
[ns_server:debug,2020-04-02T20:11:12.674+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
audit ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
 {enabled,[]},
 {disabled_users,[]},
 {auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2020-04-02T20:11:12.674+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{2,63753057672}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,false}]
[ns_server:debug,2020-04-02T20:11:12.674+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
buckets ->
[[{<<"dce74b57ae3924cb616de84cba56b09d">>,{2,63753057672}}],{configs,[]}]
[ns_server:debug,2020-04-02T20:11:12.674+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
max_bucket_count ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]}|30]
[ns_server:debug,2020-04-02T20:11:12.674+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
quorum_nodes ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-04-02T20:11:12.674+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"dce74b57ae3924cb616de84cba56b09d">>} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{5,63753057672}}]}]
[ns_server:debug,2020-04-02T20:11:12.676+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2020-04-02T20:11:12.684+05:30,ns_1@127.0.0.1:leader_lease_agent<0.459.0>:leader_lease_agent:do_handle_acquire_lease:149]Granting lease to {lease_holder,<<"2f564326b83cee3383ed3cc9f1a843f8">>,
                                'ns_1@127.0.0.1'} for 15000ms
[error_logger:info,2020-04-02T20:11:12.685+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.497.0>},
                       {id,ns_janitor_server},
                       {mfargs,{ns_janitor_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:11:12.686+05:30,ns_1@127.0.0.1:<0.493.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:302]Acquired lease from node 'ns_1@127.0.0.1' (lease uuid: <<"2f564326b83cee3383ed3cc9f1a843f8">>)
[ns_server:info,2020-04-02T20:11:12.688+05:30,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.494.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.498.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-04-02T20:11:12.688+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.498.0>},
                       {id,auto_reprovision},
                       {mfargs,{auto_reprovision,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.691+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:apply_changed_memcached_config:179]New memcached config is hot-reloadable.
[ns_server:debug,2020-04-02T20:11:12.691+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:do_read_current_memcached_config:287]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2020-04-02T20:11:12.692+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:89]Refresh of [rbac,isasl] succeeded
[ns_server:info,2020-04-02T20:11:12.692+05:30,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.494.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.499.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-04-02T20:11:12.692+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.499.0>},
                       {id,auto_rebalance},
                       {mfargs,{auto_rebalance,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:11:12.692+05:30,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.494.0>:misc:start_singleton:857]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.500.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-04-02T20:11:12.692+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.500.0>},
                       {id,ns_orchestrator},
                       {mfargs,{ns_orchestrator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.692+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.494.0>},
                       {id,ns_orchestrator_child_sup},
                       {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:11:12.694+05:30,ns_1@127.0.0.1:<0.502.0>:auto_failover:init:185]init auto_failover.
[user:info,2020-04-02T20:11:12.694+05:30,ns_1@127.0.0.1:<0.502.0>:auto_failover:handle_call:216]Enabled auto-failover with timeout 120 and max count 1
[ns_server:debug,2020-04-02T20:11:12.696+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"dce74b57ae3924cb616de84cba56b09d">>} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{6,63753057672}}]}]
[ns_server:info,2020-04-02T20:11:12.696+05:30,ns_1@127.0.0.1:ns_orchestrator_sup<0.472.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.502.0> on 'ns_1@127.0.0.1'

[ns_server:debug,2020-04-02T20:11:12.696+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([auto_failover_cfg,
                               {local_changes_count,
                                   <<"dce74b57ae3924cb616de84cba56b09d">>}]..)
[ns_server:debug,2020-04-02T20:11:12.696+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{2,63753057672}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,false}]
[ns_server:info,2020-04-02T20:11:12.696+05:30,ns_1@127.0.0.1:mb_master_sup<0.466.0>:misc:start_singleton:857]start_singleton(work_queue, start_link, [{via,leader_registry,collections}]): started as <0.505.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-04-02T20:11:12.696+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.502.0>},
                       {id,auto_failover},
                       {mfargs,{auto_failover,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.697+05:30,ns_1@127.0.0.1:<0.454.0>:restartable:start_child:98]Started child process <0.455.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2020-04-02T20:11:12.696+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.472.0>},
                       {id,ns_orchestrator_sup},
                       {mfargs,{ns_orchestrator_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:12.697+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.505.0>},
                       {id,collections},
                       {mfargs,{collections,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.697+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.464.0>},
                       {id,mb_master},
                       {mfargs,{mb_master,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:12.697+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.460.0>},
                       {id,leader_registry_sup},
                       {mfargs,{leader_registry_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:12.697+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.454.0>},
                       {name,leader_services_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{leader_services_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:12.698+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.512.0>},
                       {name,ns_tick_agent},
                       {mfargs,{ns_tick_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.698+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.514.0>},
                       {name,master_activity_events_ingress},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events_ingress}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.698+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.515.0>},
                       {name,master_activity_events_timestamper},
                       {mfargs,
                           {master_activity_events,start_link_timestamper,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[user:info,2020-04-02T20:11:12.699+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:hot_reload_config:248]Hot-reloaded memcached.json for config change of the following keys: [<<"client_cert_auth">>,
                                                                      <<"datatype_snappy">>,
                                                                      <<"scramsha_fallback_salt">>]
[error_logger:info,2020-04-02T20:11:12.699+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.516.0>},
                       {name,master_activity_events_pids_watcher},
                       {mfargs,
                           {master_activity_events_pids_watcher,start_link,
                               []}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.700+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.517.0>},
                       {name,master_activity_events_keeper},
                       {mfargs,{master_activity_events_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.703+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.520.0>},
                       {id,ns_server_monitor},
                       {mfargs,{ns_server_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.703+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.522.0>},
                       {id,service_monitor_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_monitor_children_sup},
                                health_monitor_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:12.703+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.523.0>},
                       {id,service_monitor_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<health_monitor_sup.0.112499759>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.707+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.529.0>},
                       {id,node_monitor},
                       {mfargs,{node_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.708+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.535.0>},
                       {id,node_status_analyzer},
                       {mfargs,{node_status_analyzer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:11:12.708+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.519.0>},
                       {name,health_monitor_sup},
                       {mfargs,{health_monitor_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:12.709+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.537.0>},
                       {name,rebalance_agent},
                       {mfargs,{rebalance_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.710+05:30,ns_1@127.0.0.1:ns_server_nodes_sup<0.209.0>:one_shot_barrier:notify:27]Notifying on barrier menelaus_barrier
[ns_server:debug,2020-04-02T20:11:12.711+05:30,ns_1@127.0.0.1:menelaus_barrier<0.211.0>:one_shot_barrier:barrier_body:62]Barrier menelaus_barrier got notification from <0.209.0>
[error_logger:info,2020-04-02T20:11:12.710+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.538.0>},
                       {name,ns_rebalance_report_manager},
                       {mfargs,{ns_rebalance_report_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.711+05:30,ns_1@127.0.0.1:ns_server_nodes_sup<0.209.0>:one_shot_barrier:notify:32]Successfuly notified on barrier menelaus_barrier
[error_logger:info,2020-04-02T20:11:12.711+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.249.0>},
                       {name,ns_server_sup},
                       {mfargs,{ns_server_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:11:12.711+05:30,ns_1@127.0.0.1:<0.208.0>:restartable:start_child:98]Started child process <0.209.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2020-04-02T20:11:12.711+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.208.0>},
                       {id,ns_server_nodes_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_server_nodes_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:12.712+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.540.0>},
                       {id,remote_api},
                       {mfargs,{remote_api,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:11:12.712+05:30,ns_1@127.0.0.1:<0.5.0>:child_erlang:child_loop:130]29834: Entered child_loop
[error_logger:info,2020-04-02T20:11:12.712+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.187.0>},
                       {id,ns_server_cluster_sup},
                       {mfargs,{ns_server_cluster_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:11:12.712+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
         application: ns_server
          started_at: 'ns_1@127.0.0.1'

[ns_server:debug,2020-04-02T20:11:12.713+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.225.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@",admin}
[ns_server:debug,2020-04-02T20:11:12.718+05:30,ns_1@127.0.0.1:json_rpc_connection-saslauthd-saslauthd-port<0.541.0>:json_rpc_connection:init:73]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.541.0>
[ns_server:debug,2020-04-02T20:11:12.719+05:30,ns_1@127.0.0.1:json_rpc_connection-goxdcr-cbauth<0.542.0>:json_rpc_connection:init:73]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.542.0>
[ns_server:debug,2020-04-02T20:11:12.719+05:30,ns_1@127.0.0.1:menelaus_cbauth<0.374.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"goxdcr-cbauth",<0.542.0>} started
[ns_server:debug,2020-04-02T20:11:12.720+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.225.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:debug,2020-04-02T20:11:13.601+05:30,ns_1@127.0.0.1:terse_cluster_info_uploader<0.399.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":6,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"kv\":11210,\"capi\":8092,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-04-02T20:11:13.605+05:30,ns_1@127.0.0.1:ns_audit_cfg<0.391.0>:ns_audit_cfg:write_audit_json:259]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "48537283"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2020-04-02T20:11:13.609+05:30,ns_1@127.0.0.1:ns_audit_cfg<0.391.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[ns_server:debug,2020-04-02T20:11:13.695+05:30,ns_1@127.0.0.1:<0.502.0>:auto_failover_logic:log_master_activity:177]Transitioned node {'ns_1@127.0.0.1',<<"dce74b57ae3924cb616de84cba56b09d">>} state new -> up
[ns_server:debug,2020-04-02T20:11:42.628+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:11:42.628+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:11:42.628+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:11:42.628+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:12:12.629+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:12:12.629+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:12:12.629+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:12:12.629+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:12:26.664+05:30,ns_1@127.0.0.1:ldap_auth_cache<0.217.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-04-02T20:12:42.630+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:12:42.630+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:12:42.631+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:12:42.631+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:13:12.631+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:13:12.631+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:13:12.631+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:13:12.631+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:13:41.665+05:30,ns_1@127.0.0.1:ldap_auth_cache<0.217.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-04-02T20:13:42.632+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:13:42.632+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:13:42.632+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:13:42.632+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:14:12.633+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:14:12.633+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:14:12.633+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:14:12.634+05:30,ns_1@127.0.0.1:compaction_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[error_logger:info,2020-04-02T20:14:17.908+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]SIGTERM received - shutting down~n
[error_logger:error,2020-04-02T20:14:17.908+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]** Generic server memsup terminating 
** Last message in was {'EXIT',<0.111.0>,{port_died,normal}}
** When Server state == [{data,[{"Timeout",60000}]},
                         {items,{"Memory Usage",
                                 [{"Allocated",12882472960},
                                  {"Total",16693739520}]}},
                         {items,{"Worst Memory User",
                                 [{"Pid",<0.187.0>},{"Memory",1945816}]}}]
** Reason for termination == 
** {port_died,normal}

[ns_server:debug,2020-04-02T20:14:17.912+05:30,ns_1@127.0.0.1:<0.606.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.370.0>} exited with reason normal
[ns_server:debug,2020-04-02T20:14:17.912+05:30,ns_1@127.0.0.1:<0.247.0>:remote_monitors:monitor_loop:129]Monitored remote process <12939.130.0> went down with: shutdown
[ns_server:debug,2020-04-02T20:14:17.912+05:30,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.236.0>:ns_server_nodes_sup:wait_link_to_couchdb_node_loop:199]Link to couchdb node was lost. Reason: shutdown
[user:info,2020-04-02T20:14:17.913+05:30,ns_1@127.0.0.1:<0.256.0>:ns_log:crash_consumption_loop:69]Service 'saslauthd_port' exited with status 143. Restarting. Messages:

[ns_server:debug,2020-04-02T20:14:17.913+05:30,ns_1@127.0.0.1:json_rpc_connection-saslauthd-saslauthd-port<0.541.0>:json_rpc_connection:handle_info:129]Socket closed
[ns_server:debug,2020-04-02T20:14:17.913+05:30,ns_1@127.0.0.1:<0.569.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.360.0>} exited with reason normal
[ns_server:debug,2020-04-02T20:14:17.913+05:30,ns_1@127.0.0.1:<0.589.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.568.0>} exited with reason normal
[ns_server:debug,2020-04-02T20:14:17.913+05:30,ns_1@127.0.0.1:json_rpc_connection-goxdcr-cbauth<0.542.0>:json_rpc_connection:handle_info:129]Socket closed
[ns_server:debug,2020-04-02T20:14:17.913+05:30,ns_1@127.0.0.1:menelaus_cbauth<0.374.0>:menelaus_cbauth:handle_info:142]Observed json rpc process {"goxdcr-cbauth",<0.542.0>} died with reason shutdown
[error_logger:error,2020-04-02T20:14:17.915+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memsup:init/1
    pid: <0.110.0>
    registered_name: memsup
    exception exit: {port_died,normal}
      in function  gen_server:handle_common_reply/8 (gen_server.erl, line 726)
    ancestors: [os_mon_sup,<0.107.0>]
    message_queue_len: 0
    messages: []
    links: [<0.108.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 92978
  neighbours:

[error_logger:error,2020-04-02T20:14:17.915+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,os_mon_sup}
     Context:    child_terminated
     Reason:     {port_died,normal}
     Offender:   [{pid,<0.110.0>},
                  {id,memsup},
                  {mfargs,{memsup,start_link,[]}},
                  {restart_type,permanent},
                  {shutdown,2000},
                  {child_type,worker}]


[error_logger:info,2020-04-02T20:14:17.915+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,os_mon_sup}
             started: [{pid,<0.4361.0>},
                       {id,memsup},
                       {mfargs,{memsup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:14:17.915+05:30,ns_1@127.0.0.1:<0.548.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.359.0>} exited with reason normal
[ns_server:debug,2020-04-02T20:14:17.916+05:30,ns_1@127.0.0.1:<0.539.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.538.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.916+05:30,ns_1@127.0.0.1:<0.536.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.535.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.916+05:30,ns_1@127.0.0.1:<0.530.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.529.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.916+05:30,ns_1@127.0.0.1:<0.524.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.523.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.916+05:30,ns_1@127.0.0.1:<0.521.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.520.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.916+05:30,ns_1@127.0.0.1:<0.518.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {master_activity_events,<0.517.0>} exited with reason killed
[ns_server:debug,2020-04-02T20:14:17.916+05:30,ns_1@127.0.0.1:<0.513.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {leader_events,<0.512.0>} exited with reason shutdown
[ns_server:info,2020-04-02T20:14:17.916+05:30,ns_1@127.0.0.1:mb_master<0.464.0>:mb_master:terminate:327]Synchronously shutting down child mb_master_sup
[ns_server:info,2020-04-02T20:14:17.916+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_down:253]Process <0.505.0> registered as 'collections' terminated.
[ns_server:info,2020-04-02T20:14:17.916+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_down:253]Process <0.502.0> registered as 'auto_failover' terminated.
[ns_server:debug,2020-04-02T20:14:17.916+05:30,ns_1@127.0.0.1:<0.503.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {compat_mode_events,<0.502.0>} exited with reason shutdown
[ns_server:info,2020-04-02T20:14:17.916+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_down:253]Process <0.500.0> registered as 'ns_orchestrator' terminated.
[ns_server:info,2020-04-02T20:14:17.917+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_down:253]Process <0.499.0> registered as 'auto_rebalance' terminated.
[ns_server:info,2020-04-02T20:14:17.917+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_down:253]Process <0.498.0> registered as 'auto_reprovision' terminated.
[ns_server:info,2020-04-02T20:14:17.917+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_down:253]Process <0.471.0> registered as 'ns_tick' terminated.
[ns_server:debug,2020-04-02T20:14:17.917+05:30,ns_1@127.0.0.1:leader_activities<0.458.0>:leader_activities:handle_internal_process_down:511]Process {quorum_nodes_manager,<0.469.0>} terminated with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.917+05:30,ns_1@127.0.0.1:leader_activities<0.458.0>:leader_activities:handle_internal_process_down:511]Process {acquirer,<0.467.0>} terminated with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.917+05:30,ns_1@127.0.0.1:<0.477.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.467.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.917+05:30,ns_1@127.0.0.1:leader_lease_agent<0.459.0>:leader_lease_agent:handle_abolish_lease:255]Received abolish lease request from {lease_holder,
                                     <<"2f564326b83cee3383ed3cc9f1a843f8">>,
                                     'ns_1@127.0.0.1'} when lease is {lease,
                                                                      {lease_holder,
                                                                       <<"2f564326b83cee3383ed3cc9f1a843f8">>,
                                                                       'ns_1@127.0.0.1'},
                                                                      -576460565990574853,
                                                                      -576460550990574853,
                                                                      {timer,
                                                                       #Ref<0.1274859421.433324035.30626>,
                                                                       {lease_expired,
                                                                        {lease_holder,
                                                                         <<"2f564326b83cee3383ed3cc9f1a843f8">>,
                                                                         'ns_1@127.0.0.1'}}},
                                                                      active}
[ns_server:debug,2020-04-02T20:14:17.917+05:30,ns_1@127.0.0.1:leader_lease_agent<0.459.0>:leader_lease_agent:handle_abolish_lease:260]Expiring abolished lease
[ns_server:debug,2020-04-02T20:14:17.917+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_new_leader:241]New leader is undefined. Invalidating name cache.
[ns_server:debug,2020-04-02T20:14:17.918+05:30,ns_1@127.0.0.1:<0.465.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.464.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.918+05:30,ns_1@127.0.0.1:<0.462.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {leader_events,<0.461.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.918+05:30,ns_1@127.0.0.1:leader_activities<0.458.0>:leader_activities:handle_internal_process_down:511]Process {agent,<0.459.0>} terminated with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.918+05:30,ns_1@127.0.0.1:<0.454.0>:restartable:shutdown_child:120]Successfully terminated process <0.455.0>
[ns_server:debug,2020-04-02T20:14:17.918+05:30,ns_1@127.0.0.1:<0.447.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.446.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.918+05:30,ns_1@127.0.0.1:<0.444.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.443.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.918+05:30,ns_1@127.0.0.1:<0.442.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.440.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.918+05:30,ns_1@127.0.0.1:<0.441.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.440.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.918+05:30,ns_1@127.0.0.1:<0.439.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.437.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.919+05:30,ns_1@127.0.0.1:<0.438.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.437.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.919+05:30,ns_1@127.0.0.1:<0.436.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.434.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.919+05:30,ns_1@127.0.0.1:<0.435.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.434.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.917+05:30,ns_1@127.0.0.1:<0.486.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.469.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.920+05:30,ns_1@127.0.0.1:<0.426.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_tick_event,<0.425.0>} exited with reason shutdown
[user:debug,2020-04-02T20:14:17.929+05:30,ns_1@127.0.0.1:<0.256.0>:ns_log:crash_consumption_loop:69]Service 'memcached' exited with status 0. Restarting. Messages:
EOL on stdin.  Initiating shutdown
[ns_server:debug,2020-04-02T20:14:17.929+05:30,ns_1@127.0.0.1:<0.5.0>:child_erlang:child_loop:134]29834: Got EOL
[ns_server:info,2020-04-02T20:14:17.929+05:30,ns_1@127.0.0.1:<0.5.0>:ns_bootstrap:stop:40]Initiated server shutdown
[ns_server:debug,2020-04-02T20:14:17.929+05:30,ns_1@127.0.0.1:<0.450.0>:remote_monitors:monitor_loop:129]Monitored remote process <12938.114.0> went down with: shutdown
[ns_server:debug,2020-04-02T20:14:17.930+05:30,ns_1@127.0.0.1:<0.445.0>:remote_monitors:monitor_loop:129]Monitored remote process <12938.107.0> went down with: shutdown
[ns_server:debug,2020-04-02T20:14:17.930+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:handle_info:163]Got DOWN with reason: shutdown from memcached port server: <12938.114.0>. Shutting down
[ns_server:debug,2020-04-02T20:14:17.930+05:30,ns_1@127.0.0.1:ns_ports_setup<0.380.0>:ns_ports_setup:children_loop_continue:121]ns_child_ports_sup <12938.107.0> died on babysitter node with shutdown. Restart.
[error_logger:info,2020-04-02T20:14:17.930+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]Initiated server shutdown
[ns_server:debug,2020-04-02T20:14:17.930+05:30,ns_1@127.0.0.1:<0.382.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.380.0>} exited with reason {{child_ports_sup_died,
                                                                                    <12938.107.0>,
                                                                                    shutdown},
                                                                                   [{ns_ports_setup,
                                                                                     children_loop_continue,
                                                                                     3,
                                                                                     [{file,
                                                                                       "src/ns_ports_setup.erl"},
                                                                                      {line,
                                                                                       122}]},
                                                                                    {proc_lib,
                                                                                     wake_up,
                                                                                     3,
                                                                                     [{file,
                                                                                       "proc_lib.erl"},
                                                                                      {line,
                                                                                       257}]}]}
[ns_server:debug,2020-04-02T20:14:17.930+05:30,ns_1@127.0.0.1:<0.381.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.380.0>} exited with reason {{child_ports_sup_died,
                                                                                 <12938.107.0>,
                                                                                 shutdown},
                                                                                [{ns_ports_setup,
                                                                                  children_loop_continue,
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/ns_ports_setup.erl"},
                                                                                   {line,
                                                                                    122}]},
                                                                                 {proc_lib,
                                                                                  wake_up,
                                                                                  3,
                                                                                  [{file,
                                                                                    "proc_lib.erl"},
                                                                                   {line,
                                                                                    257}]}]}
[ns_server:debug,2020-04-02T20:14:17.930+05:30,ns_1@127.0.0.1:<0.456.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.396.0>} exited with reason {shutdown,
                                                                                {memcached_port_server_down,
                                                                                 <12938.114.0>,
                                                                                 shutdown}}
[error_logger:error,2020-04-02T20:14:17.930+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_ports_setup:setup_body_tramp/0
    pid: <0.380.0>
    registered_name: ns_ports_setup
    exception error: {child_ports_sup_died,<12938.107.0>,shutdown}
      in function  ns_ports_setup:children_loop_continue/3 (src/ns_ports_setup.erl, line 122)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.208.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.381.0>,<0.382.0>,<0.249.0>]
    dictionary: [{'ns_ports_setup-indexer-available',
                      "/opt/couchbase/bin/indexer"},
                  {'ns_ports_setup-eventing-producer-available',
                      "/opt/couchbase/bin/eventing-producer"},
                  {'ns_ports_setup-saslauthd-port-available',
                      "/opt/couchbase/bin/saslauthd-port"},
                  {'ns_ports_setup-cbas-available',"/opt/couchbase/bin/cbas"},
                  {'ns_ports_setup-cbft-available',"/opt/couchbase/bin/cbft"},
                  {'ns_ports_setup-cbq-engine-available',
                      "/opt/couchbase/bin/cbq-engine"},
                  {'ns_ports_setup-goxdcr-available',
                      "/opt/couchbase/bin/goxdcr"},
                  {'ns_ports_setup-projector-available',
                      "/opt/couchbase/bin/projector"}]
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 11750
  neighbours:

[error_logger:info,2020-04-02T20:14:17.933+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.242.0>,connection_closed}}
[ns_server:debug,2020-04-02T20:14:17.933+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1274859421.433324033.23845>,
                               inet_tcp_dist,<0.242.0>,
                               #Ref<0.1274859421.433324033.23849>}
[ns_server:debug,2020-04-02T20:14:17.939+05:30,ns_1@127.0.0.1:<0.423.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_stats_event,<0.422.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.939+05:30,ns_1@127.0.0.1:<0.421.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_tick_event,<0.420.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.954+05:30,ns_1@127.0.0.1:<0.418.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_stats_event,<0.417.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.981+05:30,ns_1@127.0.0.1:<0.415.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_stats_event,<0.414.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.999+05:30,ns_1@127.0.0.1:<0.412.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_stats_event,<0.411.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.999+05:30,ns_1@127.0.0.1:<0.409.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ale_stats_events,<0.407.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.999+05:30,ns_1@127.0.0.1:<0.410.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_tick_event,<0.407.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:17.999+05:30,ns_1@127.0.0.1:<0.406.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.405.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.000+05:30,ns_1@127.0.0.1:<0.400.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {bucket_info_cache_invalidations,<0.399.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.000+05:30,ns_1@127.0.0.1:<0.392.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.391.0>} exited with reason shutdown
[error_logger:error,2020-04-02T20:14:18.000+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    shutdown_error
     Reason:     {shutdown,
                     {memcached_port_server_down,<12938.114.0>,shutdown}}
     Offender:   [{pid,<0.396.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2020-04-02T20:14:18.000+05:30,ns_1@127.0.0.1:<0.389.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.388.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.000+05:30,ns_1@127.0.0.1:<0.386.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.385.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.000+05:30,ns_1@127.0.0.1:<0.376.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.374.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.000+05:30,ns_1@127.0.0.1:<0.375.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {json_rpc_events,<0.374.0>} exited with reason shutdown
[error_logger:error,2020-04-02T20:14:18.000+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    shutdown_error
     Reason:     {{child_ports_sup_died,<12938.107.0>,shutdown},
                  [{ns_ports_setup,children_loop_continue,3,
                                   [{file,"src/ns_ports_setup.erl"},
                                    {line,122}]},
                   {proc_lib,wake_up,3,[{file,"proc_lib.erl"},{line,257}]}]}
     Offender:   [{pid,<0.380.0>},
                  {name,ns_ports_setup},
                  {mfargs,{ns_ports_setup,start,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,brutal_kill},
                  {child_type,worker}]


[ns_server:debug,2020-04-02T20:14:18.000+05:30,ns_1@127.0.0.1:<0.378.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.374.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.000+05:30,ns_1@127.0.0.1:<0.379.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ssl_service_events,<0.374.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.000+05:30,ns_1@127.0.0.1:<0.377.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.374.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.000+05:30,ns_1@127.0.0.1:<0.331.0>:restartable:shutdown_child:120]Successfully terminated process <0.334.0>
[ns_server:debug,2020-04-02T20:14:18.000+05:30,ns_1@127.0.0.1:<0.320.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.319.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.001+05:30,ns_1@127.0.0.1:<0.315.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.314.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.001+05:30,ns_1@127.0.0.1:<0.313.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.312.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.001+05:30,ns_1@127.0.0.1:<0.311.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.310.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.001+05:30,ns_1@127.0.0.1:<0.303.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.302.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.001+05:30,ns_1@127.0.0.1:<0.299.0>:restartable:shutdown_child:120]Successfully terminated process <0.300.0>
[ns_server:debug,2020-04-02T20:14:18.001+05:30,ns_1@127.0.0.1:<0.295.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {buckets_events,<0.294.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.001+05:30,ns_1@127.0.0.1:<0.288.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.286.0>} exited with reason killed
[ns_server:debug,2020-04-02T20:14:18.001+05:30,ns_1@127.0.0.1:<0.285.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.284.0>} exited with reason killed
[ns_server:debug,2020-04-02T20:14:18.001+05:30,ns_1@127.0.0.1:<0.273.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events_local,<0.272.0>} exited with reason shutdown
[error_logger:error,2020-04-02T20:14:18.001+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: gen_event:init_it/6
    pid: <0.287.0>
    registered_name: bucket_info_cache_invalidations
    exception exit: killed
      in function  gen_event:terminate_server/4 (gen_event.erl, line 351)
    ancestors: [bucket_info_cache,ns_server_sup,ns_server_nodes_sup,
                  <0.208.0>,ns_server_cluster_sup,root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: []
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 280
  neighbours:

[ns_server:debug,2020-04-02T20:14:18.002+05:30,ns_1@127.0.0.1:<0.262.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.260.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.002+05:30,ns_1@127.0.0.1:<0.261.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.260.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.002+05:30,ns_1@127.0.0.1:<0.259.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.257.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.002+05:30,ns_1@127.0.0.1:<0.258.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.257.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.002+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.235.0>:ns_port_server:terminate:196]Shutting down port ns_couchdb
[ns_server:debug,2020-04-02T20:14:18.002+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.235.0>:ns_port_server:port_shutdown:297]Shutdown command: "shutdown"
[error_logger:error,2020-04-02T20:14:18.002+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    shutdown_error
     Reason:     normal
     Offender:   [{pid,<0.236.0>},
                  {name,wait_for_couchdb_node},
                  {mfargs,{erlang,apply,
                                  [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:info,2020-04-02T20:14:18.132+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.235.0>:ns_port_server:log:224]ns_couchdb<0.235.0>: Error writing to pipe: Broken pipe

[ns_server:debug,2020-04-02T20:14:18.397+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.235.0>:ns_port_server:terminate:199]ns_couchdb has exited
[ns_server:debug,2020-04-02T20:14:18.397+05:30,ns_1@127.0.0.1:<0.229.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.228.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.397+05:30,ns_1@127.0.0.1:<0.230.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.228.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.397+05:30,ns_1@127.0.0.1:<0.226.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.225.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.397+05:30,ns_1@127.0.0.1:<0.227.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.225.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.397+05:30,ns_1@127.0.0.1:<0.218.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.217.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:14:18.397+05:30,ns_1@127.0.0.1:<0.208.0>:restartable:shutdown_child:120]Successfully terminated process <0.209.0>
[ns_server:debug,2020-04-02T20:14:18.397+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:wait_saver:866]Done waiting for saver.
[ns_server:debug,2020-04-02T20:14:18.397+05:30,ns_1@127.0.0.1:<0.203.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.202.0>} exited with reason shutdown
[error_logger:info,2020-04-02T20:14:18.398+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-calculator/544"}}

[error_logger:info,2020-04-02T20:14:18.398+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/8935"}}

[error_logger:info,2020-04-02T20:14:18.398+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-calculator/704"}}

[error_logger:info,2020-04-02T20:14:18.398+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/59"}}

[error_logger:info,2020-04-02T20:14:18.399+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/pycharm-community/188"}}

[error_logger:info,2020-04-02T20:14:18.399+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gnome-system-monitor/127"}}

[error_logger:info,2020-04-02T20:14:18.399+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core18/1705"}}

[error_logger:info,2020-04-02T20:14:18.399+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/8689"}}

[error_logger:info,2020-04-02T20:14:18.399+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gnome-system-monitor/135"}}

[error_logger:info,2020-04-02T20:14:18.399+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-characters/495"}}

[error_logger:info,2020-04-02T20:14:18.399+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-28-1804/116"}}

[error_logger:info,2020-04-02T20:14:18.399+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gtk-common-themes/1474"}}

[error_logger:info,2020-04-02T20:14:18.399+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core18/1668"}}

[error_logger:info,2020-04-02T20:14:18.399+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-logs/81"}}

[error_logger:info,2020-04-02T20:14:18.399+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-logs/93"}}

[error_logger:info,2020-04-02T20:14:18.399+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/98"}}

[error_logger:info,2020-04-02T20:14:18.399+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-characters/399"}}

[error_logger:info,2020-04-02T20:14:18.399+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gtk-common-themes/1440"}}

[ns_server:info,2020-04-02T20:15:14.285+05:30,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-04-02T20:15:14.301+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-04-02T20:15:14.301+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.301+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.301+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.301+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.301+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.301+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.301+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.301+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.301+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.301+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.301+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.301+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.301+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.301+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.302+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.302+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.302+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.302+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.302+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.302+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.302+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.302+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.302+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.302+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.302+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-04-02T20:15:14.302+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-04-02T20:15:14.307+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-04-02T20:15:14.307+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[error_logger:info,2020-04-02T20:15:14.310+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.168.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2020-04-02T20:15:14.310+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.167.0>},
                       {id,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:15:14.315+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@127.0.0.1'
[error_logger:info,2020-04-02T20:15:14.326+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.326+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.327+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:14.329+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.174.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.330+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.176.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:15:14.330+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2020-04-02T20:15:14.331+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.177.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:14.331+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.175.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:14.331+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:14.332+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.332+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.333+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.180.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:14.334+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-04-02T20:15:14.334+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-04-02T20:15:14.334+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[error_logger:info,2020-04-02T20:15:14.336+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.181.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:14.336+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-04-02T20:15:14.336+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.169.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-04-02T20:15:14.339+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-04-02T20:15:14.346+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-04-02T20:15:14.347+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2020-04-02T20:15:14.347+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:15:14.347+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T20:15:14.347+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.489408117.1238892547.188814>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T20:15:14.348+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.489408117.1238892547.188814>,
                                  inet_tcp_dist,<0.185.0>,
                                  #Ref<0.489408117.1238892547.188819>}
[ns_server:debug,2020-04-02T20:15:14.358+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-04-02T20:15:14.358+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2020-04-02T20:15:14.359+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-04-02T20:15:14.361+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-04-02T20:15:14.361+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.368+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:15:14.370+05:30,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,4,2},{20,15,14}}},
               {memory,
                   [{total,26653384},
                    {processes,9830208},
                    {processes_used,9825720},
                    {system,16823176},
                    {atom,388625},
                    {atom_used,364409},
                    {binary,125048},
                    {code,8250921},
                    {ets,1520328}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    error_handler,application,error_logger,application_master,
                    kernel,application_controller,file_server,ets,code_server,
                    file_io_server,heart,proc_lib,supervisor,gen_server,
                    gen_event,erl_eval,lists,filename,code,file,gen,erl_lint,
                    erl_parse,erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{os_mon,"CPO  CXC 138 46","2.4.4"},
                    {sasl,"SASL  CXC 138 11","3.1.2"},
                    {ns_server,"Couchbase server","6.5.0-4966-community"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ale,"Another Logger for Erlang","0.0.0"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,131},
               {node,'ns_1@127.0.0.1'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,auth,httpd_sup,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    release_handler,lhttpc_sup,httpc_sup,lhttpc_manager,
                    alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    'sink-disk_xdcr',timer_server,'sink-disk_debug',
                    inet_gethost_native,ale_sup,kernel_safe_sup,
                    'sink-disk_error',inet_db,'sink-disk_default',
                    ssl_pem_cache_dist,ale_dynamic_sup,rex,global_group,
                    net_sup,kernel_sup,ssl_connection_sup,global_name_server,
                    ssl_admin_sup,tftp_sup,ssl_sup,root_sup,erts_code_purger,
                    os_mon_sup,file_server_2,error_logger,cpu_sup,erl_epmd,
                    init,memsup,erl_signal_server,disksup,ale,net_kernel,
                    dist_manager,ssl_pem_cache,ssl_manager,ssl_dist_admin_sup,
                    ssl_dist_connection_sup,ssl_dist_sup,user,
                    ssl_tls_dist_proxy,ssl_manager_dist,sasl_safe_sup,
                    ssl_listen_tracker_sup,inet_gethost_native_sup,
                    code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,0}]
[ns_server:info,2020-04-02T20:15:14.376+05:30,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"21e0ed4ef2e27d16585f31e4458a1db0546bbb05\" upstream=\"6.5.0\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"5716bf0df2d36db8ff45c6508a328a92b9457cbf\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4966\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"74c19969e8ef1b5309077a03885d00e273378f6c\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"956a2df5f2f2abb48054bc4ce56895ce9618d2ae\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-04-02T20:15:14.378+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.379+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.379+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:15:14.379+05:30,ns_1@127.0.0.1:ns_config_sup<0.192.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-04-02T20:15:14.379+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.380+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.194.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:15:14.405+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-04-02T20:15:14.405+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2020-04-02T20:15:14.416+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[{auto_failover_cfg,
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{2,63753057672}}]},
    {enabled,true},
    {timeout,120},
    {count,0},
    {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
    {failover_server_group,false},
    {max_count,1},
    {failed_over_server_groups,[]},
    {can_abort_rebalance,false}]},
  {retry_rebalance,
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
    {enabled,false},
    {after_time_period,300},
    {max_attempts,1}]},
  {audit_decriptors,
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
    {8243,
     [{name,<<"mutate document">>},
      {description,<<"Document was mutated via the REST API">>},
      {enabled,true},
      {module,ns_server}]},
    {8255,
     [{name,<<"read document">>},
      {description,<<"Document was read via the REST API">>},
      {enabled,false},
      {module,ns_server}]},
    {8257,
     [{name,<<"alert email sent">>},
      {description,<<"An alert email was successfully sent">>},
      {enabled,true},
      {module,ns_server}]},
    {20480,
     [{name,<<"opened DCP connection">>},
      {description,<<"opened DCP connection">>},
      {enabled,true},
      {module,memcached}]},
    {20482,
     [{name,<<"external memcached bucket flush">>},
      {description,
       <<"External user flushed the content of a memcached bucket">>},
      {enabled,true},
      {module,memcached}]},
    {20483,
     [{name,<<"invalid packet">>},
      {description,<<"Rejected an invalid packet">>},
      {enabled,true},
      {module,memcached}]},
    {20485,
     [{name,<<"authentication succeeded">>},
      {description,<<"Authentication to the cluster succeeded">>},
      {enabled,false},
      {module,memcached}]},
    {20488,
     [{name,<<"document read">>},
      {description,<<"Document was read">>},
      {enabled,false},
      {module,memcached}]},
    {20489,
     [{name,<<"document locked">>},
      {description,<<"Document was locked">>},
      {enabled,false},
      {module,memcached}]},
    {20490,
     [{name,<<"document modify">>},
      {description,<<"Document was modified">>},
      {enabled,false},
      {module,memcached}]},
    {20491,
     [{name,<<"document delete">>},
      {description,<<"Document was deleted">>},
      {enabled,false},
      {module,memcached}]},
    {20492,
     [{name,<<"select bucket">>},
      {description,<<"The specified bucket was selected">>},
      {enabled,true},
      {module,memcached}]},
    {28672,
     [{name,<<"SELECT statement">>},
      {description,<<"A N1QL SELECT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28673,
     [{name,<<"EXPLAIN statement">>},
      {description,<<"A N1QL EXPLAIN statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28674,
     [{name,<<"PREPARE statement">>},
      {description,<<"A N1QL PREPARE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28675,
     [{name,<<"INFER statement">>},
      {description,<<"A N1QL INFER statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28676,
     [{name,<<"INSERT statement">>},
      {description,<<"A N1QL INSERT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28677,
     [{name,<<"UPSERT statement">>},
      {description,<<"A N1QL UPSERT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28678,
     [{name,<<"DELETE statement">>},
      {description,<<"A N1QL DELETE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28679,
     [{name,<<"UPDATE statement">>},
      {description,<<"A N1QL UPDATE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28680,
     [{name,<<"MERGE statement">>},
      {description,<<"A N1QL MERGE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28681,
     [{name,<<"CREATE INDEX statement">>},
      {description,<<"A N1QL CREATE INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28682,
     [{name,<<"DROP INDEX statement">>},
      {description,<<"A N1QL DROP INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28683,
     [{name,<<"ALTER INDEX statement">>},
      {description,<<"A N1QL ALTER INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28684,
     [{name,<<"BUILD INDEX statement">>},
      {description,<<"A N1QL BUILD INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28685,
     [{name,<<"GRANT ROLE statement">>},
      {description,<<"A N1QL GRANT ROLE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28686,
     [{name,<<"REVOKE ROLE statement">>},
      {description,<<"A N1QL REVOKE ROLE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28687,
     [{name,<<"UNRECOGNIZED statement">>},
      {description,
       <<"An unrecognized statement was received by the N1QL query engine">>},
      {enabled,false},
      {module,n1ql}]},
    {28688,
     [{name,<<"CREATE PRIMARY INDEX statement">>},
      {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28689,
     [{name,<<"/admin/stats API request">>},
      {description,<<"An HTTP request was made to the API at /admin/stats.">>},
      {enabled,false},
      {module,n1ql}]},
    {28690,
     [{name,<<"/admin/vitals API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/vitals.">>},
      {enabled,false},
      {module,n1ql}]},
    {28691,
     [{name,<<"/admin/prepareds API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/prepareds.">>},
      {enabled,false},
      {module,n1ql}]},
    {28692,
     [{name,<<"/admin/active_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/active_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28693,
     [{name,<<"/admin/indexes/prepareds API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
      {enabled,false},
      {module,n1ql}]},
    {28694,
     [{name,<<"/admin/indexes/active_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28695,
     [{name,<<"/admin/indexes/completed_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28697,
     [{name,<<"/admin/ping API request">>},
      {description,<<"An HTTP request was made to the API at /admin/ping.">>},
      {enabled,false},
      {module,n1ql}]},
    {28698,
     [{name,<<"/admin/config API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/config.">>},
      {enabled,false},
      {module,n1ql}]},
    {28699,
     [{name,<<"/admin/ssl_cert API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/ssl_cert.">>},
      {enabled,false},
      {module,n1ql}]},
    {28700,
     [{name,<<"/admin/settings API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/settings.">>},
      {enabled,false},
      {module,n1ql}]},
    {28701,
     [{name,<<"/admin/clusters API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/clusters.">>},
      {enabled,false},
      {module,n1ql}]},
    {28702,
     [{name,<<"/admin/completed_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/completed_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28704,
     [{name,<<"/admin/functions API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/functions.">>},
      {enabled,false},
      {module,n1ql}]},
    {28705,
     [{name,<<"/admin/indexes/functions API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/functions.">>},
      {enabled,false},
      {module,n1ql}]},
    {40960,
     [{name,<<"Create Design Doc">>},
      {description,<<"Design Doc is Created">>},
      {enabled,true},
      {module,view_engine}]},
    {40961,
     [{name,<<"Delete Design Doc">>},
      {description,<<"Design Doc is Deleted">>},
      {enabled,true},
      {module,view_engine}]},
    {40962,
     [{name,<<"Query DDoc Meta Data">>},
      {description,<<"Design Doc Meta Data Query Request">>},
      {enabled,true},
      {module,view_engine}]},
    {40963,
     [{name,<<"View Query">>},
      {description,<<"View Query Request">>},
      {enabled,false},
      {module,view_engine}]},
    {40964,
     [{name,<<"Update Design Doc">>},
      {description,<<"Design Doc is Updated">>},
      {enabled,true},
      {module,view_engine}]}]},
  {scramsha_fallback_salt,
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]}|
    <<112,253,85,12,194,135,218,207,1,194,207,171>>]},
  {{metakv,<<"/eventing/settings/config">>},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]}|
    <<"{\"ram_quota\":256}">>]},
  {{metakv,<<"/query/settings/config">>},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{2,63753057672}}]}|
    <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_size\":5120}">>]},
  {client_cert_auth,
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
    {state,"disable"},
    {prefixes,[]}]},
  {cluster_compat_version,
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{5,63753057672}}]},
    6,5]},
  {otp,
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
    {cookie,{sanitized,<<"ft9dkn2C+310OM1xuvfJf5sksTOKIEIFn5LWJOhgEaU=">>}}]},
  {{node,'ns_1@127.0.0.1',eventing_dir},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
    47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
    105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
  {{node,'ns_1@127.0.0.1',cbas_dirs},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
    "/opt/couchbase/var/lib/couchbase/data"]},
  {{node,'ns_1@127.0.0.1',erl_external_listeners},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
    {inet,false},
    {inet6,false}]},
  {{node,'ns_1@127.0.0.1',node_encryption},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    false]},
  {{node,'ns_1@127.0.0.1',address_family},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    inet]},
  {alert_limits,
   [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
  {audit,
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
    {enabled,[]},
    {disabled_users,[]},
    {auditd_enabled,false},
    {rotate_interval,86400},
    {rotate_size,20971520},
    {disabled,[]},
    {sync,[]},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
  {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
  {autocompaction,
   [{database_fragmentation_threshold,{30,undefined}},
    {view_fragmentation_threshold,{30,undefined}}]},
  {buckets,
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{2,63753057672}}]},
    {configs,[]}]},
  {cbas_memory_quota,2174},
  {drop_request_memory_threshold_mib,undefined},
  {email_alerts,
   [{recipients,["root@localhost"]},
    {sender,"couchbase@localhost"},
    {enabled,false},
    {email_server,
     [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
    {alerts,
     [auto_failover_node,auto_failover_maximum_reached,
      auto_failover_other_nodes_down,auto_failover_cluster_too_small,
      auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
      ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
      ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
  {fts_memory_quota,512},
  {index_aware_rebalance_disabled,false},
  {log_redaction_default_cfg,[{redact_level,none}]},
  {max_bucket_count,
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]}|
    30]},
  {memcached,[]},
  {memory_quota,8886},
  {nodes_wanted,['ns_1@127.0.0.1']},
  {password_policy,[{min_length,6},{must_present,[]}]},
  {quorum_nodes,
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
    'ns_1@127.0.0.1']},
  {remote_clusters,[]},
  {replication,[{enabled,true}]},
  {rest,[{port,8091}]},
  {rest_creds,null},
  {secure_headers,[]},
  {server_groups,
   [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
  {set_view_update_daemon,
   [{update_interval,5000},
    {update_min_changes,5000},
    {replica_update_min_changes,5000}]},
  {{couchdb,max_parallel_indexers},4},
  {{couchdb,max_parallel_replica_indexers},2},
  {{metakv,<<"/indexing/settings/config">>},
   <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":5,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
  {{request_limit,capi},undefined},
  {{request_limit,rest},undefined},
  {{node,'ns_1@127.0.0.1',audit},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}]},
  {{node,'ns_1@127.0.0.1',capi_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    8092]},
  {{node,'ns_1@127.0.0.1',cbas_admin_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9110]},
  {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9113]},
  {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9112]},
  {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9111]},
  {{node,'ns_1@127.0.0.1',cbas_cluster_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9115]},
  {{node,'ns_1@127.0.0.1',cbas_console_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9114]},
  {{node,'ns_1@127.0.0.1',cbas_data_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9116]},
  {{node,'ns_1@127.0.0.1',cbas_debug_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    -1]},
  {{node,'ns_1@127.0.0.1',cbas_http_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    8095]},
  {{node,'ns_1@127.0.0.1',cbas_messaging_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9118]},
  {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9119]},
  {{node,'ns_1@127.0.0.1',cbas_metadata_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9121]},
  {{node,'ns_1@127.0.0.1',cbas_parent_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9122]},
  {{node,'ns_1@127.0.0.1',cbas_replication_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9120]},
  {{node,'ns_1@127.0.0.1',cbas_result_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9117]},
  {{node,'ns_1@127.0.0.1',cbas_ssl_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    undefined]},
  {{node,'ns_1@127.0.0.1',compaction_daemon},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@127.0.0.1',config_version},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    {6,5}]},
  {{node,'ns_1@127.0.0.1',eventing_debug_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9140]},
  {{node,'ns_1@127.0.0.1',eventing_http_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    8096]},
  {{node,'ns_1@127.0.0.1',eventing_https_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    undefined]},
  {{node,'ns_1@127.0.0.1',fts_grpc_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9130]},
  {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    undefined]},
  {{node,'ns_1@127.0.0.1',fts_http_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    8094]},
  {{node,'ns_1@127.0.0.1',fts_ssl_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    undefined]},
  {{node,'ns_1@127.0.0.1',indexer_admin_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9100]},
  {{node,'ns_1@127.0.0.1',indexer_http_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9102]},
  {{node,'ns_1@127.0.0.1',indexer_https_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    undefined]},
  {{node,'ns_1@127.0.0.1',indexer_scan_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9101]},
  {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9104]},
  {{node,'ns_1@127.0.0.1',indexer_stinit_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9103]},
  {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9105]},
  {{node,'ns_1@127.0.0.1',is_enterprise},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    false]},
  {{node,'ns_1@127.0.0.1',isasl},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@127.0.0.1',membership},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    active]},
  {{node,'ns_1@127.0.0.1',memcached},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
    {port,11210},
    {dedicated_port,11209},
    {dedicated_ssl_port,undefined},
    {ssl_port,undefined},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_sleeptime,19},
    {log_rotation_period,39003}]},
  {{node,'ns_1@127.0.0.1',memcached_config},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    {[{interfaces,
       {memcached_config_mgr,omit_missing_mcd_ports,
        [{[{host,<<"*">>},
           {port,port},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_port},
           {system,true},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,ssl_port},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_ssl_port},
           {system,true},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
      {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
      {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {opentracing,
       {[{enabled,opentracing_enabled},
         {module,{"~s",[opentracing_module]}},
         {config,{"~s",[opentracing_config]}}]}},
      {admin,{"~s",[admin_user]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize},
         {sleeptime,log_sleeptime}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
  {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    undefined]},
  {{node,'ns_1@127.0.0.1',memcached_defaults},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
    {max_connections,65000},
    {system_connections,5000},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {opentracing_enabled,false},
    {opentracing_module,[]},
    {opentracing_config,[]},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {tracing_enabled,false},
    {datatype_snappy,true},
    {num_reader_threads,<<"default">>},
    {num_writer_threads,<<"default">>}]},
  {{node,'ns_1@127.0.0.1',moxi},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
    {port,0}]},
  {{node,'ns_1@127.0.0.1',ns_log},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@127.0.0.1',port_servers},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}]},
  {{node,'ns_1@127.0.0.1',projector_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9999]},
  {{node,'ns_1@127.0.0.1',projector_ssl_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    undefined]},
  {{node,'ns_1@127.0.0.1',query_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    8093]},
  {{node,'ns_1@127.0.0.1',rest},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@127.0.0.1',saslauthd_enabled},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    true]},
  {{node,'ns_1@127.0.0.1',ssl_capi_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    undefined]},
  {{node,'ns_1@127.0.0.1',ssl_query_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    undefined]},
  {{node,'ns_1@127.0.0.1',ssl_rest_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    undefined]},
  {{node,'ns_1@127.0.0.1',uuid},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    <<"dce74b57ae3924cb616de84cba56b09d">>]},
  {{node,'ns_1@127.0.0.1',xdcr_rest_port},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    9998]},
  {{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
    false]},
  {{local_changes_count,<<"dce74b57ae3924cb616de84cba56b09d">>},
   [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{6,63753057672}}]}]}]]
[ns_server:info,2020-04-02T20:15:14.423+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{local_changes_count,<<"dce74b57ae3924cb616de84cba56b09d">>},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{6,63753057672}}]}]},
 {{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',xdcr_rest_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9998]},
 {{node,'ns_1@127.0.0.1',uuid},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   <<"dce74b57ae3924cb616de84cba56b09d">>]},
 {{node,'ns_1@127.0.0.1',ssl_rest_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',ssl_query_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',ssl_capi_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',saslauthd_enabled},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',rest},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@127.0.0.1',query_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   8093]},
 {{node,'ns_1@127.0.0.1',projector_ssl_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',projector_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',port_servers},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}]},
 {{node,'ns_1@127.0.0.1',ns_log},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@127.0.0.1',moxi},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
   {port,0}]},
 {{node,'ns_1@127.0.0.1',memcached_defaults},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,false},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',memcached_config},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@127.0.0.1',memcached},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,undefined},
   {ssl_port,undefined},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@127.0.0.1',membership},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   active]},
 {{node,'ns_1@127.0.0.1',isasl},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@127.0.0.1',is_enterprise},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9105]},
 {{node,'ns_1@127.0.0.1',indexer_stinit_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9103]},
 {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9104]},
 {{node,'ns_1@127.0.0.1',indexer_scan_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9101]},
 {{node,'ns_1@127.0.0.1',indexer_https_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',indexer_http_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9102]},
 {{node,'ns_1@127.0.0.1',indexer_admin_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9100]},
 {{node,'ns_1@127.0.0.1',fts_ssl_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',fts_http_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   8094]},
 {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',fts_grpc_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9130]},
 {{node,'ns_1@127.0.0.1',eventing_https_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',eventing_http_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   8096]},
 {{node,'ns_1@127.0.0.1',eventing_debug_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9140]},
 {{node,'ns_1@127.0.0.1',config_version},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   {6,5}]},
 {{node,'ns_1@127.0.0.1',compaction_daemon},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@127.0.0.1',cbas_ssl_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',cbas_result_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9117]},
 {{node,'ns_1@127.0.0.1',cbas_replication_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9120]},
 {{node,'ns_1@127.0.0.1',cbas_parent_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9122]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9121]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9119]},
 {{node,'ns_1@127.0.0.1',cbas_messaging_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9118]},
 {{node,'ns_1@127.0.0.1',cbas_http_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   8095]},
 {{node,'ns_1@127.0.0.1',cbas_debug_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|-1]},
 {{node,'ns_1@127.0.0.1',cbas_data_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9116]},
 {{node,'ns_1@127.0.0.1',cbas_console_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9114]},
 {{node,'ns_1@127.0.0.1',cbas_cluster_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9115]},
 {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9111]},
 {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9112]},
 {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9113]},
 {{node,'ns_1@127.0.0.1',cbas_admin_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   9110]},
 {{node,'ns_1@127.0.0.1',capi_port},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   8092]},
 {{node,'ns_1@127.0.0.1',audit},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}]},
 {{request_limit,rest},undefined},
 {{request_limit,capi},undefined},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":5,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
 {secure_headers,[]},
 {rest_creds,null},
 {rest,[{port,8091}]},
 {replication,[{enabled,true}]},
 {remote_clusters,[]},
 {quorum_nodes,
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
   'ns_1@127.0.0.1']},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {nodes_wanted,['ns_1@127.0.0.1']},
 {memory_quota,8886},
 {memcached,[]},
 {max_bucket_count,
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]}|30]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {index_aware_rebalance_disabled,false},
 {fts_memory_quota,512},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {drop_request_memory_threshold_mib,undefined},
 {cbas_memory_quota,2174},
 {buckets,
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{2,63753057672}}]},
   {configs,[]}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {audit,
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
   {enabled,[]},
   {disabled_users,[]},
   {auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {{node,'ns_1@127.0.0.1',address_family},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   inet]},
 {{node,'ns_1@127.0.0.1',node_encryption},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',erl_external_listeners},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
   {inet,false},
   {inet6,false}]},
 {{node,'ns_1@127.0.0.1',cbas_dirs},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
   "/opt/couchbase/var/lib/couchbase/data"]},
 {{node,'ns_1@127.0.0.1',eventing_dir},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {otp,
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
   {cookie,{sanitized,<<"ft9dkn2C+310OM1xuvfJf5sksTOKIEIFn5LWJOhgEaU=">>}}]},
 {cluster_compat_version,
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{5,63753057672}}]},
   6,5]},
 {client_cert_auth,
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
   {state,"disable"},
   {prefixes,[]}]},
 {{metakv,<<"/query/settings/config">>},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{2,63753057672}}]}|
   <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_size\":5120}">>]},
 {{metakv,<<"/eventing/settings/config">>},
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]}|
   <<"{\"ram_quota\":256}">>]},
 {scramsha_fallback_salt,
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]}|
   <<112,253,85,12,194,135,218,207,1,194,207,171>>]},
 {audit_decriptors,
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
   {8243,
    [{name,<<"mutate document">>},
     {description,<<"Document was mutated via the REST API">>},
     {enabled,true},
     {module,ns_server}]},
   {8255,
    [{name,<<"read document">>},
     {description,<<"Document was read via the REST API">>},
     {enabled,false},
     {module,ns_server}]},
   {8257,
    [{name,<<"alert email sent">>},
     {description,<<"An alert email was successfully sent">>},
     {enabled,true},
     {module,ns_server}]},
   {20480,
    [{name,<<"opened DCP connection">>},
     {description,<<"opened DCP connection">>},
     {enabled,true},
     {module,memcached}]},
   {20482,
    [{name,<<"external memcached bucket flush">>},
     {description,
      <<"External user flushed the content of a memcached bucket">>},
     {enabled,true},
     {module,memcached}]},
   {20483,
    [{name,<<"invalid packet">>},
     {description,<<"Rejected an invalid packet">>},
     {enabled,true},
     {module,memcached}]},
   {20485,
    [{name,<<"authentication succeeded">>},
     {description,<<"Authentication to the cluster succeeded">>},
     {enabled,false},
     {module,memcached}]},
   {20488,
    [{name,<<"document read">>},
     {description,<<"Document was read">>},
     {enabled,false},
     {module,memcached}]},
   {20489,
    [{name,<<"document locked">>},
     {description,<<"Document was locked">>},
     {enabled,false},
     {module,memcached}]},
   {20490,
    [{name,<<"document modify">>},
     {description,<<"Document was modified">>},
     {enabled,false},
     {module,memcached}]},
   {20491,
    [{name,<<"document delete">>},
     {description,<<"Document was deleted">>},
     {enabled,false},
     {module,memcached}]},
   {20492,
    [{name,<<"select bucket">>},
     {description,<<"The specified bucket was selected">>},
     {enabled,true},
     {module,memcached}]},
   {28672,
    [{name,<<"SELECT statement">>},
     {description,<<"A N1QL SELECT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28673,
    [{name,<<"EXPLAIN statement">>},
     {description,<<"A N1QL EXPLAIN statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28674,
    [{name,<<"PREPARE statement">>},
     {description,<<"A N1QL PREPARE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28675,
    [{name,<<"INFER statement">>},
     {description,<<"A N1QL INFER statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28676,
    [{name,<<"INSERT statement">>},
     {description,<<"A N1QL INSERT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28677,
    [{name,<<"UPSERT statement">>},
     {description,<<"A N1QL UPSERT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28678,
    [{name,<<"DELETE statement">>},
     {description,<<"A N1QL DELETE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28679,
    [{name,<<"UPDATE statement">>},
     {description,<<"A N1QL UPDATE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28680,
    [{name,<<"MERGE statement">>},
     {description,<<"A N1QL MERGE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28681,
    [{name,<<"CREATE INDEX statement">>},
     {description,<<"A N1QL CREATE INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28682,
    [{name,<<"DROP INDEX statement">>},
     {description,<<"A N1QL DROP INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28683,
    [{name,<<"ALTER INDEX statement">>},
     {description,<<"A N1QL ALTER INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28684,
    [{name,<<"BUILD INDEX statement">>},
     {description,<<"A N1QL BUILD INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28685,
    [{name,<<"GRANT ROLE statement">>},
     {description,<<"A N1QL GRANT ROLE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28686,
    [{name,<<"REVOKE ROLE statement">>},
     {description,<<"A N1QL REVOKE ROLE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28687,
    [{name,<<"UNRECOGNIZED statement">>},
     {description,
      <<"An unrecognized statement was received by the N1QL query engine">>},
     {enabled,false},
     {module,n1ql}]},
   {28688,
    [{name,<<"CREATE PRIMARY INDEX statement">>},
     {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28689,
    [{name,<<"/admin/stats API request">>},
     {description,<<"An HTTP request was made to the API at /admin/stats.">>},
     {enabled,false},
     {module,n1ql}]},
   {28690,
    [{name,<<"/admin/vitals API request">>},
     {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
     {enabled,false},
     {module,n1ql}]},
   {28691,
    [{name,<<"/admin/prepareds API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/prepareds.">>},
     {enabled,false},
     {module,n1ql}]},
   {28692,
    [{name,<<"/admin/active_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/active_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28693,
    [{name,<<"/admin/indexes/prepareds API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
     {enabled,false},
     {module,n1ql}]},
   {28694,
    [{name,<<"/admin/indexes/active_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28695,
    [{name,<<"/admin/indexes/completed_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28697,
    [{name,<<"/admin/ping API request">>},
     {description,<<"An HTTP request was made to the API at /admin/ping.">>},
     {enabled,false},
     {module,n1ql}]},
   {28698,
    [{name,<<"/admin/config API request">>},
     {description,<<"An HTTP request was made to the API at /admin/config.">>},
     {enabled,false},
     {module,n1ql}]},
   {28699,
    [{name,<<"/admin/ssl_cert API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/ssl_cert.">>},
     {enabled,false},
     {module,n1ql}]},
   {28700,
    [{name,<<"/admin/settings API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/settings.">>},
     {enabled,false},
     {module,n1ql}]},
   {28701,
    [{name,<<"/admin/clusters API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/clusters.">>},
     {enabled,false},
     {module,n1ql}]},
   {28702,
    [{name,<<"/admin/completed_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/completed_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28704,
    [{name,<<"/admin/functions API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/functions.">>},
     {enabled,false},
     {module,n1ql}]},
   {28705,
    [{name,<<"/admin/indexes/functions API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/functions.">>},
     {enabled,false},
     {module,n1ql}]},
   {40960,
    [{name,<<"Create Design Doc">>},
     {description,<<"Design Doc is Created">>},
     {enabled,true},
     {module,view_engine}]},
   {40961,
    [{name,<<"Delete Design Doc">>},
     {description,<<"Design Doc is Deleted">>},
     {enabled,true},
     {module,view_engine}]},
   {40962,
    [{name,<<"Query DDoc Meta Data">>},
     {description,<<"Design Doc Meta Data Query Request">>},
     {enabled,true},
     {module,view_engine}]},
   {40963,
    [{name,<<"View Query">>},
     {description,<<"View Query Request">>},
     {enabled,false},
     {module,view_engine}]},
   {40964,
    [{name,<<"Update Design Doc">>},
     {description,<<"Design Doc is Updated">>},
     {enabled,true},
     {module,view_engine}]}]},
 {retry_rebalance,
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
   {enabled,false},
   {after_time_period,300},
   {max_attempts,1}]},
 {auto_failover_cfg,
  [{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{2,63753057672}}]},
   {enabled,true},
   {timeout,120},
   {count,0},
   {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
   {failover_server_group,false},
   {max_count,1},
   {failed_over_server_groups,[]},
   {can_abort_rebalance,false}]}]
[error_logger:info,2020-04-02T20:15:14.427+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.195.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.427+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.201.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.428+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.202.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.428+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:15:14.430+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"dce74b57ae3924cb616de84cba56b09d">>} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{7,63753057914}}]}]
[error_logger:info,2020-04-02T20:15:14.430+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.204.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:14.431+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.207.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:14.435+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:14.435+05:30,ns_1@127.0.0.1:menelaus_barrier<0.211.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-04-02T20:15:14.435+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.435+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.437+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.213.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.437+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.215.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.438+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.214.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:14.442+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.217.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.442+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.220.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.445+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.222.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:14.445+05:30,ns_1@127.0.0.1:users_replicator<0.222.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-04-02T20:15:14.446+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.222.0>
[ns_server:debug,2020-04-02T20:15:14.446+05:30,ns_1@127.0.0.1:users_replicator<0.222.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.223.0>
[ns_server:debug,2020-04-02T20:15:14.447+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-04-02T20:15:14.447+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.223.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.448+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.221.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:15:14.452+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.225.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-04-02T20:15:14.452+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.225.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.453+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.228.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:14.453+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.219.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:14.454+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.231.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:14.454+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.232.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:15:14.461+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-04-02T20:15:14.461+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 13ms
[ns_server:debug,2020-04-02T20:15:14.462+05:30,ns_1@127.0.0.1:users_replicator<0.222.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2020-04-02T20:15:14.463+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.235.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:14.463+05:30,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.236.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[error_logger:info,2020-04-02T20:15:14.464+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:15:14.464+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T20:15:14.464+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.489408117.1238892547.188843>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T20:15:14.464+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.489408117.1238892547.188843>,
                                  inet_tcp_dist,<0.239.0>,
                                  #Ref<0.489408117.1238892547.188844>}
[ns_server:debug,2020-04-02T20:15:14.464+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.489408117.1238892547.188843>,
                               inet_tcp_dist,<0.239.0>,
                               #Ref<0.489408117.1238892547.188844>}
[error_logger:info,2020-04-02T20:15:14.464+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.239.0>,shutdown}}
[ns_server:debug,2020-04-02T20:15:14.464+05:30,ns_1@127.0.0.1:<0.237.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-04-02T20:15:14.464+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:15:14.664+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T20:15:14.664+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:15:14.665+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.489408117.1238892545.189968>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T20:15:14.665+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.489408117.1238892545.189968>,
                                  inet_tcp_dist,<0.242.0>,
                                  #Ref<0.489408117.1238892545.189972>}
[ns_server:debug,2020-04-02T20:15:14.695+05:30,ns_1@127.0.0.1:<0.237.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-04-02T20:15:14.897+05:30,ns_1@127.0.0.1:<0.237.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-04-02T20:15:15.131+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.246.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.240+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.236.0>},
                       {name,wait_for_couchdb_node},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.243+05:30,ns_1@127.0.0.1:ns_server_nodes_sup<0.209.0>:ns_storage_conf:setup_db_and_ix_paths:64]Initialize db_and_ix_paths variable with [{db_path,
                                           "/opt/couchbase/var/lib/couchbase/data"},
                                          {index_path,
                                           "/opt/couchbase/var/lib/couchbase/data"}]
[error_logger:info,2020-04-02T20:15:15.245+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.249.0>},
                       {name,ns_disksup},
                       {mfargs,{ns_disksup,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.251+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.250.0>},
                       {name,diag_handler_worker},
                       {mfargs,{work_queue,start_link,[diag_handler_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:15:15.251+05:30,ns_1@127.0.0.1:ns_server_sup<0.248.0>:dir_size:start_link:39]Starting quick version of dir_size with program name: godu
[error_logger:info,2020-04-02T20:15:15.252+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.251.0>},
                       {name,dir_size},
                       {mfargs,{dir_size,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.253+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.252.0>},
                       {name,request_throttler},
                       {mfargs,{request_throttler,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.254+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.253.0>},
                       {name,ns_log},
                       {mfargs,{ns_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.254+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.254.0>},
                       {name,ns_crash_log_consumer},
                       {mfargs,{ns_log,start_link_crash_consumer,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.256+05:30,ns_1@127.0.0.1:memcached_passwords<0.255.0>:memcached_cfg:init:62]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-04-02T20:15:15.257+05:30,ns_1@127.0.0.1:memcached_passwords<0.255.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-04-02T20:15:15.287+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:302]Suspended by process <0.255.0>
[ns_server:debug,2020-04-02T20:15:15.287+05:30,ns_1@127.0.0.1:memcached_passwords<0.255.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-04-02T20:15:15.287+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:309]Released by process <0.255.0>
[ns_server:debug,2020-04-02T20:15:15.287+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[error_logger:info,2020-04-02T20:15:15.287+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.255.0>},
                       {name,memcached_passwords},
                       {mfargs,{memcached_passwords,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.288+05:30,ns_1@127.0.0.1:memcached_permissions<0.258.0>:memcached_cfg:init:62]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2020-04-02T20:15:15.292+05:30,ns_1@127.0.0.1:memcached_permissions<0.258.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:warn,2020-04-02T20:15:15.292+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-04-02T20:15:15.292+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:93]Refresh of [isasl] failed. Retry in 1000 ms.
[ns_server:debug,2020-04-02T20:15:15.292+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:302]Suspended by process <0.258.0>
[ns_server:debug,2020-04-02T20:15:15.292+05:30,ns_1@127.0.0.1:memcached_permissions<0.258.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-04-02T20:15:15.292+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:309]Released by process <0.258.0>
[ns_server:debug,2020-04-02T20:15:15.293+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[error_logger:info,2020-04-02T20:15:15.293+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.258.0>},
                       {name,memcached_permissions},
                       {mfargs,{memcached_permissions,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.293+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.261.0>},
                       {name,ns_email_alert},
                       {mfargs,{ns_email_alert,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-04-02T20:15:15.293+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-04-02T20:15:15.294+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2020-04-02T20:15:15.294+05:30,ns_1@127.0.0.1:ns_node_disco<0.264.0>:ns_node_disco:init:128]Initting ns_node_disco with []
[ns_server:debug,2020-04-02T20:15:15.294+05:30,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[error_logger:info,2020-04-02T20:15:15.294+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.263.0>},
                       {id,ns_node_disco_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_node_disco_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[user:info,2020-04-02T20:15:15.294+05:30,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:128]Node 'ns_1@127.0.0.1' synchronized otp cookie {sanitized,
                                               <<"ft9dkn2C+310OM1xuvfJf5sksTOKIEIFn5LWJOhgEaU=">>} from cluster
[ns_server:debug,2020-04-02T20:15:15.294+05:30,ns_1@127.0.0.1:<0.265.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"ft9dkn2C+310OM1xuvfJf5sksTOKIEIFn5LWJOhgEaU=">>}
[ns_server:debug,2020-04-02T20:15:15.295+05:30,ns_1@127.0.0.1:<0.265.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"ft9dkn2C+310OM1xuvfJf5sksTOKIEIFn5LWJOhgEaU=">>}
[error_logger:info,2020-04-02T20:15:15.295+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.264.0>},
                       {id,ns_node_disco},
                       {mfargs,{ns_node_disco,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.296+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.266.0>},
                       {id,ns_node_disco_log},
                       {mfargs,{ns_node_disco_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.296+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.267.0>},
                       {id,ns_node_disco_conf_events},
                       {mfargs,{ns_node_disco_conf_events,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.298+05:30,ns_1@127.0.0.1:ns_config_rep<0.269.0>:ns_config_rep:init:71]init pulling
[ns_server:debug,2020-04-02T20:15:15.298+05:30,ns_1@127.0.0.1:ns_config_rep<0.269.0>:ns_config_rep:init:73]init pushing
[error_logger:info,2020-04-02T20:15:15.298+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.268.0>},
                       {id,ns_config_rep_merger},
                       {mfargs,{ns_config_rep,start_link_merger,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.300+05:30,ns_1@127.0.0.1:ns_config_rep<0.269.0>:ns_config_rep:init:77]init reannouncing
[ns_server:debug,2020-04-02T20:15:15.300+05:30,ns_1@127.0.0.1:ns_config_events<0.193.0>:ns_node_disco_conf_events:handle_event:44]ns_node_disco_conf_events config on nodes_wanted
[ns_server:debug,2020-04-02T20:15:15.300+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.225.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from undefined to {[6,
                                                                              5],
                                                                             {0,
                                                                              61559131},
                                                                             {0,
                                                                              61559131},
                                                                             false,
                                                                             []}
[ns_server:debug,2020-04-02T20:15:15.300+05:30,ns_1@127.0.0.1:ns_config_events<0.193.0>:ns_node_disco_conf_events:handle_event:50]ns_node_disco_conf_events config on otp
[ns_server:debug,2020-04-02T20:15:15.300+05:30,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2020-04-02T20:15:15.300+05:30,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2020-04-02T20:15:15.300+05:30,ns_1@127.0.0.1:<0.275.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"ft9dkn2C+310OM1xuvfJf5sksTOKIEIFn5LWJOhgEaU=">>}
[ns_server:debug,2020-04-02T20:15:15.300+05:30,ns_1@127.0.0.1:<0.276.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"ft9dkn2C+310OM1xuvfJf5sksTOKIEIFn5LWJOhgEaU=">>}
[ns_server:debug,2020-04-02T20:15:15.300+05:30,ns_1@127.0.0.1:<0.275.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"ft9dkn2C+310OM1xuvfJf5sksTOKIEIFn5LWJOhgEaU=">>}
[ns_server:debug,2020-04-02T20:15:15.300+05:30,ns_1@127.0.0.1:<0.276.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"ft9dkn2C+310OM1xuvfJf5sksTOKIEIFn5LWJOhgEaU=">>}
[ns_server:debug,2020-04-02T20:15:15.301+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
alert_limits ->
[{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]
[ns_server:debug,2020-04-02T20:15:15.301+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
audit ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
 {enabled,[]},
 {disabled_users,[]},
 {auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2020-04-02T20:15:15.302+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
audit_decriptors ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {40960,
  [{name,<<"Create Design Doc">>},
   {description,<<"Design Doc is Created">>},
   {enabled,true},
   {module,view_engine}]},
 {40961,
  [{name,<<"Delete Design Doc">>},
   {description,<<"Design Doc is Deleted">>},
   {enabled,true},
   {module,view_engine}]},
 {40962,
  [{name,<<"Query DDoc Meta Data">>},
   {description,<<"Design Doc Meta Data Query Request">>},
   {enabled,true},
   {module,view_engine}]},
 {40963,
  [{name,<<"View Query">>},
   {description,<<"View Query Request">>},
   {enabled,false},
   {module,view_engine}]},
 {40964,
  [{name,<<"Update Design Doc">>},
   {description,<<"Design Doc is Updated">>},
   {enabled,true},
   {module,view_engine}]}]
[ns_server:debug,2020-04-02T20:15:15.302+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{2,63753057672}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,false}]
[ns_server:debug,2020-04-02T20:15:15.302+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_reprovision_cfg ->
[{enabled,true},{max_nodes,1},{count,0}]
[ns_server:debug,2020-04-02T20:15:15.302+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
autocompaction ->
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-04-02T20:15:15.302+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
buckets ->
[[{<<"dce74b57ae3924cb616de84cba56b09d">>,{2,63753057672}}],{configs,[]}]
[ns_server:debug,2020-04-02T20:15:15.302+05:30,ns_1@127.0.0.1:memcached_passwords<0.255.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
cbas_memory_quota ->
2174
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
client_cert_auth ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
 {state,"disable"},
 {prefixes,[]}]
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
cluster_compat_version ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{5,63753057672}}]},6,5]
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
drop_request_memory_threshold_mib ->
undefined
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
email_alerts ->
[{recipients,["root@localhost"]},
 {sender,"couchbase@localhost"},
 {enabled,false},
 {email_server,[{user,[]},
                {pass,"*****"},
                {host,"localhost"},
                {port,25},
                {encrypt,false}]},
 {alerts,[auto_failover_node,auto_failover_maximum_reached,
          auto_failover_other_nodes_down,auto_failover_cluster_too_small,
          auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
          ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
          ep_clock_cas_drift_threshold_exceeded,communication_issue]}]
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
fts_memory_quota ->
512
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
index_aware_rebalance_disabled ->
false
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
log_redaction_default_cfg ->
[{redact_level,none}]
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
max_bucket_count ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]}|30]
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
memcached ->
[]
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
memory_quota ->
8886
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
nodes_wanted ->
['ns_1@127.0.0.1']
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
otp ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
 {cookie,{sanitized,<<"ft9dkn2C+310OM1xuvfJf5sksTOKIEIFn5LWJOhgEaU=">>}}]
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
password_policy ->
[{min_length,6},{must_present,[]}]
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
quorum_nodes ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
remote_clusters ->
[]
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
replication ->
[{enabled,true}]
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
rest ->
[{port,8091}]
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
rest_creds ->
null
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
retry_rebalance ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
 {enabled,false},
 {after_time_period,300},
 {max_attempts,1}]
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
scramsha_fallback_salt ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]}|
 <<112,253,85,12,194,135,218,207,1,194,207,171>>]
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
secure_headers ->
[]
[ns_server:debug,2020-04-02T20:15:15.303+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
server_groups ->
[[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]
[ns_server:debug,2020-04-02T20:15:15.304+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
set_view_update_daemon ->
[{update_interval,5000},
 {update_min_changes,5000},
 {replica_update_min_changes,5000}]
[ns_server:debug,2020-04-02T20:15:15.304+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_indexers} ->
4
[ns_server:debug,2020-04-02T20:15:15.304+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_replica_indexers} ->
2
[ns_server:debug,2020-04-02T20:15:15.304+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"dce74b57ae3924cb616de84cba56b09d">>} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{7,63753057914}}]}]
[ns_server:debug,2020-04-02T20:15:15.304+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/eventing/settings/config">>} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]}|
 <<"{\"ram_quota\":256}">>]
[ns_server:debug,2020-04-02T20:15:15.304+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
<<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"in"...>>
[ns_server:debug,2020-04-02T20:15:15.304+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{2,63753057672}}]}|
 <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_si"...>>]
[ns_server:debug,2020-04-02T20:15:15.304+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{request_limit,capi} ->
undefined
[ns_server:debug,2020-04-02T20:15:15.304+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{request_limit,rest} ->
undefined
[ns_server:debug,2020-04-02T20:15:15.304+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',address_family} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|inet]
[ns_server:debug,2020-04-02T20:15:15.304+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',audit} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}]
[ns_server:debug,2020-04-02T20:15:15.304+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',capi_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|8092]
[ns_server:debug,2020-04-02T20:15:15.304+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_admin_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9110]
[ns_server:debug,2020-04-02T20:15:15.304+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_client_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9113]
[ns_server:debug,2020-04-02T20:15:15.304+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9112]
[ns_server:debug,2020-04-02T20:15:15.305+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_http_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9111]
[ns_server:debug,2020-04-02T20:15:15.305+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cluster_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9115]
[ns_server:debug,2020-04-02T20:15:15.305+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_console_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9114]
[ns_server:debug,2020-04-02T20:15:15.305+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_data_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9116]
[ns_server:debug,2020-04-02T20:15:15.305+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_debug_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|-1]
[ns_server:debug,2020-04-02T20:15:15.305+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_dirs} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2020-04-02T20:15:15.305+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_http_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|8095]
[ns_server:debug,2020-04-02T20:15:15.305+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_messaging_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9118]
[ns_server:debug,2020-04-02T20:15:15.305+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9119]
[ns_server:debug,2020-04-02T20:15:15.305+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9121]
[ns_server:debug,2020-04-02T20:15:15.305+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_parent_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9122]
[ns_server:debug,2020-04-02T20:15:15.305+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_replication_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9120]
[ns_server:debug,2020-04-02T20:15:15.305+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_result_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9117]
[ns_server:debug,2020-04-02T20:15:15.305+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_ssl_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:15:15.305+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',compaction_daemon} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2020-04-02T20:15:15.306+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',config_version} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|{6,5}]
[ns_server:debug,2020-04-02T20:15:15.306+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',erl_external_listeners} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2020-04-02T20:15:15.306+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_debug_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9140]
[ns_server:debug,2020-04-02T20:15:15.306+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_dir} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057672}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2020-04-02T20:15:15.306+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_http_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|8096]
[ns_server:debug,2020-04-02T20:15:15.306+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_https_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:15:15.306+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_grpc_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9130]
[ns_server:debug,2020-04-02T20:15:15.306+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:15:15.306+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_http_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|8094]
[ns_server:debug,2020-04-02T20:15:15.306+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_ssl_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:15:15.306+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_admin_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9100]
[ns_server:debug,2020-04-02T20:15:15.306+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_http_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9102]
[ns_server:debug,2020-04-02T20:15:15.306+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_https_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:15:15.306+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_scan_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9101]
[ns_server:debug,2020-04-02T20:15:15.307+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stcatchup_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9104]
[ns_server:debug,2020-04-02T20:15:15.307+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stinit_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9103]
[ns_server:debug,2020-04-02T20:15:15.307+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stmaint_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9105]
[ns_server:debug,2020-04-02T20:15:15.307+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',is_enterprise} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|false]
[ns_server:debug,2020-04-02T20:15:15.307+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',isasl} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2020-04-02T20:15:15.307+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',membership} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 active]
[error_logger:info,2020-04-02T20:15:15.307+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.269.0>},
                       {id,ns_config_rep},
                       {mfargs,{ns_config_rep,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.307+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.262.0>},
                       {name,ns_node_disco_sup},
                       {mfargs,{ns_node_disco_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:15:15.308+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,undefined},
 {ssl_port,undefined},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_sleeptime,19},
 {log_rotation_period,39003}]
[ns_server:debug,2020-04-02T20:15:15.307+05:30,ns_1@127.0.0.1:ns_config_rep<0.269.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([alert_limits,audit,audit_decriptors,
                               auto_failover_cfg,auto_reprovision_cfg,
                               autocompaction,buckets,cbas_memory_quota,
                               client_cert_auth,cluster_compat_version,
                               drop_request_memory_threshold_mib,email_alerts,
                               fts_memory_quota,
                               index_aware_rebalance_disabled,
                               log_redaction_default_cfg,max_bucket_count,
                               memcached,memory_quota,nodes_wanted,otp,
                               password_policy,quorum_nodes,remote_clusters,
                               replication,rest,rest_creds,retry_rebalance,
                               scramsha_fallback_salt,secure_headers,
                               server_groups,set_view_update_daemon,
                               {couchdb,max_parallel_indexers},
                               {couchdb,max_parallel_replica_indexers},
                               {local_changes_count,
                                   <<"dce74b57ae3924cb616de84cba56b09d">>},
                               {metakv,<<"/eventing/settings/config">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {metakv,<<"/query/settings/config">>},
                               {request_limit,capi},
                               {request_limit,rest},
                               {node,'ns_1@127.0.0.1',address_family},
                               {node,'ns_1@127.0.0.1',audit},
                               {node,'ns_1@127.0.0.1',capi_port},
                               {node,'ns_1@127.0.0.1',cbas_admin_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_client_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_http_port},
                               {node,'ns_1@127.0.0.1',cbas_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_console_port},
                               {node,'ns_1@127.0.0.1',cbas_data_port},
                               {node,'ns_1@127.0.0.1',cbas_debug_port},
                               {node,'ns_1@127.0.0.1',cbas_dirs},
                               {node,'ns_1@127.0.0.1',cbas_http_port},
                               {node,'ns_1@127.0.0.1',cbas_messaging_port},
                               {node,'ns_1@127.0.0.1',
                                   cbas_metadata_callback_port},
                               {node,'ns_1@127.0.0.1',cbas_metadata_port},
                               {node,'ns_1@127.0.0.1',cbas_parent_port},
                               {node,'ns_1@127.0.0.1',cbas_replication_port},
                               {node,'ns_1@127.0.0.1',cbas_result_port},
                               {node,'ns_1@127.0.0.1',cbas_ssl_port},
                               {node,'ns_1@127.0.0.1',compaction_daemon},
                               {node,'ns_1@127.0.0.1',config_version},
                               {node,'ns_1@127.0.0.1',erl_external_listeners},
                               {node,'ns_1@127.0.0.1',eventing_debug_port},
                               {node,'ns_1@127.0.0.1',eventing_dir}]..)
[ns_server:debug,2020-04-02T20:15:15.308+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_config} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 {[{interfaces,
    {memcached_config_mgr,omit_missing_mcd_ports,
     [{[{host,<<"*">>},
        {port,port},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_port},
        {system,true},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,ssl_port},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_ssl_port},
        {system,true},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
   {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
   {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {opentracing,
    {[{enabled,opentracing_enabled},
      {module,{"~s",[opentracing_module]}},
      {config,{"~s",[opentracing_config]}}]}},
   {admin,{"~s",[admin_user]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},
      {cyclesize,log_cyclesize},
      {sleeptime,log_sleeptime}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}}]}]
[ns_server:debug,2020-04-02T20:15:15.308+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:15:15.309+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_defaults} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {opentracing_enabled,false},
 {opentracing_module,[]},
 {opentracing_config,[]},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {tracing_enabled,false},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>}]
[ns_server:debug,2020-04-02T20:15:15.309+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',moxi} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
 {port,0}]
[ns_server:debug,2020-04-02T20:15:15.309+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',node_encryption} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|false]
[ns_server:debug,2020-04-02T20:15:15.309+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ns_log} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2020-04-02T20:15:15.309+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',port_servers} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}]
[ns_server:debug,2020-04-02T20:15:15.309+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',projector_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9999]
[ns_server:debug,2020-04-02T20:15:15.309+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',projector_ssl_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:15:15.309+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',query_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|8093]
[ns_server:debug,2020-04-02T20:15:15.309+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',rest} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2020-04-02T20:15:15.309+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',saslauthd_enabled} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|true]
[ns_server:debug,2020-04-02T20:15:15.309+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_capi_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:15:15.310+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_query_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:15:15.310+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_rest_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:15:15.310+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',uuid} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|
 <<"dce74b57ae3924cb616de84cba56b09d">>]
[ns_server:debug,2020-04-02T20:15:15.310+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',xdcr_rest_port} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|9998]
[ns_server:debug,2020-04-02T20:15:15.310+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{1,63753057671}}]}|false]
[error_logger:info,2020-04-02T20:15:15.310+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.281.0>},
                       {name,vbucket_map_mirror},
                       {mfargs,{vbucket_map_mirror,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.312+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.283.0>},
                       {name,bucket_info_cache},
                       {mfargs,{bucket_info_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.312+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.286.0>},
                       {name,ns_tick_event},
                       {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.312+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.287.0>},
                       {name,buckets_events},
                       {mfargs,
                           {gen_event,start_link,[{local,buckets_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.312+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.288.0>},
                       {name,ns_stats_event},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_stats_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.314+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.289.0>},
                       {name,samples_loader_tasks},
                       {mfargs,{samples_loader_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.318+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.291.0>},
                       {id,ns_heart},
                       {mfargs,{ns_heart,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.318+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.294.0>},
                       {id,ns_heart_slow_updater},
                       {mfargs,{ns_heart,start_link_slow_updater,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.318+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.290.0>},
                       {name,ns_heart_sup},
                       {mfargs,{ns_heart_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:15.320+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.298.0>},
                       {id,ns_doctor_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_doctor_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.321+05:30,ns_1@127.0.0.1:ns_heart<0.291.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]},
                 {ns_heart,handle_info,2,
                           [{file,"src/ns_heart.erl"},{line,118}]}]}}

[ns_server:debug,2020-04-02T20:15:15.321+05:30,ns_1@127.0.0.1:ns_heart<0.291.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]}]}}

[ns_server:debug,2020-04-02T20:15:15.323+05:30,ns_1@127.0.0.1:<0.296.0>:restartable:start_child:98]Started child process <0.297.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2020-04-02T20:15:15.323+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.299.0>},
                       {id,ns_doctor},
                       {mfargs,{ns_doctor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.324+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.296.0>},
                       {name,ns_doctor_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_doctor_sup,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:15.324+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.302.0>},
                       {name,master_activity_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.327+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.304.0>},
                       {name,xdcr_ckpt_store},
                       {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.327+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.305.0>},
                       {name,metakv_worker},
                       {mfargs,{work_queue,start_link,[metakv_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.327+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.306.0>},
                       {name,index_events},
                       {mfargs,{gen_event,start_link,[{local,index_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.327+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.307.0>},
                       {name,index_settings_manager},
                       {mfargs,{index_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.330+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.309.0>},
                       {name,query_settings_manager},
                       {mfargs,{query_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:15:15.331+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.235.0>:ns_port_server:log:224]ns_couchdb<0.235.0>: Apache CouchDB  (LogLevel=info) is starting.
ns_couchdb<0.235.0>: Apache CouchDB has started. Time to relax.
ns_couchdb<0.235.0>: 31469: Booted. Waiting for shutdown request
ns_couchdb<0.235.0>: working as port

[error_logger:info,2020-04-02T20:15:15.333+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.311.0>},
                       {name,eventing_settings_manager},
                       {mfargs,{eventing_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.333+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.313.0>},
                       {name,audit_events},
                       {mfargs,{gen_event,start_link,[{local,audit_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.333+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:302]Suspended by process <0.255.0>
[ns_server:debug,2020-04-02T20:15:15.333+05:30,ns_1@127.0.0.1:memcached_passwords<0.255.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-04-02T20:15:15.333+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:309]Released by process <0.255.0>
[ns_server:debug,2020-04-02T20:15:15.333+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:warn,2020-04-02T20:15:15.334+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-04-02T20:15:15.334+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2020-04-02T20:15:15.340+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.316.0>},
                       {id,menelaus_ui_auth},
                       {mfargs,{menelaus_ui_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.341+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.318.0>},
                       {id,scram_sha},
                       {mfargs,{scram_sha,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.344+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.319.0>},
                       {id,menelaus_local_auth},
                       {mfargs,{menelaus_local_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.347+05:30,ns_1@127.0.0.1:ns_heart<0.291.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2020-04-02T20:15:15.348+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.320.0>},
                       {id,menelaus_web_cache},
                       {mfargs,{menelaus_web_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.349+05:30,ns_1@127.0.0.1:ns_heart<0.291.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2020-04-02T20:15:15.350+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.324.0>},
                       {id,menelaus_stats_gatherer},
                       {mfargs,{menelaus_stats_gatherer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.350+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.326.0>},
                       {id,json_rpc_events},
                       {mfargs,
                           {gen_event,start_link,[{local,json_rpc_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.356+05:30,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.294.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]},
                 {proc_lib,init_p_do_apply,3,
                           [{file,"proc_lib.erl"},{line,247}]}]}}

[ns_server:debug,2020-04-02T20:15:15.356+05:30,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.294.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]}]}}

[ns_server:debug,2020-04-02T20:15:15.357+05:30,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.294.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-04-02T20:15:15.357+05:30,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.294.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[ns_server:info,2020-04-02T20:15:15.357+05:30,ns_1@127.0.0.1:<0.329.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-04-02T20:15:15.357+05:30,ns_1@127.0.0.1:<0.329.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-04-02T20:15:15.357+05:30,ns_1@127.0.0.1:<0.329.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-04-02T20:15:15.357+05:30,ns_1@127.0.0.1:<0.329.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2020-04-02T20:15:15.366+05:30,ns_1@127.0.0.1:<0.327.0>:restartable:start_child:98]Started child process <0.329.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2020-04-02T20:15:15.366+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.329.0>,menelaus_web}
             started: [{pid,<0.351.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                           {menelaus_web,http_server,
                               [[{ip,"0.0.0.0"},{name,menelaus_web_ipv4}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.366+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.327.0>},
                       {id,menelaus_web},
                       {mfargs,
                           {restartable,start_link,
                               [{menelaus_web,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:15.367+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.368.0>},
                       {id,menelaus_event},
                       {mfargs,{menelaus_event,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.368+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.369.0>},
                       {id,hot_keys_keeper},
                       {mfargs,{hot_keys_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.369+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.370.0>},
                       {id,menelaus_web_alerts_srv},
                       {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.370+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.371.0>},
                       {id,menelaus_cbauth},
                       {mfargs,{menelaus_cbauth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[user:info,2020-04-02T20:15:15.370+05:30,ns_1@127.0.0.1:ns_server_sup<0.248.0>:menelaus_sup:start_link:48]Couchbase Server has started on web port 8091 on node 'ns_1@127.0.0.1'. Version: "6.5.0-4966-community".
[error_logger:info,2020-04-02T20:15:15.370+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.314.0>},
                       {name,menelaus},
                       {mfargs,{menelaus_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:15.371+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.377.0>},
                       {name,ns_ports_setup},
                       {mfargs,{ns_ports_setup,start,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.371+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.381.0>},
                       {id,service_agent_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_agent_children_sup},
                                service_agent_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:15.372+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.382.0>},
                       {id,service_agent_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<service_agent_sup.0.107373856>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.372+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.380.0>},
                       {name,service_agent_sup},
                       {mfargs,{service_agent_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:15:15.373+05:30,ns_1@127.0.0.1:ns_ports_setup<0.377.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,goxdcr]
[error_logger:info,2020-04-02T20:15:15.376+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.384.0>},
                       {name,ns_memcached_sockets_pool},
                       {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.380+05:30,ns_1@127.0.0.1:memcached_auth_server<0.385.0>:memcached_auth_server:reconnect:233]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2020-04-02T20:15:15.380+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.385.0>},
                       {name,memcached_auth_server},
                       {mfargs,{memcached_auth_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.381+05:30,ns_1@127.0.0.1:ns_audit_cfg<0.388.0>:ns_audit_cfg:write_audit_json:259]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "48537283"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2020-04-02T20:15:15.383+05:30,ns_1@127.0.0.1:ns_audit_cfg<0.388.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[error_logger:info,2020-04-02T20:15:15.383+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.388.0>},
                       {name,ns_audit_cfg},
                       {mfargs,{ns_audit_cfg,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-04-02T20:15:15.384+05:30,ns_1@127.0.0.1:<0.391.0>:ns_memcached:connect:1104]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:debug,2020-04-02T20:15:15.387+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.393.0>:memcached_config_mgr:init:49]waiting for completion of initial ns_ports_setup round
[error_logger:info,2020-04-02T20:15:15.387+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.392.0>},
                       {name,ns_audit},
                       {mfargs,{ns_audit,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.387+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.393.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:15:15.389+05:30,ns_1@127.0.0.1:<0.394.0>:ns_memcached_log_rotator:init:42]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2020-04-02T20:15:15.389+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.394.0>},
                       {name,ns_memcached_log_rotator},
                       {mfargs,{ns_memcached_log_rotator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.390+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.395.0>},
                       {name,testconditions_store},
                       {mfargs,{simple_store,start_link,[testconditions]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.391+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.396.0>},
                       {name,terse_cluster_info_uploader},
                       {mfargs,{terse_cluster_info_uploader,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.393+05:30,ns_1@127.0.0.1:terse_cluster_info_uploader<0.396.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":7,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"kv\":11210,\"capi\":8092,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:warn,2020-04-02T20:15:15.394+05:30,ns_1@127.0.0.1:<0.400.0>:ns_memcached:connect:1104]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[error_logger:info,2020-04-02T20:15:15.394+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.401.0>},
                       {id,ns_bucket_sup},
                       {mfargs,{ns_bucket_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:15.395+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.402.0>},
                       {id,ns_bucket_worker},
                       {mfargs,{ns_bucket_worker,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.396+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.398.0>},
                       {name,ns_bucket_worker_sup},
                       {mfargs,{ns_bucket_worker_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:15.397+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.404.0>},
                       {name,system_stats_collector},
                       {mfargs,{system_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.397+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.408.0>},
                       {name,{stats_archiver,"@system"}},
                       {mfargs,{stats_archiver,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.399+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.410.0>},
                       {name,{stats_reader,"@system"}},
                       {mfargs,{stats_reader,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.399+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.411.0>},
                       {name,{stats_archiver,"@system-processes"}},
                       {mfargs,
                           {stats_archiver,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.399+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.413.0>},
                       {name,{stats_reader,"@system-processes"}},
                       {mfargs,
                           {stats_reader,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.400+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.414.0>},
                       {name,{stats_archiver,"@query"}},
                       {mfargs,{stats_archiver,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.400+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.416.0>},
                       {name,{stats_reader,"@query"}},
                       {mfargs,{stats_reader,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.407+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.417.0>},
                       {name,query_stats_collector},
                       {mfargs,{query_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.407+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.419.0>},
                       {name,{stats_archiver,"@global"}},
                       {mfargs,{stats_archiver,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.407+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.421.0>},
                       {name,{stats_reader,"@global"}},
                       {mfargs,{stats_reader,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.409+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.422.0>},
                       {name,global_stats_collector},
                       {mfargs,{global_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.410+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.424.0>},
                       {name,goxdcr_status_keeper},
                       {mfargs,{goxdcr_status_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.410+05:30,ns_1@127.0.0.1:goxdcr_status_keeper<0.424.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-04-02T20:15:15.411+05:30,ns_1@127.0.0.1:goxdcr_status_keeper<0.424.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2020-04-02T20:15:15.411+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.428.0>},
                       {id,service_stats_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_stats_children_sup},
                                services_stats_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:15.412+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.430.0>},
                       {id,service_status_keeper_worker},
                       {mfargs,
                           {work_queue,start_link,
                               [service_status_keeper_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.422+05:30,ns_1@127.0.0.1:ns_ports_setup<0.377.0>:ns_ports_setup:set_children:85]Monitor ns_child_ports_sup <12938.107.0>
[ns_server:debug,2020-04-02T20:15:15.423+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.393.0>:memcached_config_mgr:init:51]ns_ports_setup seems to be ready
[error_logger:info,2020-04-02T20:15:15.424+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.432.0>},
                       {id,service_status_keeper_index},
                       {mfargs,{service_index,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.427+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.393.0>:memcached_config_mgr:find_port_pid_loop:137]Found memcached port <12938.114.0>
[error_logger:info,2020-04-02T20:15:15.428+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.437.0>},
                       {id,service_status_keeper_fts},
                       {mfargs,{service_fts,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.431+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.440.0>},
                       {id,service_status_keeper_eventing},
                       {mfargs,{service_eventing,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.431+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.429.0>},
                       {id,service_status_keeper_sup},
                       {mfargs,{service_status_keeper_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:15.431+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.443.0>},
                       {id,service_stats_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<services_stats_sup.0.108537742>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.431+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.427.0>},
                       {name,services_stats_sup},
                       {mfargs,{services_stats_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:15:15.436+05:30,ns_1@127.0.0.1:<0.447.0>:new_concurrency_throttle:init:115]init concurrent throttle process, pid: <0.447.0>, type: kv_throttle# of available token: 1
[ns_server:debug,2020-04-02T20:15:15.437+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:15:15.437+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[error_logger:info,2020-04-02T20:15:15.437+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.445.0>},
                       {name,compaction_daemon},
                       {mfargs,{compaction_daemon,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.437+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:15:15.437+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:15:15.438+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:15:15.438+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2020-04-02T20:15:15.439+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,cluster_logs_sup}
             started: [{pid,<0.450.0>},
                       {id,ets_holder},
                       {mfargs,
                           {cluster_logs_collection_task,
                               start_link_ets_holder,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.439+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.449.0>},
                       {name,cluster_logs_sup},
                       {mfargs,{cluster_logs_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:15.439+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.451.0>},
                       {name,leader_events},
                       {mfargs,{gen_event,start_link,[{local,leader_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.446+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.393.0>:memcached_config_mgr:init:82]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[error_logger:info,2020-04-02T20:15:15.445+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.455.0>},
                       {id,leader_activities},
                       {mfargs,{leader_activities,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.446+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.393.0>:memcached_config_mgr:init:86]activated memcached port server
[error_logger:info,2020-04-02T20:15:15.447+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.456.0>},
                       {id,leader_lease_agent},
                       {mfargs,{leader_lease_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.447+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.454.0>},
                       {id,leader_leases_sup},
                       {mfargs,{leader_leases_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:15.449+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.458.0>},
                       {id,leader_registry_server},
                       {mfargs,{leader_registry_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.451+05:30,ns_1@127.0.0.1:leader_registry_sup<0.457.0>:mb_master:check_master_takeover_needed:283]Sending master node question to the following nodes: []
[ns_server:debug,2020-04-02T20:15:15.451+05:30,ns_1@127.0.0.1:leader_registry_sup<0.457.0>:mb_master:check_master_takeover_needed:285]Got replies: []
[ns_server:debug,2020-04-02T20:15:15.451+05:30,ns_1@127.0.0.1:leader_registry_sup<0.457.0>:mb_master:check_master_takeover_needed:291]Was unable to discover master, not going to force mastership takeover
[user:info,2020-04-02T20:15:15.453+05:30,ns_1@127.0.0.1:mb_master<0.461.0>:mb_master:init:103]I'm the only node, so I'm the master.
[ns_server:debug,2020-04-02T20:15:15.453+05:30,ns_1@127.0.0.1:leader_registry<0.458.0>:leader_registry_server:handle_new_leader:241]New leader is 'ns_1@127.0.0.1'. Invalidating name cache.
[ns_server:debug,2020-04-02T20:15:15.457+05:30,ns_1@127.0.0.1:mb_master<0.461.0>:master_activity_events:submit_cast:82]Failed to send master activity event: {error,badarg}
[error_logger:info,2020-04-02T20:15:15.458+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.464.0>},
                       {id,leader_lease_acquirer},
                       {mfargs,{leader_lease_acquirer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.459+05:30,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.466.0>:leader_quorum_nodes_manager:pull_config:114]Attempting to pull config from nodes:
[]
[ns_server:debug,2020-04-02T20:15:15.459+05:30,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.466.0>:leader_quorum_nodes_manager:pull_config:119]Pulled config successfully.
[error_logger:info,2020-04-02T20:15:15.459+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.466.0>},
                       {id,leader_quorum_nodes_manager},
                       {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.461+05:30,ns_1@127.0.0.1:leader_lease_agent<0.456.0>:leader_lease_agent:do_handle_acquire_lease:149]Granting lease to {lease_holder,<<"c18c3cca366a5e87e04ddda76eb50bce">>,
                                'ns_1@127.0.0.1'} for 15000ms
[ns_server:info,2020-04-02T20:15:15.462+05:30,ns_1@127.0.0.1:mb_master_sup<0.463.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.473.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-04-02T20:15:15.462+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.473.0>},
                       {id,ns_tick},
                       {mfargs,{ns_tick,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.463+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.475.0>},
                       {id,compat_mode_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,compat_mode_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:15:15.464+05:30,ns_1@127.0.0.1:<0.470.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:302]Acquired lease from node 'ns_1@127.0.0.1' (lease uuid: <<"c18c3cca366a5e87e04ddda76eb50bce">>)
[error_logger:info,2020-04-02T20:15:15.464+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.476.0>},
                       {id,compat_mode_manager},
                       {mfargs,{compat_mode_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.466+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.478.0>},
                       {id,ns_janitor_server},
                       {mfargs,{ns_janitor_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:15:15.467+05:30,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.477.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.479.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-04-02T20:15:15.467+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.479.0>},
                       {id,auto_reprovision},
                       {mfargs,{auto_reprovision,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:15:15.468+05:30,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.477.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.480.0> on 'ns_1@127.0.0.1'

[ns_server:info,2020-04-02T20:15:15.468+05:30,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.477.0>:misc:start_singleton:857]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.481.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-04-02T20:15:15.468+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.480.0>},
                       {id,auto_rebalance},
                       {mfargs,{auto_rebalance,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.468+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.481.0>},
                       {id,ns_orchestrator},
                       {mfargs,{ns_orchestrator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.468+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.477.0>},
                       {id,ns_orchestrator_child_sup},
                       {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:15:15.470+05:30,ns_1@127.0.0.1:<0.483.0>:auto_failover:init:185]init auto_failover.
[user:info,2020-04-02T20:15:15.470+05:30,ns_1@127.0.0.1:<0.483.0>:auto_failover:handle_call:216]Enabled auto-failover with timeout 120 and max count 1
[ns_server:info,2020-04-02T20:15:15.473+05:30,ns_1@127.0.0.1:ns_orchestrator_sup<0.474.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.483.0> on 'ns_1@127.0.0.1'

[ns_server:debug,2020-04-02T20:15:15.473+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"dce74b57ae3924cb616de84cba56b09d">>} ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{8,63753057915}}]}]
[ns_server:debug,2020-04-02T20:15:15.473+05:30,ns_1@127.0.0.1:ns_config_rep<0.269.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([auto_failover_cfg,
                               {local_changes_count,
                                   <<"dce74b57ae3924cb616de84cba56b09d">>}]..)
[ns_server:debug,2020-04-02T20:15:15.473+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"dce74b57ae3924cb616de84cba56b09d">>,{2,63753057672}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,false}]
[error_logger:info,2020-04-02T20:15:15.473+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.483.0>},
                       {id,auto_failover},
                       {mfargs,{auto_failover,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:15:15.473+05:30,ns_1@127.0.0.1:mb_master_sup<0.463.0>:misc:start_singleton:857]start_singleton(work_queue, start_link, [{via,leader_registry,collections}]): started as <0.486.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-04-02T20:15:15.473+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.474.0>},
                       {id,ns_orchestrator_sup},
                       {mfargs,{ns_orchestrator_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:15:15.473+05:30,ns_1@127.0.0.1:<0.452.0>:restartable:start_child:98]Started child process <0.453.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2020-04-02T20:15:15.473+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.486.0>},
                       {id,collections},
                       {mfargs,{collections,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.473+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.461.0>},
                       {id,mb_master},
                       {mfargs,{mb_master,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:15.473+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.457.0>},
                       {id,leader_registry_sup},
                       {mfargs,{leader_registry_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:15.473+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.452.0>},
                       {name,leader_services_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{leader_services_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:15.475+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.491.0>},
                       {name,ns_tick_agent},
                       {mfargs,{ns_tick_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.475+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.493.0>},
                       {name,master_activity_events_ingress},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events_ingress}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.475+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.494.0>},
                       {name,master_activity_events_timestamper},
                       {mfargs,
                           {master_activity_events,start_link_timestamper,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.479+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.495.0>},
                       {name,master_activity_events_pids_watcher},
                       {mfargs,
                           {master_activity_events_pids_watcher,start_link,
                               []}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.481+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.496.0>},
                       {name,master_activity_events_keeper},
                       {mfargs,{master_activity_events_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.484+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.499.0>},
                       {id,ns_server_monitor},
                       {mfargs,{ns_server_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.484+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.501.0>},
                       {id,service_monitor_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_monitor_children_sup},
                                health_monitor_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:15.484+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.502.0>},
                       {id,service_monitor_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<health_monitor_sup.0.112499759>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.485+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.508.0>},
                       {id,node_monitor},
                       {mfargs,{node_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.486+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.514.0>},
                       {id,node_status_analyzer},
                       {mfargs,{node_status_analyzer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.486+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.498.0>},
                       {name,health_monitor_sup},
                       {mfargs,{health_monitor_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:15.487+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.516.0>},
                       {name,rebalance_agent},
                       {mfargs,{rebalance_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.489+05:30,ns_1@127.0.0.1:ns_server_nodes_sup<0.209.0>:one_shot_barrier:notify:27]Notifying on barrier menelaus_barrier
[ns_server:debug,2020-04-02T20:15:15.489+05:30,ns_1@127.0.0.1:menelaus_barrier<0.211.0>:one_shot_barrier:barrier_body:62]Barrier menelaus_barrier got notification from <0.209.0>
[error_logger:info,2020-04-02T20:15:15.489+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.517.0>},
                       {name,ns_rebalance_report_manager},
                       {mfargs,{ns_rebalance_report_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:15.489+05:30,ns_1@127.0.0.1:ns_server_nodes_sup<0.209.0>:one_shot_barrier:notify:32]Successfuly notified on barrier menelaus_barrier
[error_logger:info,2020-04-02T20:15:15.489+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.248.0>},
                       {name,ns_server_sup},
                       {mfargs,{ns_server_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:15:15.489+05:30,ns_1@127.0.0.1:<0.208.0>:restartable:start_child:98]Started child process <0.209.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2020-04-02T20:15:15.489+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.208.0>},
                       {id,ns_server_nodes_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_server_nodes_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:15:15.490+05:30,ns_1@127.0.0.1:<0.5.0>:child_erlang:child_loop:130]31409: Entered child_loop
[error_logger:info,2020-04-02T20:15:15.490+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.519.0>},
                       {id,remote_api},
                       {mfargs,{remote_api,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:15:15.490+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.187.0>},
                       {id,ns_server_cluster_sup},
                       {mfargs,{ns_server_cluster_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:15:15.490+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
         application: ns_server
          started_at: 'ns_1@127.0.0.1'

[ns_server:debug,2020-04-02T20:15:15.491+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.225.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@",admin}
[ns_server:debug,2020-04-02T20:15:15.497+05:30,ns_1@127.0.0.1:json_rpc_connection-goxdcr-cbauth<0.520.0>:json_rpc_connection:init:73]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.520.0>
[ns_server:debug,2020-04-02T20:15:15.497+05:30,ns_1@127.0.0.1:json_rpc_connection-saslauthd-saslauthd-port<0.521.0>:json_rpc_connection:init:73]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.521.0>
[ns_server:debug,2020-04-02T20:15:15.497+05:30,ns_1@127.0.0.1:menelaus_cbauth<0.371.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"goxdcr-cbauth",<0.520.0>} started
[ns_server:debug,2020-04-02T20:15:15.503+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.225.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:debug,2020-04-02T20:15:16.316+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:89]Refresh of [rbac,isasl] succeeded
[ns_server:debug,2020-04-02T20:15:16.471+05:30,ns_1@127.0.0.1:<0.483.0>:auto_failover_logic:log_master_activity:177]Transitioned node {'ns_1@127.0.0.1',<<"dce74b57ae3924cb616de84cba56b09d">>} state new -> up
[ns_server:debug,2020-04-02T20:15:45.439+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:15:45.439+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:15:45.439+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:15:45.439+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:15:58.169+05:30,ns_1@127.0.0.1:<0.580.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.367.0>} exited with reason normal
[ns_server:debug,2020-04-02T20:15:58.170+05:30,ns_1@127.0.0.1:<0.543.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.357.0>} exited with reason normal
[ns_server:debug,2020-04-02T20:15:58.170+05:30,ns_1@127.0.0.1:<0.527.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.356.0>} exited with reason normal
[ns_server:debug,2020-04-02T20:15:58.170+05:30,ns_1@127.0.0.1:json_rpc_connection-goxdcr-cbauth<0.520.0>:json_rpc_connection:handle_info:129]Socket closed
[ns_server:debug,2020-04-02T20:15:58.170+05:30,ns_1@127.0.0.1:menelaus_cbauth<0.371.0>:menelaus_cbauth:handle_info:142]Observed json rpc process {"goxdcr-cbauth",<0.520.0>} died with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.169+05:30,ns_1@127.0.0.1:<0.563.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.542.0>} exited with reason normal
[user:debug,2020-04-02T20:15:58.171+05:30,ns_1@127.0.0.1:<0.254.0>:ns_log:crash_consumption_loop:69]Service 'goxdcr' exited with status 0. Restarting. Messages:
2020-04-02T20:15:45.514+05:30 INFO GOXDCR.ResourceMgr: Resource Manager State = overallTP: 0 highTP: 0 highExist: false lowExist: false backlogExist: false maxTP: 0 highTPNeeded: 0 highTokens: 0 maxTokens: 0 lowTPLimit: 0 calibration: None dcpAction: Reset processCpu: 0 idleCpu: 93
2020-04-02T20:15:45.514+05:30 INFO GOXDCR.ResourceMgr: backlogCount=0, noBacklogCount=30 extraQuota=false cpuNotMaxedCount=0 throughputDropCount=0
2020-04-02T20:15:45.514+05:30 INFO GOXDCR.ResourceMgr: DcpPriorityMap=map[]
ongoingReplMap=map[]
2020-04-02T20:15:45.514+05:30 INFO GOXDCR.PipelineMgr: Replication Status = map[]
2020-04-02T20:15:55.514+05:30 INFO GOXDCR.ResourceMgr: Resource Manager State = overallTP: 0 highTP: 0 highExist: false lowExist: false backlogExist: false maxTP: 0 highTPNeeded: 0 highTokens: 0 maxTokens: 0 lowTPLimit: 0 calibration: None dcpAction: Reset processCpu: 0 idleCpu: 91
2020-04-02T20:15:55.514+05:30 INFO GOXDCR.ResourceMgr: backlogCount=0, noBacklogCount=40 extraQuota=false cpuNotMaxedCount=0 throughputDropCount=0
2020-04-02T20:15:55.514+05:30 INFO GOXDCR.ResourceMgr: DcpPriorityMap=map[]
ongoingReplMap=map[]

[ns_server:debug,2020-04-02T20:15:58.172+05:30,ns_1@127.0.0.1:json_rpc_connection-saslauthd-saslauthd-port<0.521.0>:json_rpc_connection:handle_info:129]Socket closed
[user:debug,2020-04-02T20:15:58.172+05:30,ns_1@127.0.0.1:<0.254.0>:ns_log:crash_consumption_loop:69]Service 'saslauthd_port' exited with status 0. Restarting. Messages:
2020/04/02 20:15:58 Got EOL. Exiting
[user:debug,2020-04-02T20:15:58.175+05:30,ns_1@127.0.0.1:<0.254.0>:ns_log:crash_consumption_loop:69]Service 'memcached' exited with status 0. Restarting. Messages:
EOL on stdin.  Initiating shutdown
[ns_server:debug,2020-04-02T20:15:58.175+05:30,ns_1@127.0.0.1:<0.436.0>:remote_monitors:monitor_loop:129]Monitored remote process <12938.114.0> went down with: shutdown
[ns_server:debug,2020-04-02T20:15:58.175+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.393.0>:memcached_config_mgr:handle_info:163]Got DOWN with reason: shutdown from memcached port server: <12938.114.0>. Shutting down
[ns_server:debug,2020-04-02T20:15:58.175+05:30,ns_1@127.0.0.1:<0.448.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.393.0>} exited with reason {shutdown,
                                                                                {memcached_port_server_down,
                                                                                 <12938.114.0>,
                                                                                 shutdown}}
[ns_server:debug,2020-04-02T20:15:58.175+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.1393.0>:memcached_config_mgr:init:49]waiting for completion of initial ns_ports_setup round
[error_logger:error,2020-04-02T20:15:58.175+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {shutdown,
                     {memcached_port_server_down,<12938.114.0>,shutdown}}
     Offender:   [{pid,<0.393.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2020-04-02T20:15:58.175+05:30,ns_1@127.0.0.1:<0.431.0>:remote_monitors:monitor_loop:129]Monitored remote process <12938.107.0> went down with: shutdown
[ns_server:debug,2020-04-02T20:15:58.175+05:30,ns_1@127.0.0.1:<0.5.0>:child_erlang:child_loop:134]31409: Got EOL
[ns_server:info,2020-04-02T20:15:58.175+05:30,ns_1@127.0.0.1:<0.5.0>:ns_bootstrap:stop:40]Initiated server shutdown
[ns_server:debug,2020-04-02T20:15:58.175+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.1393.0>:memcached_config_mgr:init:51]ns_ports_setup seems to be ready
[error_logger:info,2020-04-02T20:15:58.175+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.1393.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:15:58.175+05:30,ns_1@127.0.0.1:ns_ports_setup<0.377.0>:ns_ports_setup:children_loop_continue:121]ns_child_ports_sup <12938.107.0> died on babysitter node with shutdown. Restart.
[error_logger:info,2020-04-02T20:15:58.175+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]Initiated server shutdown
[ns_server:debug,2020-04-02T20:15:58.175+05:30,ns_1@127.0.0.1:<0.379.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.377.0>} exited with reason {{child_ports_sup_died,
                                                                                    <12938.107.0>,
                                                                                    shutdown},
                                                                                   [{ns_ports_setup,
                                                                                     children_loop_continue,
                                                                                     3,
                                                                                     [{file,
                                                                                       "src/ns_ports_setup.erl"},
                                                                                      {line,
                                                                                       122}]},
                                                                                    {proc_lib,
                                                                                     wake_up,
                                                                                     3,
                                                                                     [{file,
                                                                                       "proc_lib.erl"},
                                                                                      {line,
                                                                                       257}]}]}
[ns_server:debug,2020-04-02T20:15:58.175+05:30,ns_1@127.0.0.1:<0.378.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.377.0>} exited with reason {{child_ports_sup_died,
                                                                                 <12938.107.0>,
                                                                                 shutdown},
                                                                                [{ns_ports_setup,
                                                                                  children_loop_continue,
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/ns_ports_setup.erl"},
                                                                                   {line,
                                                                                    122}]},
                                                                                 {proc_lib,
                                                                                  wake_up,
                                                                                  3,
                                                                                  [{file,
                                                                                    "proc_lib.erl"},
                                                                                   {line,
                                                                                    257}]}]}
[ns_server:debug,2020-04-02T20:15:58.175+05:30,ns_1@127.0.0.1:<0.518.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.517.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.175+05:30,ns_1@127.0.0.1:<0.515.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.514.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.175+05:30,ns_1@127.0.0.1:<0.509.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.508.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.175+05:30,ns_1@127.0.0.1:<0.500.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.499.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.176+05:30,ns_1@127.0.0.1:<0.497.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {master_activity_events,<0.496.0>} exited with reason killed
[ns_server:debug,2020-04-02T20:15:58.176+05:30,ns_1@127.0.0.1:<0.503.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.502.0>} exited with reason shutdown
[ns_server:info,2020-04-02T20:15:58.176+05:30,ns_1@127.0.0.1:mb_master<0.461.0>:mb_master:terminate:327]Synchronously shutting down child mb_master_sup
[ns_server:debug,2020-04-02T20:15:58.176+05:30,ns_1@127.0.0.1:<0.492.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {leader_events,<0.491.0>} exited with reason shutdown
[ns_server:info,2020-04-02T20:15:58.176+05:30,ns_1@127.0.0.1:leader_registry<0.458.0>:leader_registry_server:handle_down:253]Process <0.486.0> registered as 'collections' terminated.
[ns_server:info,2020-04-02T20:15:58.176+05:30,ns_1@127.0.0.1:leader_registry<0.458.0>:leader_registry_server:handle_down:253]Process <0.483.0> registered as 'auto_failover' terminated.
[ns_server:debug,2020-04-02T20:15:58.176+05:30,ns_1@127.0.0.1:<0.484.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {compat_mode_events,<0.483.0>} exited with reason shutdown
[ns_server:info,2020-04-02T20:15:58.176+05:30,ns_1@127.0.0.1:leader_registry<0.458.0>:leader_registry_server:handle_down:253]Process <0.481.0> registered as 'ns_orchestrator' terminated.
[ns_server:info,2020-04-02T20:15:58.176+05:30,ns_1@127.0.0.1:leader_registry<0.458.0>:leader_registry_server:handle_down:253]Process <0.480.0> registered as 'auto_rebalance' terminated.
[ns_server:info,2020-04-02T20:15:58.176+05:30,ns_1@127.0.0.1:leader_registry<0.458.0>:leader_registry_server:handle_down:253]Process <0.479.0> registered as 'auto_reprovision' terminated.
[ns_server:debug,2020-04-02T20:15:58.176+05:30,ns_1@127.0.0.1:leader_activities<0.455.0>:leader_activities:handle_internal_process_down:511]Process {quorum_nodes_manager,<0.466.0>} terminated with reason shutdown
[ns_server:info,2020-04-02T20:15:58.176+05:30,ns_1@127.0.0.1:leader_registry<0.458.0>:leader_registry_server:handle_down:253]Process <0.473.0> registered as 'ns_tick' terminated.
[ns_server:debug,2020-04-02T20:15:58.176+05:30,ns_1@127.0.0.1:<0.469.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.466.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.176+05:30,ns_1@127.0.0.1:leader_activities<0.455.0>:leader_activities:handle_internal_process_down:511]Process {acquirer,<0.464.0>} terminated with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.176+05:30,ns_1@127.0.0.1:leader_lease_agent<0.456.0>:leader_lease_agent:handle_abolish_lease:255]Received abolish lease request from {lease_holder,
                                     <<"c18c3cca366a5e87e04ddda76eb50bce">>,
                                     'ns_1@127.0.0.1'} when lease is {lease,
                                                                      {lease_holder,
                                                                       <<"c18c3cca366a5e87e04ddda76eb50bce">>,
                                                                       'ns_1@127.0.0.1'},
                                                                      -576460708083871558,
                                                                      -576460693083871558,
                                                                      {timer,
                                                                       #Ref<0.489408117.1238892545.193952>,
                                                                       {lease_expired,
                                                                        {lease_holder,
                                                                         <<"c18c3cca366a5e87e04ddda76eb50bce">>,
                                                                         'ns_1@127.0.0.1'}}},
                                                                      active}
[ns_server:debug,2020-04-02T20:15:58.176+05:30,ns_1@127.0.0.1:leader_lease_agent<0.456.0>:leader_lease_agent:handle_abolish_lease:260]Expiring abolished lease
[ns_server:debug,2020-04-02T20:15:58.176+05:30,ns_1@127.0.0.1:<0.465.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.464.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.176+05:30,ns_1@127.0.0.1:leader_registry<0.458.0>:leader_registry_server:handle_new_leader:241]New leader is undefined. Invalidating name cache.
[ns_server:debug,2020-04-02T20:15:58.176+05:30,ns_1@127.0.0.1:<0.462.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.461.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.176+05:30,ns_1@127.0.0.1:<0.459.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {leader_events,<0.458.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.177+05:30,ns_1@127.0.0.1:leader_activities<0.455.0>:leader_activities:handle_internal_process_down:511]Process {agent,<0.456.0>} terminated with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.177+05:30,ns_1@127.0.0.1:<0.452.0>:restartable:shutdown_child:120]Successfully terminated process <0.453.0>
[ns_server:debug,2020-04-02T20:15:58.177+05:30,ns_1@127.0.0.1:ns_ports_setup<0.1394.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,goxdcr]
[ns_server:debug,2020-04-02T20:15:58.177+05:30,ns_1@127.0.0.1:<0.446.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.445.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.177+05:30,ns_1@127.0.0.1:<0.442.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.440.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.177+05:30,ns_1@127.0.0.1:<0.444.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.443.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.177+05:30,ns_1@127.0.0.1:<0.441.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.440.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.177+05:30,ns_1@127.0.0.1:<0.439.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.437.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.177+05:30,ns_1@127.0.0.1:<0.438.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.437.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.177+05:30,ns_1@127.0.0.1:<0.434.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.432.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.177+05:30,ns_1@127.0.0.1:<0.433.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.432.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.177+05:30,ns_1@127.0.0.1:ns_ports_setup<0.1394.0>:misc:delaying_crash:1608]Delaying crash exit:{noproc,
                     {gen_server,call,
                      [{ns_ports_manager,'babysitter_of_ns_1@cb.local'},
                       {set_dynamic_children,
                        [{memcached,"/opt/couchbase/bin/memcached",
                          ["-C",
                           "/opt/couchbase/var/lib/couchbase/config/memcached.json"],
                          [{env,
                            [{"EVENT_NOSELECT","1"},
                             {"MEMCACHED_TOP_KEYS","5"},
                             {"CBSASL_PWFILE",
                              "/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
                           use_stdio,stderr_to_stdout,exit_status,
                           port_server_dont_start,stream]},
                         {saslauthd_port,"/opt/couchbase/bin/saslauthd-port",
                          [],
                          [use_stdio,exit_status,stderr_to_stdout,
                           {env,
                            [{"GOTRACEBACK","single"},
                             {"CBAUTH_REVRPC_URL",
                              "http://%40:c34ee7c5b5fcae0e90401b9e672db376@127.0.0.1:8091/saslauthd"}]}]},
                         {goxdcr,"/opt/couchbase/bin/goxdcr",
                          ["-sourceKVAdminPort=8091","-xdcrRestPort=9998",
                           "-isEnterprise=false","-ipv6=false"],
                          [via_goport,exit_status,stderr_to_stdout,
                           {env,
                            [{"GOTRACEBACK","single"},
                             {"CBAUTH_REVRPC_URL",
                              "http://%40:c34ee7c5b5fcae0e90401b9e672db376@127.0.0.1:8091/goxdcr"}]},
                           {log,"goxdcr.log"}]}]},
                       infinity]}} by 1000ms
Stacktrace: [{gen_server,call,3,[{file,"gen_server.erl"},{line,214}]},
             {ns_ports_setup,set_children,2,
                             [{file,"src/ns_ports_setup.erl"},{line,81}]},
             {ns_ports_setup,set_children_and_loop,3,
                             [{file,"src/ns_ports_setup.erl"},{line,97}]},
             {misc,delaying_crash,2,[{file,"src/misc.erl"},{line,1605}]},
             {proc_lib,init_p_do_apply,3,[{file,"proc_lib.erl"},{line,247}]}]
[ns_server:debug,2020-04-02T20:15:58.177+05:30,ns_1@127.0.0.1:<0.423.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_tick_event,<0.422.0>} exited with reason shutdown
[error_logger:error,2020-04-02T20:15:58.177+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_ports_setup:setup_body_tramp/0
    pid: <0.377.0>
    registered_name: ns_ports_setup
    exception error: {child_ports_sup_died,<12938.107.0>,shutdown}
      in function  ns_ports_setup:children_loop_continue/3 (src/ns_ports_setup.erl, line 122)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.208.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.378.0>,<0.379.0>,<0.248.0>]
    dictionary: [{'ns_ports_setup-goxdcr-available',
                      "/opt/couchbase/bin/goxdcr"},
                  {'ns_ports_setup-projector-available',
                      "/opt/couchbase/bin/projector"},
                  {'ns_ports_setup-indexer-available',
                      "/opt/couchbase/bin/indexer"},
                  {'ns_ports_setup-eventing-producer-available',
                      "/opt/couchbase/bin/eventing-producer"},
                  {'ns_ports_setup-saslauthd-port-available',
                      "/opt/couchbase/bin/saslauthd-port"},
                  {'ns_ports_setup-cbas-available',"/opt/couchbase/bin/cbas"},
                  {'ns_ports_setup-cbft-available',"/opt/couchbase/bin/cbft"},
                  {'ns_ports_setup-cbq-engine-available',
                      "/opt/couchbase/bin/cbq-engine"}]
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 11602
  neighbours:

[error_logger:error,2020-04-02T20:15:58.178+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{child_ports_sup_died,<12938.107.0>,shutdown},
                  [{ns_ports_setup,children_loop_continue,3,
                                   [{file,"src/ns_ports_setup.erl"},
                                    {line,122}]},
                   {proc_lib,wake_up,3,[{file,"proc_lib.erl"},{line,257}]}]}
     Offender:   [{pid,<0.377.0>},
                  {name,ns_ports_setup},
                  {mfargs,{ns_ports_setup,start,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,brutal_kill},
                  {child_type,worker}]


[error_logger:info,2020-04-02T20:15:58.178+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.1394.0>},
                       {name,ns_ports_setup},
                       {mfargs,{ns_ports_setup,start,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:error,2020-04-02T20:15:58.178+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.1393.0>
    registered_name: memcached_config_mgr
    exception exit: {noproc,
                        {gen_server,call,
                            [{ns_ports_manager,'babysitter_of_ns_1@cb.local'},
                             {find_port,memcached},
                             infinity]}}
      in function  gen_server:call/3 (gen_server.erl, line 214)
      in call from memcached_config_mgr:find_port_pid_loop/2 (src/memcached_config_mgr.erl, line 133)
      in call from memcached_config_mgr:init/1 (src/memcached_config_mgr.erl, line 52)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.208.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.248.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 1598
    stack_size: 27
    reductions: 2842
  neighbours:

[ns_server:debug,2020-04-02T20:15:58.195+05:30,ns_1@127.0.0.1:<0.420.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_stats_event,<0.419.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.195+05:30,ns_1@127.0.0.1:<0.418.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_tick_event,<0.417.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.217+05:30,ns_1@127.0.0.1:<0.415.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_stats_event,<0.414.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.248+05:30,ns_1@127.0.0.1:<0.412.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_stats_event,<0.411.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.274+05:30,ns_1@127.0.0.1:<0.409.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_stats_event,<0.408.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.274+05:30,ns_1@127.0.0.1:<0.407.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_tick_event,<0.404.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.274+05:30,ns_1@127.0.0.1:<0.406.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ale_stats_events,<0.404.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.275+05:30,ns_1@127.0.0.1:<0.403.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.402.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.275+05:30,ns_1@127.0.0.1:<0.397.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {bucket_info_cache_invalidations,<0.396.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.275+05:30,ns_1@127.0.0.1:<0.386.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.385.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.275+05:30,ns_1@127.0.0.1:<0.389.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.388.0>} exited with reason shutdown
[error_logger:error,2020-04-02T20:15:58.275+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    shutdown_error
     Reason:     {noproc,
                     {gen_server,call,
                         [{ns_ports_manager,'babysitter_of_ns_1@cb.local'},
                          {find_port,memcached},
                          infinity]}}
     Offender:   [{pid,<0.1393.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2020-04-02T20:15:58.276+05:30,ns_1@127.0.0.1:<0.1395.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.1394.0>} exited with reason killed
[ns_server:debug,2020-04-02T20:15:58.276+05:30,ns_1@127.0.0.1:<0.383.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.382.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.276+05:30,ns_1@127.0.0.1:<0.1396.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.1394.0>} exited with reason killed
[ns_server:debug,2020-04-02T20:15:58.276+05:30,ns_1@127.0.0.1:<0.372.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {json_rpc_events,<0.371.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.276+05:30,ns_1@127.0.0.1:<0.373.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.371.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.276+05:30,ns_1@127.0.0.1:<0.376.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ssl_service_events,<0.371.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.276+05:30,ns_1@127.0.0.1:<0.375.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.371.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.276+05:30,ns_1@127.0.0.1:<0.374.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.371.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.277+05:30,ns_1@127.0.0.1:<0.327.0>:restartable:shutdown_child:120]Successfully terminated process <0.329.0>
[ns_server:debug,2020-04-02T20:15:58.278+05:30,ns_1@127.0.0.1:<0.317.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.316.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.278+05:30,ns_1@127.0.0.1:<0.312.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.311.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.278+05:30,ns_1@127.0.0.1:<0.310.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.309.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.278+05:30,ns_1@127.0.0.1:<0.308.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.307.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.279+05:30,ns_1@127.0.0.1:<0.300.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.299.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.279+05:30,ns_1@127.0.0.1:<0.296.0>:restartable:shutdown_child:120]Successfully terminated process <0.297.0>
[ns_server:debug,2020-04-02T20:15:58.279+05:30,ns_1@127.0.0.1:<0.292.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {buckets_events,<0.291.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.279+05:30,ns_1@127.0.0.1:<0.285.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.283.0>} exited with reason killed
[ns_server:debug,2020-04-02T20:15:58.279+05:30,ns_1@127.0.0.1:<0.282.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.281.0>} exited with reason killed
[ns_server:debug,2020-04-02T20:15:58.279+05:30,ns_1@127.0.0.1:<0.270.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events_local,<0.269.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.280+05:30,ns_1@127.0.0.1:<0.260.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.258.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.280+05:30,ns_1@127.0.0.1:<0.259.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.258.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.280+05:30,ns_1@127.0.0.1:<0.257.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.255.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.280+05:30,ns_1@127.0.0.1:<0.256.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.255.0>} exited with reason shutdown
[error_logger:error,2020-04-02T20:15:58.281+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: gen_event:init_it/6
    pid: <0.284.0>
    registered_name: bucket_info_cache_invalidations
    exception exit: killed
      in function  gen_event:terminate_server/4 (gen_event.erl, line 351)
    ancestors: [bucket_info_cache,ns_server_sup,ns_server_nodes_sup,
                  <0.208.0>,ns_server_cluster_sup,root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: []
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 262
  neighbours:

[ns_server:debug,2020-04-02T20:15:58.283+05:30,ns_1@127.0.0.1:<0.247.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.236.0> died with shutdown. Exiting
[ns_server:debug,2020-04-02T20:15:58.284+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.235.0>:ns_port_server:terminate:196]Shutting down port ns_couchdb
[ns_server:debug,2020-04-02T20:15:58.284+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.235.0>:ns_port_server:port_shutdown:297]Shutdown command: "shutdown"
[ns_server:debug,2020-04-02T20:15:58.293+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.489408117.1238892545.189968>,
                               inet_tcp_dist,<0.242.0>,
                               #Ref<0.489408117.1238892545.189972>}
[error_logger:info,2020-04-02T20:15:58.293+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.242.0>,connection_closed}}
[ns_server:debug,2020-04-02T20:15:58.293+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T20:15:58.293+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:15:58.293+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.235.0>:ns_port_server:terminate:199]ns_couchdb has exited
[ns_server:debug,2020-04-02T20:15:58.293+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.489408117.1238892548.191900>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:info,2020-04-02T20:15:58.293+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.235.0>:ns_port_server:log:224]ns_couchdb<0.235.0>: 31469: got shutdown request. Exiting
ns_couchdb<0.235.0>: [os_mon] cpu supervisor port (cpu_sup): Erlang has closed
ns_couchdb<0.235.0>: [os_mon] memory supervisor port (memsup): Erlang has closed

[ns_server:debug,2020-04-02T20:15:58.293+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.489408117.1238892548.191900>,
                                  inet_tcp_dist,<0.1399.0>,
                                  #Ref<0.489408117.1238892548.191904>}
[ns_server:debug,2020-04-02T20:15:58.293+05:30,ns_1@127.0.0.1:<0.230.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.228.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.293+05:30,ns_1@127.0.0.1:<0.229.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.228.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.293+05:30,ns_1@127.0.0.1:<0.227.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.225.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.294+05:30,ns_1@127.0.0.1:<0.226.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.225.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.294+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.489408117.1238892548.191900>,
                               inet_tcp_dist,<0.1399.0>,
                               #Ref<0.489408117.1238892548.191904>}
[error_logger:info,2020-04-02T20:15:58.294+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.1399.0>,shutdown}}
[error_logger:info,2020-04-02T20:15:58.294+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:15:58.294+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T20:15:58.294+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:15:58.294+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.489408117.1238892548.191911>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T20:15:58.295+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.489408117.1238892548.191911>,
                                  inet_tcp_dist,<0.1402.0>,
                                  #Ref<0.489408117.1238892548.191913>}
[error_logger:info,2020-04-02T20:15:58.295+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.1402.0>,shutdown}}
[ns_server:debug,2020-04-02T20:15:58.295+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.489408117.1238892548.191911>,
                               inet_tcp_dist,<0.1402.0>,
                               #Ref<0.489408117.1238892548.191913>}
[ns_server:debug,2020-04-02T20:15:58.295+05:30,ns_1@127.0.0.1:<0.218.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.217.0>} exited with reason shutdown
[error_logger:info,2020-04-02T20:15:58.295+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:15:58.295+05:30,ns_1@127.0.0.1:<0.208.0>:restartable:shutdown_child:120]Successfully terminated process <0.209.0>
[ns_server:debug,2020-04-02T20:15:58.295+05:30,ns_1@127.0.0.1:<0.203.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.202.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:15:58.296+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:wait_saver:866]Done waiting for saver.
[ns_server:info,2020-04-02T20:15:58.297+05:30,ns_1@127.0.0.1:<0.5.0>:ns_bootstrap:stop:44]Successfully stopped ns_server
[error_logger:info,2020-04-02T20:15:58.297+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
         application: ns_server
              exited: stopped
                type: permanent

[ns_server:info,2020-04-02T20:17:28.560+05:30,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-04-02T20:17:28.574+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-04-02T20:17:28.575+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.575+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.575+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.575+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.575+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.575+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.575+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.575+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.575+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.575+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.575+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.575+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.575+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.575+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.575+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.575+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.575+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.575+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.575+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.575+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.576+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.576+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.576+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.576+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.576+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-04-02T20:17:28.576+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-04-02T20:17:28.582+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-04-02T20:17:28.582+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[error_logger:info,2020-04-02T20:17:28.583+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.168.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2020-04-02T20:17:28.583+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.167.0>},
                       {id,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:17:28.585+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@127.0.0.1'
[error_logger:info,2020-04-02T20:17:28.593+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.593+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.593+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:28.596+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.174.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.599+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.176.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:28.599+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.177.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:17:28.599+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2020-04-02T20:17:28.599+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.175.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:28.599+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:28.601+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.601+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.602+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.180.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:28.603+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-04-02T20:17:28.603+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-04-02T20:17:28.604+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[error_logger:info,2020-04-02T20:17:28.605+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.181.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:28.605+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-04-02T20:17:28.605+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.169.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-04-02T20:17:28.606+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-04-02T20:17:28.609+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-04-02T20:17:28.609+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2020-04-02T20:17:28.610+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:17:28.610+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T20:17:28.610+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2999594286.2850029570.254919>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T20:17:28.610+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2999594286.2850029570.254919>,
                                  inet_tcp_dist,<0.185.0>,
                                  #Ref<0.2999594286.2850029570.254921>}
[ns_server:debug,2020-04-02T20:17:28.624+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-04-02T20:17:28.624+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": ok
[ns_server:info,2020-04-02T20:17:28.624+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-04-02T20:17:28.627+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-04-02T20:17:28.627+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.635+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:17:28.637+05:30,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,4,2},{20,17,28}}},
               {memory,
                   [{total,27091368},
                    {processes,10243296},
                    {processes_used,10238328},
                    {system,16848072},
                    {atom,388625},
                    {atom_used,364409},
                    {binary,153832},
                    {code,8250921},
                    {ets,1520296}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,filelib,cpu_sup,
                    io_lib_fread,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    error_handler,error_logger,file,code_server,
                    application_controller,code,application,gen_event,
                    supervisor,erl_lint,gen_server,gen,kernel,ets,heart,
                    application_master,file_io_server,filename,proc_lib,lists,
                    erl_parse,file_server,erl_eval,
                    erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{os_mon,"CPO  CXC 138 46","2.4.4"},
                    {sasl,"SASL  CXC 138 11","3.1.2"},
                    {ns_server,"Couchbase server","6.5.0-4966-community"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ale,"Another Logger for Erlang","0.0.0"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,131},
               {node,'ns_1@127.0.0.1'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,auth,httpd_sup,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    release_handler,lhttpc_sup,httpc_sup,lhttpc_manager,
                    alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    kernel_safe_sup,'sink-disk_access_int','sink-disk_access',
                    standard_error,'sink-disk_reports',ale_stats_events,
                    'sink-disk_stats','sink-disk_xdcr',timer_server,
                    'sink-disk_debug',inet_gethost_native,ale_sup,
                    'sink-disk_error',inet_db,'sink-disk_default',
                    ssl_pem_cache_dist,ale_dynamic_sup,rex,global_group,
                    net_sup,kernel_sup,ssl_connection_sup,global_name_server,
                    ssl_admin_sup,tftp_sup,ssl_sup,root_sup,erts_code_purger,
                    os_mon_sup,file_server_2,error_logger,cpu_sup,erl_epmd,
                    init,memsup,erl_signal_server,disksup,ale,net_kernel,
                    dist_manager,ssl_pem_cache,ssl_manager,ssl_dist_admin_sup,
                    ssl_dist_connection_sup,ssl_dist_sup,user,
                    ssl_tls_dist_proxy,ssl_manager_dist,sasl_safe_sup,
                    ssl_listen_tracker_sup,inet_gethost_native_sup,
                    code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,0}]
[ns_server:info,2020-04-02T20:17:28.640+05:30,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"21e0ed4ef2e27d16585f31e4458a1db0546bbb05\" upstream=\"6.5.0\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"5716bf0df2d36db8ff45c6508a328a92b9457cbf\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4966\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"74c19969e8ef1b5309077a03885d00e273378f6c\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"956a2df5f2f2abb48054bc4ce56895ce9618d2ae\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-04-02T20:17:28.643+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.643+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.644+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:17:28.644+05:30,ns_1@127.0.0.1:ns_config_sup<0.192.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-04-02T20:17:28.644+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.644+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.194.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:17:28.662+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-04-02T20:17:28.662+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:info,2020-04-02T20:17:28.663+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1125]No dynamic config file found. Assuming we're brand new node
[ns_server:debug,2020-04-02T20:17:28.665+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[]]
[ns_server:info,2020-04-02T20:17:28.667+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',cbas_debug_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|-1]},
 {{node,'ns_1@127.0.0.1',cbas_parent_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9122]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9121]},
 {{node,'ns_1@127.0.0.1',cbas_replication_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9120]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9119]},
 {{node,'ns_1@127.0.0.1',cbas_messaging_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9118]},
 {{node,'ns_1@127.0.0.1',cbas_result_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9117]},
 {{node,'ns_1@127.0.0.1',cbas_data_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9116]},
 {{node,'ns_1@127.0.0.1',cbas_cluster_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9115]},
 {{node,'ns_1@127.0.0.1',cbas_console_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9114]},
 {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9113]},
 {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9112]},
 {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9111]},
 {{node,'ns_1@127.0.0.1',cbas_admin_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9110]},
 {{node,'ns_1@127.0.0.1',cbas_ssl_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',cbas_http_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   8095]},
 {{node,'ns_1@127.0.0.1',eventing_https_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',eventing_debug_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9140]},
 {{node,'ns_1@127.0.0.1',eventing_http_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   8096]},
 {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',fts_grpc_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9130]},
 {{node,'ns_1@127.0.0.1',fts_ssl_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',fts_http_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   8094]},
 {{node,'ns_1@127.0.0.1',indexer_https_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9105]},
 {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9104]},
 {{node,'ns_1@127.0.0.1',indexer_stinit_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9103]},
 {{node,'ns_1@127.0.0.1',indexer_http_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9102]},
 {{node,'ns_1@127.0.0.1',indexer_scan_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9101]},
 {{node,'ns_1@127.0.0.1',indexer_admin_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9100]},
 {{node,'ns_1@127.0.0.1',ssl_query_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',query_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   8093]},
 {{node,'ns_1@127.0.0.1',projector_ssl_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',projector_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',ssl_capi_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',capi_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   8092]},
 {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',xdcr_rest_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   9998]},
 {{node,'ns_1@127.0.0.1',ssl_rest_port},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',rest},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]},
   {port,8091},
   {port_meta,global}]},
 {rest,[{port,8091}]},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {drop_request_memory_threshold_mib,undefined},
 {{request_limit,capi},undefined},
 {{request_limit,rest},undefined},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {replication,[{enabled,true}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {{node,'ns_1@127.0.0.1',ns_log},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@127.0.0.1',port_servers},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}]},
 {{node,'ns_1@127.0.0.1',moxi},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]},
   {port,0}]},
 {secure_headers,[]},
 {buckets,[{configs,[]}]},
 {cbas_memory_quota,2174},
 {fts_memory_quota,512},
 {memory_quota,8886},
 {{node,'ns_1@127.0.0.1',memcached_config},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@127.0.0.1',memcached},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,undefined},
   {ssl_port,undefined},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@127.0.0.1',memcached_defaults},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,false},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {memcached,[]},
 {{node,'ns_1@127.0.0.1',audit},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {{node,'ns_1@127.0.0.1',isasl},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {remote_clusters,[]},
 {rest_creds,null},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":5,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {{node,'ns_1@127.0.0.1',membership},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   active]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
 {quorum_nodes,['ns_1@127.0.0.1']},
 {nodes_wanted,['ns_1@127.0.0.1']},
 {{node,'ns_1@127.0.0.1',compaction_daemon},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {max_bucket_count,30},
 {index_aware_rebalance_disabled,false},
 {{node,'ns_1@127.0.0.1',saslauthd_enabled},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',is_enterprise},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',config_version},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   {6,5}]},
 {{node,'ns_1@127.0.0.1',uuid},
  [{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
   <<"4999225eea5ceee750c85d0413db863c">>]}]
[error_logger:info,2020-04-02T20:17:28.669+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.195.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.670+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.201.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.670+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.202.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.670+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:17:28.672+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',erl_external_listeners} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2020-04-02T20:17:28.672+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',node_encryption} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|false]
[error_logger:info,2020-04-02T20:17:28.672+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.204.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:17:28.672+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',address_family} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|inet]
[ns_server:debug,2020-04-02T20:17:28.672+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"4999225eea5ceee750c85d0413db863c">>} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}]
[error_logger:info,2020-04-02T20:17:28.673+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.207.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:28.676+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:28.677+05:30,ns_1@127.0.0.1:menelaus_barrier<0.211.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-04-02T20:17:28.677+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.677+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.678+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.213.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.679+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.215.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.679+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.214.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:28.683+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.217.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.684+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.220.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.686+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.222.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:28.687+05:30,ns_1@127.0.0.1:users_replicator<0.222.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-04-02T20:17:28.688+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.222.0>
[ns_server:debug,2020-04-02T20:17:28.688+05:30,ns_1@127.0.0.1:users_replicator<0.222.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.223.0>
[ns_server:debug,2020-04-02T20:17:28.689+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-04-02T20:17:28.689+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.223.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.689+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.221.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:17:28.693+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.225.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-04-02T20:17:28.693+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.225.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.695+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.228.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:28.695+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.219.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:28.695+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.231.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:28.695+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.232.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:17:28.703+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-04-02T20:17:28.703+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 14ms
[ns_server:debug,2020-04-02T20:17:28.704+05:30,ns_1@127.0.0.1:users_replicator<0.222.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2020-04-02T20:17:28.705+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.235.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:28.705+05:30,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.236.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[error_logger:info,2020-04-02T20:17:28.706+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:17:28.706+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T20:17:28.706+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2999594286.2850029569.256101>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T20:17:28.706+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2999594286.2850029569.256101>,
                                  inet_tcp_dist,<0.239.0>,
                                  #Ref<0.2999594286.2850029569.256105>}
[error_logger:info,2020-04-02T20:17:28.706+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.239.0>,shutdown}}
[ns_server:debug,2020-04-02T20:17:28.706+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2999594286.2850029569.256101>,
                               inet_tcp_dist,<0.239.0>,
                               #Ref<0.2999594286.2850029569.256105>}
[ns_server:debug,2020-04-02T20:17:28.706+05:30,ns_1@127.0.0.1:<0.237.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-04-02T20:17:28.706+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-04-02T20:17:28.907+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:17:28.907+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T20:17:28.907+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2999594286.2850029569.256116>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T20:17:28.907+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2999594286.2850029569.256116>,
                                  inet_tcp_dist,<0.242.0>,
                                  #Ref<0.2999594286.2850029569.256120>}
[ns_server:debug,2020-04-02T20:17:28.936+05:30,ns_1@127.0.0.1:<0.237.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-04-02T20:17:29.137+05:30,ns_1@127.0.0.1:<0.237.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-04-02T20:17:29.363+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.246.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.453+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.236.0>},
                       {name,wait_for_couchdb_node},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.457+05:30,ns_1@127.0.0.1:ns_server_nodes_sup<0.209.0>:ns_storage_conf:setup_db_and_ix_paths:64]Initialize db_and_ix_paths variable with [{db_path,
                                           "/opt/couchbase/var/lib/couchbase/data"},
                                          {index_path,
                                           "/opt/couchbase/var/lib/couchbase/data"}]
[ns_server:debug,2020-04-02T20:17:29.457+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"4999225eea5ceee750c85d0413db863c">>} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{2,63753058049}}]}]
[ns_server:debug,2020-04-02T20:17:29.457+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_dirs} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058049}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2020-04-02T20:17:29.457+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"4999225eea5ceee750c85d0413db863c">>} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{3,63753058049}}]}]
[ns_server:debug,2020-04-02T20:17:29.458+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_dir} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058049}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[error_logger:info,2020-04-02T20:17:29.459+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.250.0>},
                       {name,ns_disksup},
                       {mfargs,{ns_disksup,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.460+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.251.0>},
                       {name,diag_handler_worker},
                       {mfargs,{work_queue,start_link,[diag_handler_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:17:29.461+05:30,ns_1@127.0.0.1:ns_server_sup<0.249.0>:dir_size:start_link:39]Starting quick version of dir_size with program name: godu
[error_logger:info,2020-04-02T20:17:29.461+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.252.0>},
                       {name,dir_size},
                       {mfargs,{dir_size,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.465+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.254.0>},
                       {name,request_throttler},
                       {mfargs,{request_throttler,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.466+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.255.0>},
                       {name,ns_log},
                       {mfargs,{ns_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.466+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.256.0>},
                       {name,ns_crash_log_consumer},
                       {mfargs,{ns_log,start_link_crash_consumer,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.469+05:30,ns_1@127.0.0.1:memcached_passwords<0.257.0>:memcached_cfg:init:62]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-04-02T20:17:29.469+05:30,ns_1@127.0.0.1:memcached_passwords<0.257.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-04-02T20:17:29.496+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:302]Suspended by process <0.257.0>
[ns_server:debug,2020-04-02T20:17:29.496+05:30,ns_1@127.0.0.1:memcached_passwords<0.257.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-04-02T20:17:29.496+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:309]Released by process <0.257.0>
[ns_server:debug,2020-04-02T20:17:29.496+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[error_logger:info,2020-04-02T20:17:29.496+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.257.0>},
                       {name,memcached_passwords},
                       {mfargs,{memcached_passwords,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.498+05:30,ns_1@127.0.0.1:memcached_permissions<0.260.0>:memcached_cfg:init:62]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:warn,2020-04-02T20:17:29.503+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-04-02T20:17:29.503+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:93]Refresh of [isasl] failed. Retry in 1000 ms.
[ns_server:debug,2020-04-02T20:17:29.504+05:30,ns_1@127.0.0.1:memcached_permissions<0.260.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2020-04-02T20:17:29.504+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:302]Suspended by process <0.260.0>
[ns_server:debug,2020-04-02T20:17:29.504+05:30,ns_1@127.0.0.1:memcached_permissions<0.260.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-04-02T20:17:29.504+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:309]Released by process <0.260.0>
[ns_server:debug,2020-04-02T20:17:29.505+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[error_logger:info,2020-04-02T20:17:29.505+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.260.0>},
                       {name,memcached_permissions},
                       {mfargs,{memcached_permissions,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-04-02T20:17:29.505+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-04-02T20:17:29.505+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2020-04-02T20:17:29.505+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.263.0>},
                       {name,ns_email_alert},
                       {mfargs,{ns_email_alert,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.506+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.265.0>},
                       {id,ns_node_disco_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_node_disco_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.506+05:30,ns_1@127.0.0.1:ns_node_disco<0.266.0>:ns_node_disco:init:128]Initting ns_node_disco with []
[ns_server:debug,2020-04-02T20:17:29.506+05:30,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2020-04-02T20:17:29.507+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"4999225eea5ceee750c85d0413db863c">>} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{4,63753058049}}]}]
[user:info,2020-04-02T20:17:29.507+05:30,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_init:84]Initial otp cookie generated: {sanitized,
                                  <<"gYT9myu0YqK7C0HV8J1OeFjKsYv2qJ/toU3tD1/aPbg=">>}
[ns_server:debug,2020-04-02T20:17:29.507+05:30,ns_1@127.0.0.1:<0.267.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"gYT9myu0YqK7C0HV8J1OeFjKsYv2qJ/toU3tD1/aPbg=">>}
[ns_server:debug,2020-04-02T20:17:29.507+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
otp ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058049}}]},
 {cookie,{sanitized,<<"gYT9myu0YqK7C0HV8J1OeFjKsYv2qJ/toU3tD1/aPbg=">>}}]
[ns_server:debug,2020-04-02T20:17:29.508+05:30,ns_1@127.0.0.1:<0.267.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"gYT9myu0YqK7C0HV8J1OeFjKsYv2qJ/toU3tD1/aPbg=">>}
[error_logger:info,2020-04-02T20:17:29.508+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.266.0>},
                       {id,ns_node_disco},
                       {mfargs,{ns_node_disco,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.508+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.269.0>},
                       {id,ns_node_disco_log},
                       {mfargs,{ns_node_disco_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.509+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.270.0>},
                       {id,ns_node_disco_conf_events},
                       {mfargs,{ns_node_disco_conf_events,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.510+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:init:71]init pulling
[error_logger:info,2020-04-02T20:17:29.510+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.271.0>},
                       {id,ns_config_rep_merger},
                       {mfargs,{ns_config_rep,start_link_merger,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.510+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:init:73]init pushing
[ns_server:debug,2020-04-02T20:17:29.512+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:init:77]init reannouncing
[ns_server:debug,2020-04-02T20:17:29.512+05:30,ns_1@127.0.0.1:ns_config_events<0.193.0>:ns_node_disco_conf_events:handle_event:50]ns_node_disco_conf_events config on otp
[ns_server:debug,2020-04-02T20:17:29.512+05:30,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2020-04-02T20:17:29.512+05:30,ns_1@127.0.0.1:<0.278.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"gYT9myu0YqK7C0HV8J1OeFjKsYv2qJ/toU3tD1/aPbg=">>}
[ns_server:debug,2020-04-02T20:17:29.512+05:30,ns_1@127.0.0.1:<0.278.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"gYT9myu0YqK7C0HV8J1OeFjKsYv2qJ/toU3tD1/aPbg=">>}
[ns_server:debug,2020-04-02T20:17:29.512+05:30,ns_1@127.0.0.1:ns_config_events<0.193.0>:ns_node_disco_conf_events:handle_event:44]ns_node_disco_conf_events config on nodes_wanted
[ns_server:debug,2020-04-02T20:17:29.513+05:30,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2020-04-02T20:17:29.513+05:30,ns_1@127.0.0.1:<0.279.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"gYT9myu0YqK7C0HV8J1OeFjKsYv2qJ/toU3tD1/aPbg=">>}
[ns_server:debug,2020-04-02T20:17:29.513+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
otp ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058049}}]},
 {cookie,{sanitized,<<"gYT9myu0YqK7C0HV8J1OeFjKsYv2qJ/toU3tD1/aPbg=">>}}]
[ns_server:debug,2020-04-02T20:17:29.513+05:30,ns_1@127.0.0.1:<0.279.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"gYT9myu0YqK7C0HV8J1OeFjKsYv2qJ/toU3tD1/aPbg=">>}
[ns_server:debug,2020-04-02T20:17:29.513+05:30,ns_1@127.0.0.1:memcached_passwords<0.257.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-04-02T20:17:29.513+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_dir} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058049}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2020-04-02T20:17:29.513+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_dirs} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058049}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2020-04-02T20:17:29.513+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',erl_external_listeners} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',node_encryption} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|false]
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',address_family} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|inet]
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
alert_limits ->
[{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
audit ->
[{auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_reprovision_cfg ->
[{enabled,true},{max_nodes,1},{count,0}]
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
autocompaction ->
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
buckets ->
[[],{configs,[]}]
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
cbas_memory_quota ->
2174
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
drop_request_memory_threshold_mib ->
undefined
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
email_alerts ->
[{recipients,["root@localhost"]},
 {sender,"couchbase@localhost"},
 {enabled,false},
 {email_server,[{user,[]},
                {pass,"*****"},
                {host,"localhost"},
                {port,25},
                {encrypt,false}]},
 {alerts,[auto_failover_node,auto_failover_maximum_reached,
          auto_failover_other_nodes_down,auto_failover_cluster_too_small,
          auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
          ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
          ep_clock_cas_drift_threshold_exceeded,communication_issue]}]
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
fts_memory_quota ->
512
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
index_aware_rebalance_disabled ->
false
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
log_redaction_default_cfg ->
[{redact_level,none}]
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
max_bucket_count ->
30
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
memcached ->
[]
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
memory_quota ->
8886
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
nodes_wanted ->
['ns_1@127.0.0.1']
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
password_policy ->
[{min_length,6},{must_present,[]}]
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
quorum_nodes ->
['ns_1@127.0.0.1']
[ns_server:debug,2020-04-02T20:17:29.514+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
remote_clusters ->
[]
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
replication ->
[{enabled,true}]
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
rest ->
[{port,8091}]
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
rest_creds ->
null
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
secure_headers ->
[]
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
server_groups ->
[[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
set_view_update_daemon ->
[{update_interval,5000},
 {update_min_changes,5000},
 {replica_update_min_changes,5000}]
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_indexers} ->
4
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_replica_indexers} ->
2
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
<<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"in"...>>
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{request_limit,capi} ->
undefined
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{request_limit,rest} ->
undefined
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',audit} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}]
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',capi_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|8092]
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_admin_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9110]
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_client_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9113]
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9112]
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_http_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9111]
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cluster_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9115]
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_console_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9114]
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_data_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9116]
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_debug_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|-1]
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_http_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|8095]
[ns_server:debug,2020-04-02T20:17:29.515+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_messaging_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9118]
[ns_server:debug,2020-04-02T20:17:29.516+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9119]
[ns_server:debug,2020-04-02T20:17:29.516+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9121]
[ns_server:debug,2020-04-02T20:17:29.516+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_parent_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9122]
[ns_server:debug,2020-04-02T20:17:29.516+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_replication_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9120]
[ns_server:debug,2020-04-02T20:17:29.516+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_result_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9117]
[ns_server:debug,2020-04-02T20:17:29.516+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_ssl_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:17:29.516+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',compaction_daemon} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2020-04-02T20:17:29.516+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',config_version} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|{6,5}]
[ns_server:debug,2020-04-02T20:17:29.516+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_debug_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9140]
[ns_server:debug,2020-04-02T20:17:29.516+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_http_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|8096]
[ns_server:debug,2020-04-02T20:17:29.516+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_https_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:17:29.516+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_grpc_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9130]
[ns_server:debug,2020-04-02T20:17:29.516+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:17:29.516+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_http_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|8094]
[ns_server:debug,2020-04-02T20:17:29.516+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_ssl_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:17:29.516+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_admin_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9100]
[ns_server:debug,2020-04-02T20:17:29.516+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_http_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9102]
[ns_server:debug,2020-04-02T20:17:29.516+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_https_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:17:29.517+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_scan_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9101]
[ns_server:debug,2020-04-02T20:17:29.517+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stcatchup_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9104]
[ns_server:debug,2020-04-02T20:17:29.517+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stinit_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9103]
[ns_server:debug,2020-04-02T20:17:29.517+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stmaint_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9105]
[ns_server:debug,2020-04-02T20:17:29.517+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',is_enterprise} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|false]
[ns_server:debug,2020-04-02T20:17:29.517+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',isasl} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2020-04-02T20:17:29.517+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',membership} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
 active]
[ns_server:debug,2020-04-02T20:17:29.517+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,undefined},
 {ssl_port,undefined},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_sleeptime,19},
 {log_rotation_period,39003}]
[ns_server:debug,2020-04-02T20:17:29.518+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_config} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
 {[{interfaces,
    {memcached_config_mgr,omit_missing_mcd_ports,
     [{[{host,<<"*">>},
        {port,port},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_port},
        {system,true},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,ssl_port},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_ssl_port},
        {system,true},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
   {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
   {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {opentracing,
    {[{enabled,opentracing_enabled},
      {module,{"~s",[opentracing_module]}},
      {config,{"~s",[opentracing_config]}}]}},
   {admin,{"~s",[admin_user]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},
      {cyclesize,log_cyclesize},
      {sleeptime,log_sleeptime}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}}]}]
[ns_server:debug,2020-04-02T20:17:29.518+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:17:29.518+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_defaults} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {opentracing_enabled,false},
 {opentracing_module,[]},
 {opentracing_config,[]},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {tracing_enabled,false},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>}]
[ns_server:debug,2020-04-02T20:17:29.518+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',moxi} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]},
 {port,0}]
[ns_server:debug,2020-04-02T20:17:29.518+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ns_log} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2020-04-02T20:17:29.518+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',port_servers} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}]
[ns_server:debug,2020-04-02T20:17:29.513+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.225.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from undefined to {undefined,
                                                                             {0,
                                                                              2090473001},
                                                                             {0,
                                                                              2090473001},
                                                                             false,
                                                                             []}
[ns_server:debug,2020-04-02T20:17:29.519+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',projector_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9999]
[ns_server:debug,2020-04-02T20:17:29.519+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',projector_ssl_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:17:29.519+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',query_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|8093]
[ns_server:debug,2020-04-02T20:17:29.519+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',rest} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2020-04-02T20:17:29.519+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',saslauthd_enabled} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|true]
[ns_server:debug,2020-04-02T20:17:29.519+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_capi_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:17:29.519+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_query_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:17:29.519+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_rest_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
 undefined]
[ns_server:debug,2020-04-02T20:17:29.519+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',uuid} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|
 <<"4999225eea5ceee750c85d0413db863c">>]
[ns_server:debug,2020-04-02T20:17:29.519+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',xdcr_rest_port} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|9998]
[ns_server:debug,2020-04-02T20:17:29.519+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058048}}]}|false]
[error_logger:info,2020-04-02T20:17:29.519+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.272.0>},
                       {id,ns_config_rep},
                       {mfargs,{ns_config_rep,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.519+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"4999225eea5ceee750c85d0413db863c">>} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{4,63753058049}}]}]
[error_logger:info,2020-04-02T20:17:29.519+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.264.0>},
                       {name,ns_node_disco_sup},
                       {mfargs,{ns_node_disco_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:17:29.519+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([alert_limits,audit,auto_failover_cfg,
                               auto_reprovision_cfg,autocompaction,buckets,
                               cbas_memory_quota,
                               drop_request_memory_threshold_mib,email_alerts,
                               fts_memory_quota,
                               index_aware_rebalance_disabled,
                               log_redaction_default_cfg,max_bucket_count,
                               memcached,memory_quota,nodes_wanted,otp,
                               password_policy,quorum_nodes,remote_clusters,
                               replication,rest,rest_creds,secure_headers,
                               server_groups,set_view_update_daemon,
                               {couchdb,max_parallel_indexers},
                               {couchdb,max_parallel_replica_indexers},
                               {local_changes_count,
                                   <<"4999225eea5ceee750c85d0413db863c">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {request_limit,capi},
                               {request_limit,rest},
                               {node,'ns_1@127.0.0.1',address_family},
                               {node,'ns_1@127.0.0.1',audit},
                               {node,'ns_1@127.0.0.1',capi_port},
                               {node,'ns_1@127.0.0.1',cbas_admin_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_client_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_http_port},
                               {node,'ns_1@127.0.0.1',cbas_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_console_port},
                               {node,'ns_1@127.0.0.1',cbas_data_port},
                               {node,'ns_1@127.0.0.1',cbas_debug_port},
                               {node,'ns_1@127.0.0.1',cbas_dirs},
                               {node,'ns_1@127.0.0.1',cbas_http_port},
                               {node,'ns_1@127.0.0.1',cbas_messaging_port},
                               {node,'ns_1@127.0.0.1',
                                   cbas_metadata_callback_port},
                               {node,'ns_1@127.0.0.1',cbas_metadata_port},
                               {node,'ns_1@127.0.0.1',cbas_parent_port},
                               {node,'ns_1@127.0.0.1',cbas_replication_port},
                               {node,'ns_1@127.0.0.1',cbas_result_port},
                               {node,'ns_1@127.0.0.1',cbas_ssl_port},
                               {node,'ns_1@127.0.0.1',compaction_daemon},
                               {node,'ns_1@127.0.0.1',config_version},
                               {node,'ns_1@127.0.0.1',erl_external_listeners},
                               {node,'ns_1@127.0.0.1',eventing_debug_port},
                               {node,'ns_1@127.0.0.1',eventing_dir},
                               {node,'ns_1@127.0.0.1',eventing_http_port},
                               {node,'ns_1@127.0.0.1',eventing_https_port},
                               {node,'ns_1@127.0.0.1',fts_grpc_port},
                               {node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
                               {node,'ns_1@127.0.0.1',fts_http_port},
                               {node,'ns_1@127.0.0.1',fts_ssl_port},
                               {node,'ns_1@127.0.0.1',indexer_admin_port}]..)
[error_logger:info,2020-04-02T20:17:29.524+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.284.0>},
                       {name,vbucket_map_mirror},
                       {mfargs,{vbucket_map_mirror,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.527+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.286.0>},
                       {name,bucket_info_cache},
                       {mfargs,{bucket_info_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.527+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.289.0>},
                       {name,ns_tick_event},
                       {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.527+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.290.0>},
                       {name,buckets_events},
                       {mfargs,
                           {gen_event,start_link,[{local,buckets_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.527+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.291.0>},
                       {name,ns_stats_event},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_stats_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.529+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.292.0>},
                       {name,samples_loader_tasks},
                       {mfargs,{samples_loader_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.532+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.294.0>},
                       {id,ns_heart},
                       {mfargs,{ns_heart,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.533+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.297.0>},
                       {id,ns_heart_slow_updater},
                       {mfargs,{ns_heart,start_link_slow_updater,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.533+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.293.0>},
                       {name,ns_heart_sup},
                       {mfargs,{ns_heart_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:29.534+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.301.0>},
                       {id,ns_doctor_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_doctor_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.536+05:30,ns_1@127.0.0.1:ns_heart<0.294.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]},
                 {ns_heart,handle_info,2,
                           [{file,"src/ns_heart.erl"},{line,118}]}]}}

[ns_server:debug,2020-04-02T20:17:29.536+05:30,ns_1@127.0.0.1:ns_heart<0.294.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]}]}}

[ns_server:debug,2020-04-02T20:17:29.540+05:30,ns_1@127.0.0.1:<0.299.0>:restartable:start_child:98]Started child process <0.300.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2020-04-02T20:17:29.540+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.302.0>},
                       {id,ns_doctor},
                       {mfargs,{ns_doctor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.540+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.299.0>},
                       {name,ns_doctor_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_doctor_sup,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:29.540+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.305.0>},
                       {name,master_activity_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.543+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.307.0>},
                       {name,xdcr_ckpt_store},
                       {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.544+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.308.0>},
                       {name,metakv_worker},
                       {mfargs,{work_queue,start_link,[metakv_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.544+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.309.0>},
                       {name,index_events},
                       {mfargs,{gen_event,start_link,[{local,index_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.544+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.310.0>},
                       {name,index_settings_manager},
                       {mfargs,{index_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.546+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:302]Suspended by process <0.257.0>
[error_logger:info,2020-04-02T20:17:29.546+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.312.0>},
                       {name,query_settings_manager},
                       {mfargs,{query_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.546+05:30,ns_1@127.0.0.1:memcached_passwords<0.257.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-04-02T20:17:29.546+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:309]Released by process <0.257.0>
[ns_server:debug,2020-04-02T20:17:29.547+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:warn,2020-04-02T20:17:29.547+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-04-02T20:17:29.547+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2020-04-02T20:17:29.548+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.314.0>},
                       {name,eventing_settings_manager},
                       {mfargs,{eventing_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.548+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.316.0>},
                       {name,audit_events},
                       {mfargs,{gen_event,start_link,[{local,audit_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.551+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.319.0>},
                       {id,menelaus_ui_auth},
                       {mfargs,{menelaus_ui_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.551+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.321.0>},
                       {id,scram_sha},
                       {mfargs,{scram_sha,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.553+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.322.0>},
                       {id,menelaus_local_auth},
                       {mfargs,{menelaus_local_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.556+05:30,ns_1@127.0.0.1:ns_heart<0.294.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2020-04-02T20:17:29.556+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.323.0>},
                       {id,menelaus_web_cache},
                       {mfargs,{menelaus_web_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.557+05:30,ns_1@127.0.0.1:ns_heart<0.294.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2020-04-02T20:17:29.558+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.329.0>},
                       {id,menelaus_stats_gatherer},
                       {mfargs,{menelaus_stats_gatherer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.558+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.330.0>},
                       {id,json_rpc_events},
                       {mfargs,
                           {gen_event,start_link,[{local,json_rpc_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.563+05:30,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.297.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]},
                 {proc_lib,init_p_do_apply,3,
                           [{file,"proc_lib.erl"},{line,247}]}]}}

[ns_server:info,2020-04-02T20:17:29.563+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.235.0>:ns_port_server:log:224]ns_couchdb<0.235.0>: Apache CouchDB  (LogLevel=info) is starting.
ns_couchdb<0.235.0>: Apache CouchDB has started. Time to relax.
ns_couchdb<0.235.0>: 350: Booted. Waiting for shutdown request
ns_couchdb<0.235.0>: working as port

[ns_server:debug,2020-04-02T20:17:29.563+05:30,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.297.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]}]}}

[ns_server:debug,2020-04-02T20:17:29.563+05:30,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.297.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-04-02T20:17:29.564+05:30,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.297.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[ns_server:info,2020-04-02T20:17:29.566+05:30,ns_1@127.0.0.1:<0.334.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-04-02T20:17:29.566+05:30,ns_1@127.0.0.1:<0.334.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-04-02T20:17:29.566+05:30,ns_1@127.0.0.1:<0.334.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-04-02T20:17:29.566+05:30,ns_1@127.0.0.1:<0.334.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2020-04-02T20:17:29.577+05:30,ns_1@127.0.0.1:<0.331.0>:restartable:start_child:98]Started child process <0.334.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2020-04-02T20:17:29.577+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.334.0>,menelaus_web}
             started: [{pid,<0.354.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                           {menelaus_web,http_server,
                               [[{ip,"0.0.0.0"},{name,menelaus_web_ipv4}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.577+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.331.0>},
                       {id,menelaus_web},
                       {mfargs,
                           {restartable,start_link,
                               [{menelaus_web,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:29.578+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.371.0>},
                       {id,menelaus_event},
                       {mfargs,{menelaus_event,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.579+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.372.0>},
                       {id,hot_keys_keeper},
                       {mfargs,{hot_keys_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.580+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.373.0>},
                       {id,menelaus_web_alerts_srv},
                       {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.581+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.374.0>},
                       {id,menelaus_cbauth},
                       {mfargs,{menelaus_cbauth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[user:info,2020-04-02T20:17:29.581+05:30,ns_1@127.0.0.1:ns_server_sup<0.249.0>:menelaus_sup:start_link:48]Couchbase Server has started on web port 8091 on node 'ns_1@127.0.0.1'. Version: "6.5.0-4966-community".
[error_logger:info,2020-04-02T20:17:29.581+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.317.0>},
                       {name,menelaus},
                       {mfargs,{menelaus_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:29.582+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.380.0>},
                       {name,ns_ports_setup},
                       {mfargs,{ns_ports_setup,start,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.582+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.384.0>},
                       {id,service_agent_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_agent_children_sup},
                                service_agent_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:29.583+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.385.0>},
                       {id,service_agent_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<service_agent_sup.0.107373856>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.583+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.383.0>},
                       {name,service_agent_sup},
                       {mfargs,{service_agent_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:17:29.587+05:30,ns_1@127.0.0.1:ns_ports_setup<0.380.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,goxdcr]
[error_logger:info,2020-04-02T20:17:29.591+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.387.0>},
                       {name,ns_memcached_sockets_pool},
                       {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.594+05:30,ns_1@127.0.0.1:memcached_auth_server<0.388.0>:memcached_auth_server:reconnect:233]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2020-04-02T20:17:29.595+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.388.0>},
                       {name,memcached_auth_server},
                       {mfargs,{memcached_auth_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.595+05:30,ns_1@127.0.0.1:ns_audit_cfg<0.390.0>:ns_audit_cfg:write_audit_json:259]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      1},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {disabled,
                                                                                      []},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2020-04-02T20:17:29.598+05:30,ns_1@127.0.0.1:ns_audit_cfg<0.390.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[error_logger:info,2020-04-02T20:17:29.598+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.390.0>},
                       {name,ns_audit_cfg},
                       {mfargs,{ns_audit_cfg,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-04-02T20:17:29.598+05:30,ns_1@127.0.0.1:<0.394.0>:ns_memcached:connect:1104]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:debug,2020-04-02T20:17:29.602+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:init:49]waiting for completion of initial ns_ports_setup round
[error_logger:info,2020-04-02T20:17:29.602+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.395.0>},
                       {name,ns_audit},
                       {mfargs,{ns_audit,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.602+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.396.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:17:29.608+05:30,ns_1@127.0.0.1:<0.397.0>:ns_memcached_log_rotator:init:42]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2020-04-02T20:17:29.608+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.397.0>},
                       {name,ns_memcached_log_rotator},
                       {mfargs,{ns_memcached_log_rotator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.608+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.398.0>},
                       {name,testconditions_store},
                       {mfargs,{simple_store,start_link,[testconditions]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.609+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.399.0>},
                       {name,terse_cluster_info_uploader},
                       {mfargs,{terse_cluster_info_uploader,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.613+05:30,ns_1@127.0.0.1:terse_cluster_info_uploader<0.399.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":4,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"kv\":11210,\"capi\":8092,\"projector\":9999},\"thisNode\":true}]}">>
[error_logger:info,2020-04-02T20:17:29.614+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.404.0>},
                       {id,ns_bucket_sup},
                       {mfargs,{ns_bucket_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:warn,2020-04-02T20:17:29.614+05:30,ns_1@127.0.0.1:<0.403.0>:ns_memcached:connect:1104]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[error_logger:info,2020-04-02T20:17:29.615+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.405.0>},
                       {id,ns_bucket_worker},
                       {mfargs,{ns_bucket_worker,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.616+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.401.0>},
                       {name,ns_bucket_worker_sup},
                       {mfargs,{ns_bucket_worker_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:29.617+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.407.0>},
                       {name,system_stats_collector},
                       {mfargs,{system_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.617+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.411.0>},
                       {name,{stats_archiver,"@system"}},
                       {mfargs,{stats_archiver,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.619+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.413.0>},
                       {name,{stats_reader,"@system"}},
                       {mfargs,{stats_reader,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.620+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.414.0>},
                       {name,{stats_archiver,"@system-processes"}},
                       {mfargs,
                           {stats_archiver,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.620+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.416.0>},
                       {name,{stats_reader,"@system-processes"}},
                       {mfargs,
                           {stats_reader,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.620+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.417.0>},
                       {name,{stats_archiver,"@query"}},
                       {mfargs,{stats_archiver,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.620+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.419.0>},
                       {name,{stats_reader,"@query"}},
                       {mfargs,{stats_reader,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.622+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.420.0>},
                       {name,query_stats_collector},
                       {mfargs,{query_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.623+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.422.0>},
                       {name,{stats_archiver,"@global"}},
                       {mfargs,{stats_archiver,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.623+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.424.0>},
                       {name,{stats_reader,"@global"}},
                       {mfargs,{stats_reader,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.625+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.425.0>},
                       {name,global_stats_collector},
                       {mfargs,{global_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.628+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.427.0>},
                       {name,goxdcr_status_keeper},
                       {mfargs,{goxdcr_status_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.629+05:30,ns_1@127.0.0.1:goxdcr_status_keeper<0.427.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-04-02T20:17:29.630+05:30,ns_1@127.0.0.1:goxdcr_status_keeper<0.427.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2020-04-02T20:17:29.630+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.431.0>},
                       {id,service_stats_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_stats_children_sup},
                                services_stats_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:29.632+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.433.0>},
                       {id,service_status_keeper_worker},
                       {mfargs,
                           {work_queue,start_link,
                               [service_status_keeper_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.636+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.434.0>},
                       {id,service_status_keeper_index},
                       {mfargs,{service_index,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.637+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.437.0>},
                       {id,service_status_keeper_fts},
                       {mfargs,{service_fts,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.638+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.440.0>},
                       {id,service_status_keeper_eventing},
                       {mfargs,{service_eventing,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.639+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.432.0>},
                       {id,service_status_keeper_sup},
                       {mfargs,{service_status_keeper_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:29.639+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.443.0>},
                       {id,service_stats_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<services_stats_sup.0.108537742>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.639+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.430.0>},
                       {name,services_stats_sup},
                       {mfargs,{services_stats_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:17:29.644+05:30,ns_1@127.0.0.1:<0.447.0>:new_concurrency_throttle:init:115]init concurrent throttle process, pid: <0.447.0>, type: kv_throttle# of available token: 1
[ns_server:debug,2020-04-02T20:17:29.645+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:17:29.645+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:17:29.646+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:17:29.646+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[error_logger:info,2020-04-02T20:17:29.646+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.445.0>},
                       {name,compaction_daemon},
                       {mfargs,{compaction_daemon,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.646+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:17:29.646+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2020-04-02T20:17:29.647+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,cluster_logs_sup}
             started: [{pid,<0.449.0>},
                       {id,ets_holder},
                       {mfargs,
                           {cluster_logs_collection_task,
                               start_link_ets_holder,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.647+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.448.0>},
                       {name,cluster_logs_sup},
                       {mfargs,{cluster_logs_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:29.647+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.450.0>},
                       {name,leader_events},
                       {mfargs,{gen_event,start_link,[{local,leader_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.648+05:30,ns_1@127.0.0.1:ns_ports_setup<0.380.0>:ns_ports_setup:set_children:85]Monitor ns_child_ports_sup <12938.107.0>
[ns_server:debug,2020-04-02T20:17:29.648+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:init:51]ns_ports_setup seems to be ready
[ns_server:debug,2020-04-02T20:17:29.656+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:find_port_pid_loop:137]Found memcached port <12938.114.0>
[error_logger:info,2020-04-02T20:17:29.660+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.457.0>},
                       {id,leader_activities},
                       {mfargs,{leader_activities,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.662+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.459.0>},
                       {id,leader_lease_agent},
                       {mfargs,{leader_lease_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.663+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.455.0>},
                       {id,leader_leases_sup},
                       {mfargs,{leader_leases_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:29.665+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.461.0>},
                       {id,leader_registry_server},
                       {mfargs,{leader_registry_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.668+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:init:82]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[ns_server:debug,2020-04-02T20:17:29.668+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.396.0>:memcached_config_mgr:init:86]activated memcached port server
[ns_server:debug,2020-04-02T20:17:29.669+05:30,ns_1@127.0.0.1:leader_registry_sup<0.460.0>:mb_master:check_master_takeover_needed:283]Sending master node question to the following nodes: []
[ns_server:debug,2020-04-02T20:17:29.669+05:30,ns_1@127.0.0.1:leader_registry_sup<0.460.0>:mb_master:check_master_takeover_needed:285]Got replies: []
[ns_server:debug,2020-04-02T20:17:29.669+05:30,ns_1@127.0.0.1:leader_registry_sup<0.460.0>:mb_master:check_master_takeover_needed:291]Was unable to discover master, not going to force mastership takeover
[user:info,2020-04-02T20:17:29.672+05:30,ns_1@127.0.0.1:mb_master<0.464.0>:mb_master:init:103]I'm the only node, so I'm the master.
[ns_server:debug,2020-04-02T20:17:29.672+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_new_leader:241]New leader is 'ns_1@127.0.0.1'. Invalidating name cache.
[ns_server:debug,2020-04-02T20:17:29.676+05:30,ns_1@127.0.0.1:mb_master<0.464.0>:master_activity_events:submit_cast:82]Failed to send master activity event: {error,badarg}
[ns_server:debug,2020-04-02T20:17:29.677+05:30,ns_1@127.0.0.1:leader_lease_acquirer<0.467.0>:leader_utils:wait_cluster_is_55:54]Delaying start since cluster is not fully upgraded to 5.5 yet.
[error_logger:info,2020-04-02T20:17:29.677+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.467.0>},
                       {id,leader_lease_acquirer},
                       {mfargs,{leader_lease_acquirer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.678+05:30,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.469.0>:leader_utils:wait_cluster_is_55:54]Delaying start since cluster is not fully upgraded to 5.5 yet.
[error_logger:info,2020-04-02T20:17:29.678+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.469.0>},
                       {id,leader_quorum_nodes_manager},
                       {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:17:29.680+05:30,ns_1@127.0.0.1:mb_master_sup<0.466.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.471.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-04-02T20:17:29.680+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.471.0>},
                       {id,ns_tick},
                       {mfargs,{ns_tick,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.681+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.473.0>},
                       {id,compat_mode_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,compat_mode_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.682+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[5,0]}]

[ns_server:info,2020-04-02T20:17:29.683+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [5,1]
[ns_server:debug,2020-04-02T20:17:29.683+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[5,1]},
 {set,client_cert_auth,[{state,"disable"},{prefixes,[]}]},
 {set,buckets,[{configs,[]}]}]

[ns_server:info,2020-04-02T20:17:29.684+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [5,5]
[ns_server:debug,2020-04-02T20:17:29.684+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[5,5]},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]}]},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120}">>},
 {set,{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
 {set,buckets,[{configs,[]}]},
 {delete,{rbac_upgrade,[5,5]}},
 {set,audit,
      [{enabled,[]},
       {disabled_users,[]},
       {auditd_enabled,false},
       {rotate_interval,86400},
       {rotate_size,20971520},
       {disabled,[]},
       {sync,[]},
       {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {set,quorum_nodes,['ns_1@127.0.0.1']},
 {set,scramsha_fallback_salt,
      <<100,134,98,203,108,229,204,57,183,44,141,153>>}]

[ns_server:info,2020-04-02T20:17:29.684+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [6,0]
[ns_server:debug,2020-04-02T20:17:29.684+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[6,0]}]

[ns_server:info,2020-04-02T20:17:29.687+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [6,5]
[ns_server:debug,2020-04-02T20:17:29.689+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[6,5]},
 {set,audit_decriptors,
      [{8243,
        [{name,<<"mutate document">>},
         {description,<<"Document was mutated via the REST API">>},
         {enabled,true},
         {module,ns_server}]},
       {8255,
        [{name,<<"read document">>},
         {description,<<"Document was read via the REST API">>},
         {enabled,false},
         {module,ns_server}]},
       {8257,
        [{name,<<"alert email sent">>},
         {description,<<"An alert email was successfully sent">>},
         {enabled,true},
         {module,ns_server}]},
       {20480,
        [{name,<<"opened DCP connection">>},
         {description,<<"opened DCP connection">>},
         {enabled,true},
         {module,memcached}]},
       {20482,
        [{name,<<"external memcached bucket flush">>},
         {description,<<"External user flushed the content of a memcached bucket">>},
         {enabled,true},
         {module,memcached}]},
       {20483,
        [{name,<<"invalid packet">>},
         {description,<<"Rejected an invalid packet">>},
         {enabled,true},
         {module,memcached}]},
       {20485,
        [{name,<<"authentication succeeded">>},
         {description,<<"Authentication to the cluster succeeded">>},
         {enabled,false},
         {module,memcached}]},
       {20488,
        [{name,<<"document read">>},
         {description,<<"Document was read">>},
         {enabled,false},
         {module,memcached}]},
       {20489,
        [{name,<<"document locked">>},
         {description,<<"Document was locked">>},
         {enabled,false},
         {module,memcached}]},
       {20490,
        [{name,<<"document modify">>},
         {description,<<"Document was modified">>},
         {enabled,false},
         {module,memcached}]},
       {20491,
        [{name,<<"document delete">>},
         {description,<<"Document was deleted">>},
         {enabled,false},
         {module,memcached}]},
       {20492,
        [{name,<<"select bucket">>},
         {description,<<"The specified bucket was selected">>},
         {enabled,true},
         {module,memcached}]},
       {28672,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28673,
        [{name,<<"EXPLAIN statement">>},
         {description,<<"A N1QL EXPLAIN statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28674,
        [{name,<<"PREPARE statement">>},
         {description,<<"A N1QL PREPARE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28675,
        [{name,<<"INFER statement">>},
         {description,<<"A N1QL INFER statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28676,
        [{name,<<"INSERT statement">>},
         {description,<<"A N1QL INSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28677,
        [{name,<<"UPSERT statement">>},
         {description,<<"A N1QL UPSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28678,
        [{name,<<"DELETE statement">>},
         {description,<<"A N1QL DELETE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28679,
        [{name,<<"UPDATE statement">>},
         {description,<<"A N1QL UPDATE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28680,
        [{name,<<"MERGE statement">>},
         {description,<<"A N1QL MERGE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28681,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28682,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28683,
        [{name,<<"ALTER INDEX statement">>},
         {description,<<"A N1QL ALTER INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28684,
        [{name,<<"BUILD INDEX statement">>},
         {description,<<"A N1QL BUILD INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28685,
        [{name,<<"GRANT ROLE statement">>},
         {description,<<"A N1QL GRANT ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28686,
        [{name,<<"REVOKE ROLE statement">>},
         {description,<<"A N1QL REVOKE ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28687,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An unrecognized statement was received by the N1QL query engine">>},
         {enabled,false},
         {module,n1ql}]},
       {28688,
        [{name,<<"CREATE PRIMARY INDEX statement">>},
         {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28689,
        [{name,<<"/admin/stats API request">>},
         {description,<<"An HTTP request was made to the API at /admin/stats.">>},
         {enabled,false},
         {module,n1ql}]},
       {28690,
        [{name,<<"/admin/vitals API request">>},
         {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
         {enabled,false},
         {module,n1ql}]},
       {28691,
        [{name,<<"/admin/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28692,
        [{name,<<"/admin/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28693,
        [{name,<<"/admin/indexes/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28694,
        [{name,<<"/admin/indexes/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28695,
        [{name,<<"/admin/indexes/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28697,
        [{name,<<"/admin/ping API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ping.">>},
         {enabled,false},
         {module,n1ql}]},
       {28698,
        [{name,<<"/admin/config API request">>},
         {description,<<"An HTTP request was made to the API at /admin/config.">>},
         {enabled,false},
         {module,n1ql}]},
       {28699,
        [{name,<<"/admin/ssl_cert API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
         {enabled,false},
         {module,n1ql}]},
       {28700,
        [{name,<<"/admin/settings API request">>},
         {description,<<"An HTTP request was made to the API at /admin/settings.">>},
         {enabled,false},
         {module,n1ql}]},
       {28701,
        [{name,<<"/admin/clusters API request">>},
         {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
         {enabled,false},
         {module,n1ql}]},
       {28702,
        [{name,<<"/admin/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28704,
        [{name,<<"/admin/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28705,
        [{name,<<"/admin/indexes/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {40960,
        [{name,<<"Create Design Doc">>},
         {description,<<"Design Doc is Created">>},
         {enabled,true},
         {module,view_engine}]},
       {40961,
        [{name,<<"Delete Design Doc">>},
         {description,<<"Design Doc is Deleted">>},
         {enabled,true},
         {module,view_engine}]},
       {40962,
        [{name,<<"Query DDoc Meta Data">>},
         {description,<<"Design Doc Meta Data Query Request">>},
         {enabled,true},
         {module,view_engine}]},
       {40963,
        [{name,<<"View Query">>},
         {description,<<"View Query Request">>},
         {enabled,false},
         {module,view_engine}]},
       {40964,
        [{name,<<"Update Design Doc">>},
         {description,<<"Design Doc is Updated">>},
         {enabled,true},
         {module,view_engine}]}]},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]},
       {can_abort_rebalance,false}]},
 {set,max_bucket_count,30},
 {set,retry_rebalance,
      [{enabled,false},{after_time_period,300},{max_attempts,1}]},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_size\":5120}">>}]

[ns_server:debug,2020-04-02T20:17:29.691+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([audit,audit_decriptors,auto_failover_cfg,
                               buckets,client_cert_auth,
                               cluster_compat_version,max_bucket_count,
                               quorum_nodes,retry_rebalance,
                               scramsha_fallback_salt,
                               {local_changes_count,
                                   <<"4999225eea5ceee750c85d0413db863c">>},
                               {metakv,<<"/eventing/settings/config">>},
                               {metakv,<<"/query/settings/config">>}]..)
[ns_server:debug,2020-04-02T20:17:29.691+05:30,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.469.0>:leader_utils:wait_cluster_is_55_loop:78]Cluster upgraded to 5.5. Starting.
[ns_server:debug,2020-04-02T20:17:29.691+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
retry_rebalance ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058049}}]},
 {enabled,false},
 {after_time_period,300},
 {max_attempts,1}]
[ns_server:debug,2020-04-02T20:17:29.691+05:30,ns_1@127.0.0.1:leader_lease_acquirer<0.467.0>:leader_utils:wait_cluster_is_55_loop:78]Cluster upgraded to 5.5. Starting.
[ns_server:debug,2020-04-02T20:17:29.691+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.225.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from {undefined,
                                                                {0,2090473001},
                                                                {0,2090473001},
                                                                false,[]} to {[6,
                                                                               5],
                                                                              {0,
                                                                               2090473001},
                                                                              {0,
                                                                               2090473001},
                                                                              false,
                                                                              []}
[ns_server:debug,2020-04-02T20:17:29.692+05:30,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.469.0>:leader_quorum_nodes_manager:pull_config:114]Attempting to pull config from nodes:
[]
[ns_server:debug,2020-04-02T20:17:29.693+05:30,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.469.0>:leader_quorum_nodes_manager:pull_config:119]Pulled config successfully.
[ns_server:debug,2020-04-02T20:17:29.693+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:handle_call:122]Got full synchronization request from 'ns_1@127.0.0.1'
[ns_server:debug,2020-04-02T20:17:29.693+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:handle_call:128]Fully synchronized config in 7 us
[user:warn,2020-04-02T20:17:29.693+05:30,ns_1@127.0.0.1:compat_mode_manager<0.474.0>:compat_mode_manager:handle_consider_switching_compat_mode:49]Changed cluster compat mode from undefined to [6,5]
[error_logger:info,2020-04-02T20:17:29.694+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.474.0>},
                       {id,compat_mode_manager},
                       {mfargs,{compat_mode_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.694+05:30,ns_1@127.0.0.1:memcached_permissions<0.260.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2020-04-02T20:17:29.694+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:302]Suspended by process <0.260.0>
[ns_server:debug,2020-04-02T20:17:29.694+05:30,ns_1@127.0.0.1:memcached_permissions<0.260.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-04-02T20:17:29.695+05:30,ns_1@127.0.0.1:users_storage<0.223.0>:replicated_dets:handle_call:309]Released by process <0.260.0>
[ns_server:warn,2020-04-02T20:17:29.695+05:30,ns_1@127.0.0.1:<0.483.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2020-04-02T20:17:29.695+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]** Generic server <0.396.0> terminating 
** Last message in was do_check
** When Server state == {state,<12938.114.0>,
                               <<"{\n  \"active_external_users_push_interval\": 600,\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"collections_enabled\": false,\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"external_auth_service\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"system\": true,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"max_connections\": 65000,\n  \"num_reader_threads\": \"default\",\n  \"num_writer_threads\": \"default\",\n  \"opentracing\": {\n    \"enabled\": false,\n    \"module\": \"\",\n    \"config\": \"\"\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": {\n    \"tls 1.2\": \"HIGH\",\n    \"tls 1.3\": \"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256\"\n  },\n  \"ssl_cipher_order\": true,\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"system_connections\": 5000,\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": true\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1382}]},
     {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,197}]}]}

[error_logger:error,2020-04-02T20:17:29.696+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]Error in process <0.483.0> on node 'ns_1@127.0.0.1' with exit value:
{{badmatch,{error,couldnt_connect_to_memcached}},
 [{ns_memcached,'-config_validate/1-fun-0-',1,
                [{file,"src/ns_memcached.erl"},{line,1382}]},
  {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,197}]}]}

[ns_server:debug,2020-04-02T20:17:29.696+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2020-04-02T20:17:29.696+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
audit_decriptors ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058049}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {40960,
  [{name,<<"Create Design Doc">>},
   {description,<<"Design Doc is Created">>},
   {enabled,true},
   {module,view_engine}]},
 {40961,
  [{name,<<"Delete Design Doc">>},
   {description,<<"Design Doc is Deleted">>},
   {enabled,true},
   {module,view_engine}]},
 {40962,
  [{name,<<"Query DDoc Meta Data">>},
   {description,<<"Design Doc Meta Data Query Request">>},
   {enabled,true},
   {module,view_engine}]},
 {40963,
  [{name,<<"View Query">>},
   {description,<<"View Query Request">>},
   {enabled,false},
   {module,view_engine}]},
 {40964,
  [{name,<<"Update Design Doc">>},
   {description,<<"Design Doc is Updated">>},
   {enabled,true},
   {module,view_engine}]}]
[ns_server:debug,2020-04-02T20:17:29.696+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
scramsha_fallback_salt ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058049}}]}|
 <<100,134,98,203,108,229,204,57,183,44,141,153>>]
[ns_server:debug,2020-04-02T20:17:29.696+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/eventing/settings/config">>} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058049}}]}|
 <<"{\"ram_quota\":256}">>]
[ns_server:debug,2020-04-02T20:17:29.697+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{2,63753058049}}]}|
 <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_si"...>>]
[ns_server:debug,2020-04-02T20:17:29.697+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
client_cert_auth ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058049}}]},
 {state,"disable"},
 {prefixes,[]}]
[ns_server:warn,2020-04-02T20:17:29.697+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-04-02T20:17:29.697+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
cluster_compat_version ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{5,63753058049}}]},6,5]
[ns_server:debug,2020-04-02T20:17:29.697+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
audit ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058049}}]},
 {enabled,[]},
 {disabled_users,[]},
 {auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2020-04-02T20:17:29.697+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2020-04-02T20:17:29.697+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{2,63753058049}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,false}]
[ns_server:debug,2020-04-02T20:17:29.697+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
buckets ->
[[{<<"4999225eea5ceee750c85d0413db863c">>,{2,63753058049}}],{configs,[]}]
[ns_server:debug,2020-04-02T20:17:29.697+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
max_bucket_count ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058049}}]}|30]
[ns_server:debug,2020-04-02T20:17:29.697+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
quorum_nodes ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{1,63753058049}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-04-02T20:17:29.697+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"4999225eea5ceee750c85d0413db863c">>} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{5,63753058049}}]}]
[ns_server:debug,2020-04-02T20:17:29.706+05:30,ns_1@127.0.0.1:<0.456.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.396.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1382}]},
                                               {async,'-async_init/4-fun-1-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,197}]}]}. Exiting
[ns_server:debug,2020-04-02T20:17:29.706+05:30,ns_1@127.0.0.1:<0.458.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.396.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1382}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-1-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    197}]}]}
[ns_server:debug,2020-04-02T20:17:29.707+05:30,ns_1@127.0.0.1:leader_lease_agent<0.459.0>:leader_lease_agent:do_handle_acquire_lease:149]Granting lease to {lease_holder,<<"40880c332dfc1c595d9a156ef02d872b">>,
                                'ns_1@127.0.0.1'} for 15000ms
[error_logger:error,2020-04-02T20:17:29.710+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.396.0>
    registered_name: memcached_config_mgr
    exception error: no match of right hand side value {error,
                                                        couldnt_connect_to_memcached}
      in function  ns_memcached:'-config_validate/1-fun-0-'/1 (src/ns_memcached.erl, line 1382)
      in call from async:'-async_init/4-fun-1-'/3 (src/async.erl, line 197)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.208.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
    message_queue_len: 2
    messages: [do_check,do_check]
    links: [<0.249.0>,<0.458.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 6772
    stack_size: 27
    reductions: 79700
  neighbours:

[error_logger:info,2020-04-02T20:17:29.710+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.497.0>},
                       {id,ns_janitor_server},
                       {mfargs,{ns_janitor_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:17:29.710+05:30,ns_1@127.0.0.1:<0.493.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:302]Acquired lease from node 'ns_1@127.0.0.1' (lease uuid: <<"40880c332dfc1c595d9a156ef02d872b">>)
[ns_server:info,2020-04-02T20:17:29.710+05:30,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.494.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.498.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-04-02T20:17:29.711+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.498.0>},
                       {id,auto_reprovision},
                       {mfargs,{auto_reprovision,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T20:17:29.712+05:30,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.494.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.499.0> on 'ns_1@127.0.0.1'

[ns_server:info,2020-04-02T20:17:29.712+05:30,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.494.0>:misc:start_singleton:857]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.500.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-04-02T20:17:29.712+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.499.0>},
                       {id,auto_rebalance},
                       {mfargs,{auto_rebalance,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.712+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.500.0>},
                       {id,ns_orchestrator},
                       {mfargs,{ns_orchestrator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.712+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.494.0>},
                       {id,ns_orchestrator_child_sup},
                       {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:17:29.714+05:30,ns_1@127.0.0.1:<0.502.0>:auto_failover:init:185]init auto_failover.
[user:info,2020-04-02T20:17:29.714+05:30,ns_1@127.0.0.1:<0.502.0>:auto_failover:handle_call:216]Enabled auto-failover with timeout 120 and max count 1
[ns_server:info,2020-04-02T20:17:29.716+05:30,ns_1@127.0.0.1:ns_orchestrator_sup<0.472.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.502.0> on 'ns_1@127.0.0.1'

[ns_server:debug,2020-04-02T20:17:29.716+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"4999225eea5ceee750c85d0413db863c">>} ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{6,63753058049}}]}]
[ns_server:debug,2020-04-02T20:17:29.716+05:30,ns_1@127.0.0.1:ns_config_rep<0.272.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([auto_failover_cfg,
                               {local_changes_count,
                                   <<"4999225eea5ceee750c85d0413db863c">>}]..)
[ns_server:info,2020-04-02T20:17:29.716+05:30,ns_1@127.0.0.1:mb_master_sup<0.466.0>:misc:start_singleton:857]start_singleton(work_queue, start_link, [{via,leader_registry,collections}]): started as <0.505.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-04-02T20:17:29.716+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.502.0>},
                       {id,auto_failover},
                       {mfargs,{auto_failover,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.716+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"4999225eea5ceee750c85d0413db863c">>,{2,63753058049}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,false}]
[ns_server:debug,2020-04-02T20:17:29.716+05:30,ns_1@127.0.0.1:<0.451.0>:restartable:start_child:98]Started child process <0.453.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2020-04-02T20:17:29.716+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.472.0>},
                       {id,ns_orchestrator_sup},
                       {mfargs,{ns_orchestrator_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:29.716+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.505.0>},
                       {id,collections},
                       {mfargs,{collections,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.716+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.464.0>},
                       {id,mb_master},
                       {mfargs,{mb_master,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:29.716+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.460.0>},
                       {id,leader_registry_sup},
                       {mfargs,{leader_registry_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:29.716+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.451.0>},
                       {name,leader_services_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{leader_services_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:29.718+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.510.0>},
                       {name,ns_tick_agent},
                       {mfargs,{ns_tick_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.718+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.512.0>},
                       {name,master_activity_events_ingress},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events_ingress}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.718+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.513.0>},
                       {name,master_activity_events_timestamper},
                       {mfargs,
                           {master_activity_events,start_link_timestamper,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.719+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.514.0>},
                       {name,master_activity_events_pids_watcher},
                       {mfargs,
                           {master_activity_events_pids_watcher,start_link,
                               []}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.720+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.515.0>},
                       {name,master_activity_events_keeper},
                       {mfargs,{master_activity_events_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.722+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.518.0>},
                       {id,ns_server_monitor},
                       {mfargs,{ns_server_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.722+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.520.0>},
                       {id,service_monitor_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_monitor_children_sup},
                                health_monitor_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:29.722+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.521.0>},
                       {id,service_monitor_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<health_monitor_sup.0.112499759>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.725+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.527.0>},
                       {id,node_monitor},
                       {mfargs,{node_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.726+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.533.0>},
                       {id,node_status_analyzer},
                       {mfargs,{node_status_analyzer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.726+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.517.0>},
                       {name,health_monitor_sup},
                       {mfargs,{health_monitor_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:29.727+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.535.0>},
                       {name,rebalance_agent},
                       {mfargs,{rebalance_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.729+05:30,ns_1@127.0.0.1:ns_server_nodes_sup<0.209.0>:one_shot_barrier:notify:27]Notifying on barrier menelaus_barrier
[ns_server:debug,2020-04-02T20:17:29.729+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.538.0>:memcached_config_mgr:init:49]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2020-04-02T20:17:29.729+05:30,ns_1@127.0.0.1:menelaus_barrier<0.211.0>:one_shot_barrier:barrier_body:62]Barrier menelaus_barrier got notification from <0.209.0>
[error_logger:info,2020-04-02T20:17:29.729+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.536.0>},
                       {name,ns_rebalance_report_manager},
                       {mfargs,{ns_rebalance_report_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:17:29.729+05:30,ns_1@127.0.0.1:ns_server_nodes_sup<0.209.0>:one_shot_barrier:notify:32]Successfuly notified on barrier menelaus_barrier
[ns_server:debug,2020-04-02T20:17:29.729+05:30,ns_1@127.0.0.1:<0.208.0>:restartable:start_child:98]Started child process <0.209.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2020-04-02T20:17:29.729+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.249.0>},
                       {name,ns_server_sup},
                       {mfargs,{ns_server_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:error,2020-04-02T20:17:29.729+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1382}]},
                   {async,'-async_init/4-fun-1-',3,
                          [{file,"src/async.erl"},{line,197}]}]}
     Offender:   [{pid,<0.396.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2020-04-02T20:17:29.729+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.538.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.729+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.208.0>},
                       {id,ns_server_nodes_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_server_nodes_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T20:17:29.731+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.225.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@",admin}
[ns_server:debug,2020-04-02T20:17:29.731+05:30,ns_1@127.0.0.1:<0.5.0>:child_erlang:child_loop:130]32735: Entered child_loop
[error_logger:info,2020-04-02T20:17:29.731+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.539.0>},
                       {id,remote_api},
                       {mfargs,{remote_api,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T20:17:29.732+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.187.0>},
                       {id,ns_server_cluster_sup},
                       {mfargs,{ns_server_cluster_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T20:17:29.732+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
         application: ns_server
          started_at: 'ns_1@127.0.0.1'

[ns_server:debug,2020-04-02T20:17:29.737+05:30,ns_1@127.0.0.1:json_rpc_connection-saslauthd-saslauthd-port<0.540.0>:json_rpc_connection:init:73]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.540.0>
[ns_server:debug,2020-04-02T20:17:29.737+05:30,ns_1@127.0.0.1:json_rpc_connection-goxdcr-cbauth<0.541.0>:json_rpc_connection:init:73]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.541.0>
[ns_server:debug,2020-04-02T20:17:29.737+05:30,ns_1@127.0.0.1:menelaus_cbauth<0.374.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"goxdcr-cbauth",<0.541.0>} started
[ns_server:debug,2020-04-02T20:17:29.738+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.225.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:debug,2020-04-02T20:17:29.741+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.538.0>:memcached_config_mgr:init:51]ns_ports_setup seems to be ready
[ns_server:debug,2020-04-02T20:17:29.741+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.538.0>:memcached_config_mgr:find_port_pid_loop:137]Found memcached port <12938.114.0>
[ns_server:debug,2020-04-02T20:17:29.743+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.538.0>:memcached_config_mgr:do_read_current_memcached_config:287]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2020-04-02T20:17:29.743+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.538.0>:memcached_config_mgr:init:89]found memcached port to be already active
[ns_server:debug,2020-04-02T20:17:29.749+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.538.0>:memcached_config_mgr:apply_changed_memcached_config:179]New memcached config is hot-reloadable.
[ns_server:debug,2020-04-02T20:17:29.750+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.538.0>:memcached_config_mgr:do_read_current_memcached_config:287]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[user:info,2020-04-02T20:17:29.756+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.538.0>:memcached_config_mgr:hot_reload_config:248]Hot-reloaded memcached.json for config change of the following keys: [<<"client_cert_auth">>,
                                                                      <<"datatype_snappy">>,
                                                                      <<"scramsha_fallback_salt">>]
[ns_server:debug,2020-04-02T20:17:30.507+05:30,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:89]Refresh of [rbac,isasl] succeeded
[ns_server:debug,2020-04-02T20:17:30.618+05:30,ns_1@127.0.0.1:terse_cluster_info_uploader<0.399.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":6,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"kv\":11210,\"capi\":8092,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-04-02T20:17:30.618+05:30,ns_1@127.0.0.1:ns_audit_cfg<0.390.0>:ns_audit_cfg:write_audit_json:259]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "48537283"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2020-04-02T20:17:30.625+05:30,ns_1@127.0.0.1:ns_audit_cfg<0.390.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[ns_server:debug,2020-04-02T20:17:30.716+05:30,ns_1@127.0.0.1:<0.502.0>:auto_failover_logic:log_master_activity:177]Transitioned node {'ns_1@127.0.0.1',<<"4999225eea5ceee750c85d0413db863c">>} state new -> up
[ns_server:debug,2020-04-02T20:17:59.646+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:17:59.646+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:17:59.646+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:17:59.647+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:18:29.647+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:18:29.647+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:18:29.648+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-04-02T20:18:29.648+05:30,ns_1@127.0.0.1:compaction_daemon<0.445.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-04-02T20:18:42.604+05:30,ns_1@127.0.0.1:json_rpc_connection-goxdcr-cbauth<0.541.0>:json_rpc_connection:handle_info:129]Socket closed
[ns_server:debug,2020-04-02T20:18:42.604+05:30,ns_1@127.0.0.1:menelaus_cbauth<0.374.0>:menelaus_cbauth:handle_info:142]Observed json rpc process {"goxdcr-cbauth",<0.541.0>} died with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.604+05:30,ns_1@127.0.0.1:<0.573.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.360.0>} exited with reason normal
[ns_server:debug,2020-04-02T20:18:42.604+05:30,ns_1@127.0.0.1:<0.551.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.359.0>} exited with reason normal
[ns_server:debug,2020-04-02T20:18:42.604+05:30,ns_1@127.0.0.1:<0.610.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.370.0>} exited with reason normal
[ns_server:debug,2020-04-02T20:18:42.604+05:30,ns_1@127.0.0.1:<0.593.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.572.0>} exited with reason normal
[user:debug,2020-04-02T20:18:42.605+05:30,ns_1@127.0.0.1:<0.256.0>:ns_log:crash_consumption_loop:69]Service 'goxdcr' exited with status 0. Restarting. Messages:
2020-04-02T20:18:29.753+05:30 INFO GOXDCR.ResourceMgr: Resource Manager State = overallTP: 0 highTP: 0 highExist: false lowExist: false backlogExist: false maxTP: 0 highTPNeeded: 0 highTokens: 0 maxTokens: 0 lowTPLimit: 0 calibration: None dcpAction: Reset processCpu: 0 idleCpu: 90
2020-04-02T20:18:29.753+05:30 INFO GOXDCR.ResourceMgr: backlogCount=0, noBacklogCount=60 extraQuota=false cpuNotMaxedCount=0 throughputDropCount=0
2020-04-02T20:18:29.753+05:30 INFO GOXDCR.ResourceMgr: DcpPriorityMap=map[]
ongoingReplMap=map[]
2020-04-02T20:18:39.752+05:30 INFO GOXDCR.ResourceMgr: Resource Manager State = overallTP: 0 highTP: 0 highExist: false lowExist: false backlogExist: false maxTP: 0 highTPNeeded: 0 highTokens: 0 maxTokens: 0 lowTPLimit: 0 calibration: None dcpAction: Reset processCpu: 0 idleCpu: 88
2020-04-02T20:18:39.752+05:30 INFO GOXDCR.ResourceMgr: backlogCount=0, noBacklogCount=70 extraQuota=false cpuNotMaxedCount=0 throughputDropCount=0
2020-04-02T20:18:39.752+05:30 INFO GOXDCR.ResourceMgr: DcpPriorityMap=map[]
ongoingReplMap=map[]

[ns_server:debug,2020-04-02T20:18:42.606+05:30,ns_1@127.0.0.1:json_rpc_connection-saslauthd-saslauthd-port<0.540.0>:json_rpc_connection:handle_info:129]Socket closed
[user:debug,2020-04-02T20:18:42.606+05:30,ns_1@127.0.0.1:<0.256.0>:ns_log:crash_consumption_loop:69]Service 'saslauthd_port' exited with status 0. Restarting. Messages:
2020/04/02 20:18:42 Got EOL. Exiting
[user:debug,2020-04-02T20:18:42.608+05:30,ns_1@127.0.0.1:<0.256.0>:ns_log:crash_consumption_loop:69]Service 'memcached' exited with status 0. Restarting. Messages:
EOL on stdin.  Initiating shutdown
[ns_server:debug,2020-04-02T20:18:42.608+05:30,ns_1@127.0.0.1:<0.545.0>:remote_monitors:monitor_loop:129]Monitored remote process <12938.114.0> went down with: shutdown
[ns_server:debug,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.538.0>:memcached_config_mgr:handle_info:163]Got DOWN with reason: shutdown from memcached port server: <12938.114.0>. Shutting down
[ns_server:debug,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:<0.452.0>:remote_monitors:monitor_loop:129]Monitored remote process <12938.107.0> went down with: shutdown
[ns_server:debug,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.2040.0>:memcached_config_mgr:init:49]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:<0.546.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.538.0>} exited with reason {shutdown,
                                                                                {memcached_port_server_down,
                                                                                 <12938.114.0>,
                                                                                 shutdown}}
[ns_server:debug,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:ns_ports_setup<0.380.0>:ns_ports_setup:children_loop_continue:121]ns_child_ports_sup <12938.107.0> died on babysitter node with shutdown. Restart.
[ns_server:debug,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:<0.5.0>:child_erlang:child_loop:134]32735: Got EOL
[ns_server:info,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:<0.5.0>:ns_bootstrap:stop:40]Initiated server shutdown
[error_logger:error,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {shutdown,
                     {memcached_port_server_down,<12938.114.0>,shutdown}}
     Offender:   [{pid,<0.538.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:memcached_config_mgr<0.2043.0>:memcached_config_mgr:init:49]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:<0.382.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.380.0>} exited with reason {{child_ports_sup_died,
                                                                                    <12938.107.0>,
                                                                                    shutdown},
                                                                                   [{ns_ports_setup,
                                                                                     children_loop_continue,
                                                                                     3,
                                                                                     [{file,
                                                                                       "src/ns_ports_setup.erl"},
                                                                                      {line,
                                                                                       122}]},
                                                                                    {proc_lib,
                                                                                     wake_up,
                                                                                     3,
                                                                                     [{file,
                                                                                       "proc_lib.erl"},
                                                                                      {line,
                                                                                       257}]}]}
[ns_server:debug,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:<0.537.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.536.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:<0.381.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.380.0>} exited with reason {{child_ports_sup_died,
                                                                                 <12938.107.0>,
                                                                                 shutdown},
                                                                                [{ns_ports_setup,
                                                                                  children_loop_continue,
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/ns_ports_setup.erl"},
                                                                                   {line,
                                                                                    122}]},
                                                                                 {proc_lib,
                                                                                  wake_up,
                                                                                  3,
                                                                                  [{file,
                                                                                    "proc_lib.erl"},
                                                                                   {line,
                                                                                    257}]}]}
[error_logger:info,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.2040.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:<0.534.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.533.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:<0.528.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.527.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:<0.522.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.521.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:<0.519.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.518.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:<0.516.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {master_activity_events,<0.515.0>} exited with reason killed
[ns_server:debug,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:<0.511.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {leader_events,<0.510.0>} exited with reason shutdown
[ns_server:info,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:mb_master<0.464.0>:mb_master:terminate:327]Synchronously shutting down child mb_master_sup
[ns_server:info,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_down:253]Process <0.505.0> registered as 'collections' terminated.
[ns_server:debug,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:<0.503.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {compat_mode_events,<0.502.0>} exited with reason shutdown
[ns_server:info,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_down:253]Process <0.502.0> registered as 'auto_failover' terminated.
[ns_server:info,2020-04-02T20:18:42.609+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_down:253]Process <0.500.0> registered as 'ns_orchestrator' terminated.
[error_logger:error,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_ports_setup:setup_body_tramp/0
    pid: <0.380.0>
    registered_name: ns_ports_setup
    exception error: {child_ports_sup_died,<12938.107.0>,shutdown}
      in function  ns_ports_setup:children_loop_continue/3 (src/ns_ports_setup.erl, line 122)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.208.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
    message_queue_len: 1
    messages: [{'$gen_call',{<0.2040.0>,
                                #Ref<0.2999594286.2850029570.258250>},
                               sync}]
    links: [<0.381.0>,<0.382.0>,<0.249.0>]
    dictionary: [{'ns_ports_setup-indexer-available',
                      "/opt/couchbase/bin/indexer"},
                  {'ns_ports_setup-eventing-producer-available',
                      "/opt/couchbase/bin/eventing-producer"},
                  {'ns_ports_setup-saslauthd-port-available',
                      "/opt/couchbase/bin/saslauthd-port"},
                  {'ns_ports_setup-cbas-available',"/opt/couchbase/bin/cbas"},
                  {'ns_ports_setup-cbft-available',"/opt/couchbase/bin/cbft"},
                  {'ns_ports_setup-cbq-engine-available',
                      "/opt/couchbase/bin/cbq-engine"},
                  {'ns_ports_setup-projector-available',
                      "/opt/couchbase/bin/projector"},
                  {'ns_ports_setup-goxdcr-available',
                      "/opt/couchbase/bin/goxdcr"}]
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 11761
  neighbours:

[ns_server:info,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_down:253]Process <0.499.0> registered as 'auto_rebalance' terminated.
[error_logger:info,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]Initiated server shutdown
[ns_server:debug,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:leader_activities<0.457.0>:leader_activities:handle_internal_process_down:511]Process {quorum_nodes_manager,<0.469.0>} terminated with reason shutdown
[ns_server:info,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_down:253]Process <0.498.0> registered as 'auto_reprovision' terminated.
[ns_server:debug,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:<0.486.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.469.0>} exited with reason shutdown
[ns_server:info,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_down:253]Process <0.471.0> registered as 'ns_tick' terminated.
[ns_server:debug,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:leader_activities<0.457.0>:leader_activities:handle_internal_process_down:511]Process {acquirer,<0.467.0>} terminated with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:<0.481.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.467.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:leader_registry<0.461.0>:leader_registry_server:handle_new_leader:241]New leader is undefined. Invalidating name cache.
[error_logger:error,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{child_ports_sup_died,<12938.107.0>,shutdown},
                  [{ns_ports_setup,children_loop_continue,3,
                                   [{file,"src/ns_ports_setup.erl"},
                                    {line,122}]},
                   {proc_lib,wake_up,3,[{file,"proc_lib.erl"},{line,257}]}]}
     Offender:   [{pid,<0.380.0>},
                  {name,ns_ports_setup},
                  {mfargs,{ns_ports_setup,start,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,brutal_kill},
                  {child_type,worker}]


[ns_server:debug,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:leader_lease_agent<0.459.0>:leader_lease_agent:handle_abolish_lease:255]Received abolish lease request from {lease_holder,
                                     <<"40880c332dfc1c595d9a156ef02d872b">>,
                                     'ns_1@127.0.0.1'} when lease is {lease,
                                                                      {lease_holder,
                                                                       <<"40880c332dfc1c595d9a156ef02d872b">>,
                                                                       'ns_1@127.0.0.1'},
                                                                      -576460678067639246,
                                                                      -576460663067639246,
                                                                      {timer,
                                                                       #Ref<0.2999594286.2850029571.258574>,
                                                                       {lease_expired,
                                                                        {lease_holder,
                                                                         <<"40880c332dfc1c595d9a156ef02d872b">>,
                                                                         'ns_1@127.0.0.1'}}},
                                                                      active}
[ns_server:debug,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:leader_lease_agent<0.459.0>:leader_lease_agent:handle_abolish_lease:260]Expiring abolished lease
[ns_server:debug,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:<0.465.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.464.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:<0.462.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {leader_events,<0.461.0>} exited with reason shutdown
[ns_server:warn,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:leader_lease_agent<0.459.0>:leader_lease_agent:handle_terminate:317]Terminating with reason shutdown when lease is expiring:
{lease,
    {lease_holder,<<"40880c332dfc1c595d9a156ef02d872b">>,'ns_1@127.0.0.1'},
    -576460678067639246,-576460663067639246,
    {timer,undefined,
        {lease_expired,
            {lease_holder,<<"40880c332dfc1c595d9a156ef02d872b">>,
                'ns_1@127.0.0.1'}}},
    expiring}
Removing the persisted lease.
[error_logger:error,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.2040.0>
    registered_name: memcached_config_mgr
    exception exit: {{{child_ports_sup_died,<12938.107.0>,shutdown},
                      [{ns_ports_setup,children_loop_continue,3,
                           [{file,"src/ns_ports_setup.erl"},{line,122}]},
                       {proc_lib,wake_up,3,
                           [{file,"proc_lib.erl"},{line,257}]}]},
                     {gen_server,call,[ns_ports_setup,sync,infinity]}}
      in function  gen_server:call/3 (gen_server.erl, line 214)
      in call from memcached_config_mgr:init/1 (src/memcached_config_mgr.erl, line 50)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.208.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.249.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 987
    stack_size: 27
    reductions: 1541
  neighbours:

[ns_server:debug,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:leader_activities<0.457.0>:leader_activities:handle_internal_process_down:511]Process {agent,<0.459.0>} terminated with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:<0.451.0>:restartable:shutdown_child:120]Successfully terminated process <0.453.0>
[error_logger:info,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.2041.0>},
                       {name,ns_ports_setup},
                       {mfargs,{ns_ports_setup,start,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:<0.446.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.445.0>} exited with reason shutdown
[error_logger:error,2020-04-02T20:18:42.610+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{{child_ports_sup_died,<12938.107.0>,shutdown},
                   [{ns_ports_setup,children_loop_continue,3,
                                    [{file,"src/ns_ports_setup.erl"},
                                     {line,122}]},
                    {proc_lib,wake_up,3,[{file,"proc_lib.erl"},{line,257}]}]},
                  {gen_server,call,[ns_ports_setup,sync,infinity]}}
     Offender:   [{pid,<0.2040.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2020-04-02T20:18:42.611+05:30,ns_1@127.0.0.1:<0.444.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.443.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.611+05:30,ns_1@127.0.0.1:<0.442.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.440.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.611+05:30,ns_1@127.0.0.1:<0.441.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.440.0>} exited with reason shutdown
[error_logger:info,2020-04-02T20:18:42.611+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.2043.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T20:18:42.611+05:30,ns_1@127.0.0.1:<0.439.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.437.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.611+05:30,ns_1@127.0.0.1:<0.438.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.437.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.611+05:30,ns_1@127.0.0.1:<0.436.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.434.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.611+05:30,ns_1@127.0.0.1:<0.435.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.434.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.611+05:30,ns_1@127.0.0.1:<0.426.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_tick_event,<0.425.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.611+05:30,ns_1@127.0.0.1:ns_ports_setup<0.2041.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,goxdcr]
[ns_server:debug,2020-04-02T20:18:42.611+05:30,ns_1@127.0.0.1:ns_ports_setup<0.2041.0>:misc:delaying_crash:1608]Delaying crash exit:{noproc,
                     {gen_server,call,
                      [{ns_ports_manager,'babysitter_of_ns_1@cb.local'},
                       {set_dynamic_children,
                        [{memcached,"/opt/couchbase/bin/memcached",
                          ["-C",
                           "/opt/couchbase/var/lib/couchbase/config/memcached.json"],
                          [{env,
                            [{"EVENT_NOSELECT","1"},
                             {"MEMCACHED_TOP_KEYS","5"},
                             {"CBSASL_PWFILE",
                              "/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
                           use_stdio,stderr_to_stdout,exit_status,
                           port_server_dont_start,stream]},
                         {saslauthd_port,"/opt/couchbase/bin/saslauthd-port",
                          [],
                          [use_stdio,exit_status,stderr_to_stdout,
                           {env,
                            [{"GOTRACEBACK","single"},
                             {"CBAUTH_REVRPC_URL",
                              "http://%40:cd9f4e7809c92776964e73f156d66d0a@127.0.0.1:8091/saslauthd"}]}]},
                         {goxdcr,"/opt/couchbase/bin/goxdcr",
                          ["-sourceKVAdminPort=8091","-xdcrRestPort=9998",
                           "-isEnterprise=false","-ipv6=false"],
                          [via_goport,exit_status,stderr_to_stdout,
                           {env,
                            [{"GOTRACEBACK","single"},
                             {"CBAUTH_REVRPC_URL",
                              "http://%40:cd9f4e7809c92776964e73f156d66d0a@127.0.0.1:8091/goxdcr"}]},
                           {log,"goxdcr.log"}]}]},
                       infinity]}} by 1000ms
Stacktrace: [{gen_server,call,3,[{file,"gen_server.erl"},{line,214}]},
             {ns_ports_setup,set_children,2,
                             [{file,"src/ns_ports_setup.erl"},{line,81}]},
             {ns_ports_setup,set_children_and_loop,3,
                             [{file,"src/ns_ports_setup.erl"},{line,97}]},
             {misc,delaying_crash,2,[{file,"src/misc.erl"},{line,1605}]},
             {proc_lib,init_p_do_apply,3,[{file,"proc_lib.erl"},{line,247}]}]
[ns_server:debug,2020-04-02T20:18:42.627+05:30,ns_1@127.0.0.1:<0.423.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_stats_event,<0.422.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.627+05:30,ns_1@127.0.0.1:<0.421.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_tick_event,<0.420.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.645+05:30,ns_1@127.0.0.1:<0.418.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_stats_event,<0.417.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.682+05:30,ns_1@127.0.0.1:<0.415.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_stats_event,<0.414.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.703+05:30,ns_1@127.0.0.1:<0.412.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_stats_event,<0.411.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.703+05:30,ns_1@127.0.0.1:<0.410.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_tick_event,<0.407.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.703+05:30,ns_1@127.0.0.1:<0.409.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ale_stats_events,<0.407.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.704+05:30,ns_1@127.0.0.1:<0.406.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.405.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.704+05:30,ns_1@127.0.0.1:<0.400.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {bucket_info_cache_invalidations,<0.399.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.704+05:30,ns_1@127.0.0.1:<0.391.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.390.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.704+05:30,ns_1@127.0.0.1:<0.386.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.385.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.704+05:30,ns_1@127.0.0.1:<0.389.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.388.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.704+05:30,ns_1@127.0.0.1:<0.2042.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.2041.0>} exited with reason killed
[ns_server:debug,2020-04-02T20:18:42.704+05:30,ns_1@127.0.0.1:<0.2044.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.2041.0>} exited with reason killed
[ns_server:debug,2020-04-02T20:18:42.705+05:30,ns_1@127.0.0.1:<0.377.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.374.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.705+05:30,ns_1@127.0.0.1:<0.376.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.374.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.705+05:30,ns_1@127.0.0.1:<0.375.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {json_rpc_events,<0.374.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.705+05:30,ns_1@127.0.0.1:<0.379.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ssl_service_events,<0.374.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.705+05:30,ns_1@127.0.0.1:<0.378.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.374.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.706+05:30,ns_1@127.0.0.1:<0.331.0>:restartable:shutdown_child:120]Successfully terminated process <0.334.0>
[ns_server:debug,2020-04-02T20:18:42.706+05:30,ns_1@127.0.0.1:<0.320.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.319.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.707+05:30,ns_1@127.0.0.1:<0.315.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.314.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.707+05:30,ns_1@127.0.0.1:<0.313.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.312.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.707+05:30,ns_1@127.0.0.1:<0.311.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.310.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.707+05:30,ns_1@127.0.0.1:<0.303.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.302.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.707+05:30,ns_1@127.0.0.1:<0.299.0>:restartable:shutdown_child:120]Successfully terminated process <0.300.0>
[ns_server:debug,2020-04-02T20:18:42.708+05:30,ns_1@127.0.0.1:<0.295.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {buckets_events,<0.294.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.708+05:30,ns_1@127.0.0.1:<0.288.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.286.0>} exited with reason killed
[ns_server:debug,2020-04-02T20:18:42.708+05:30,ns_1@127.0.0.1:<0.285.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.284.0>} exited with reason killed
[ns_server:debug,2020-04-02T20:18:42.709+05:30,ns_1@127.0.0.1:<0.273.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events_local,<0.272.0>} exited with reason shutdown
[error_logger:error,2020-04-02T20:18:42.709+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: gen_event:init_it/6
    pid: <0.287.0>
    registered_name: bucket_info_cache_invalidations
    exception exit: killed
      in function  gen_event:terminate_server/4 (gen_event.erl, line 351)
    ancestors: [bucket_info_cache,ns_server_sup,ns_server_nodes_sup,
                  <0.208.0>,ns_server_cluster_sup,root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: []
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 280
  neighbours:

[ns_server:debug,2020-04-02T20:18:42.710+05:30,ns_1@127.0.0.1:<0.262.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.260.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.710+05:30,ns_1@127.0.0.1:<0.261.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.260.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.710+05:30,ns_1@127.0.0.1:<0.258.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.257.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.710+05:30,ns_1@127.0.0.1:<0.259.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.257.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.712+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.235.0>:ns_port_server:terminate:196]Shutting down port ns_couchdb
[ns_server:debug,2020-04-02T20:18:42.712+05:30,ns_1@127.0.0.1:<0.247.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.236.0> died with shutdown. Exiting
[ns_server:debug,2020-04-02T20:18:42.712+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.235.0>:ns_port_server:port_shutdown:297]Shutdown command: "shutdown"
[error_logger:info,2020-04-02T20:18:42.719+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-04-02T20:18:42.719+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-04-02T20:18:42.719+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.242.0>,connection_closed}}
[ns_server:debug,2020-04-02T20:18:42.719+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2999594286.2850029569.256116>,
                               inet_tcp_dist,<0.242.0>,
                               #Ref<0.2999594286.2850029569.256120>}
[error_logger:info,2020-04-02T20:18:42.719+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:18:42.720+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.235.0>:ns_port_server:terminate:199]ns_couchdb has exited
[ns_server:debug,2020-04-02T20:18:42.720+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:info,2020-04-02T20:18:42.720+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.235.0>:ns_port_server:log:224]ns_couchdb<0.235.0>: 350: got shutdown request. Exiting
ns_couchdb<0.235.0>: [os_mon] cpu supervisor port (cpu_sup): Erlang has closed
ns_couchdb<0.235.0>: [os_mon] memory supervisor port (memsup): Erlang has closed

[ns_server:debug,2020-04-02T20:18:42.720+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2999594286.2850029569.260403>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T20:18:42.720+05:30,ns_1@127.0.0.1:<0.230.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.228.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.720+05:30,ns_1@127.0.0.1:<0.229.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.228.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.720+05:30,ns_1@127.0.0.1:<0.227.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.225.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.720+05:30,ns_1@127.0.0.1:<0.226.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.225.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.720+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2999594286.2850029569.260403>,
                                  inet_tcp_dist,<0.2047.0>,
                                  #Ref<0.2999594286.2850029572.258135>}
[ns_server:debug,2020-04-02T20:18:42.720+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2999594286.2850029569.260403>,
                               inet_tcp_dist,<0.2047.0>,
                               #Ref<0.2999594286.2850029572.258135>}
[error_logger:info,2020-04-02T20:18:42.720+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.2047.0>,shutdown}}
[ns_server:debug,2020-04-02T20:18:42.721+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T20:18:42.721+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:18:42.721+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2999594286.2850029572.258140>,
                               inet_tcp_dist,undefined,undefined}
[error_logger:info,2020-04-02T20:18:42.721+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:18:42.721+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2999594286.2850029572.258140>,
                                  inet_tcp_dist,<0.2050.0>,
                                  #Ref<0.2999594286.2850029572.258142>}
[ns_server:debug,2020-04-02T20:18:42.721+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2999594286.2850029572.258140>,
                               inet_tcp_dist,<0.2050.0>,
                               #Ref<0.2999594286.2850029572.258142>}
[error_logger:info,2020-04-02T20:18:42.721+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.2050.0>,shutdown}}
[ns_server:debug,2020-04-02T20:18:42.721+05:30,ns_1@127.0.0.1:<0.218.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.217.0>} exited with reason shutdown
[error_logger:info,2020-04-02T20:18:42.721+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T20:18:42.721+05:30,ns_1@127.0.0.1:<0.208.0>:restartable:shutdown_child:120]Successfully terminated process <0.209.0>
[ns_server:debug,2020-04-02T20:18:42.721+05:30,ns_1@127.0.0.1:<0.203.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.202.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T20:18:42.722+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:wait_saver:866]Done waiting for saver.
[ns_server:info,2020-04-02T20:18:42.723+05:30,ns_1@127.0.0.1:<0.5.0>:ns_bootstrap:stop:44]Successfully stopped ns_server
[error_logger:info,2020-04-02T20:18:42.723+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
         application: ns_server
              exited: stopped
                type: permanent

[ns_server:info,2020-04-02T21:10:41.588+05:30,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-04-02T21:10:41.601+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-04-02T21:10:41.601+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.601+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.601+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.601+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.601+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.601+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.601+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.601+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.601+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.601+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.601+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.601+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.601+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.601+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.601+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.601+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.602+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.602+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.602+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.602+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.602+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.602+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.602+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.602+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.602+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-04-02T21:10:41.602+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-04-02T21:10:41.606+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-04-02T21:10:41.606+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[error_logger:info,2020-04-02T21:10:41.607+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.168.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2020-04-02T21:10:41.607+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.167.0>},
                       {id,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:10:41.609+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@127.0.0.1'
[error_logger:info,2020-04-02T21:10:41.616+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.616+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.616+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:41.618+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.174.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.619+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.176.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:41.619+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.177.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:41.619+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.175.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T21:10:41.619+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2020-04-02T21:10:41.620+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:41.621+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.621+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.622+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.180.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:10:41.623+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-04-02T21:10:41.623+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-04-02T21:10:41.623+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2020-04-02T21:10:41.625+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-04-02T21:10:41.625+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.181.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.625+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.169.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-04-02T21:10:41.625+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-04-02T21:10:41.632+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-04-02T21:10:41.632+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[ns_server:debug,2020-04-02T21:10:41.632+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T21:10:41.632+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:10:41.633+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.4247871495.979107842.131908>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:10:41.633+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.4247871495.979107842.131908>,
                                  inet_tcp_dist,<0.185.0>,
                                  #Ref<0.4247871495.979107843.132121>}
[ns_server:debug,2020-04-02T21:10:41.641+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-04-02T21:10:41.641+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": ok
[ns_server:info,2020-04-02T21:10:41.641+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-04-02T21:10:41.643+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-04-02T21:10:41.643+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.650+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:10:41.652+05:30,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,4,2},{21,10,41}}},
               {memory,
                   [{total,26322184},
                    {processes,9516800},
                    {processes_used,9512232},
                    {system,16805384},
                    {atom,388625},
                    {atom_used,364409},
                    {binary,122608},
                    {code,8250921},
                    {ets,1509096}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    application_master,code,error_handler,application,
                    code_server,application_controller,supervisor,file,
                    file_io_server,proc_lib,file_server,gen_server,
                    error_logger,heart,lists,kernel,gen_event,gen,filename,
                    erl_parse,ets,erl_lint,erl_eval,
                    erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{os_mon,"CPO  CXC 138 46","2.4.4"},
                    {sasl,"SASL  CXC 138 11","3.1.2"},
                    {ns_server,"Couchbase server","6.5.0-4960-enterprise"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ale,"Another Logger for Erlang","0.0.0"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,131},
               {node,'ns_1@127.0.0.1'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,auth,httpd_sup,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    release_handler,lhttpc_sup,httpc_sup,lhttpc_manager,
                    alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    'sink-disk_xdcr',timer_server,'sink-disk_debug',
                    inet_gethost_native,ale_sup,'sink-disk_error',inet_db,
                    'sink-disk_default',kernel_safe_sup,ssl_pem_cache_dist,
                    ale_dynamic_sup,rex,global_group,net_sup,kernel_sup,
                    ssl_connection_sup,global_name_server,ssl_admin_sup,
                    tftp_sup,ssl_sup,root_sup,erts_code_purger,os_mon_sup,
                    file_server_2,error_logger,cpu_sup,erl_epmd,init,memsup,
                    erl_signal_server,disksup,ale,net_kernel,dist_manager,
                    ssl_pem_cache,ssl_manager,ssl_dist_admin_sup,
                    ssl_dist_connection_sup,ssl_dist_sup,user,
                    ssl_tls_dist_proxy,ssl_manager_dist,sasl_safe_sup,
                    ssl_listen_tracker_sup,inet_gethost_native_sup,
                    code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,1}]
[ns_server:info,2020-04-02T21:10:41.655+05:30,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"cfa0f75f28402d2e1aa254b2a374bead19433526\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" upstream=\"master\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"f2a16b53bb74146f20d18ba2c0443d5f10a9a550\" upstream=\"master\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4960\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" upstream=\"master\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" upstream=\"mad-hatter\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" upstream=\"master\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" upstream=\"master\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" upstream=\"master\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" upstream=\"mad-hatter\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" upstream=\"cb-master\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" upstream=\"master\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" upstream=\"master\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" upstream=\"mad-hatter\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" upstream=\"mad-hatter\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" upstream=\"mad-hatter\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" upstream=\"master\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" upstream=\"master\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" upstream=\"mad-hatter\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" upstream=\"mad-hatter\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" upstream=\"mad-hatter\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" upstream=\"mad-hatter\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" upstream=\"master\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" upstream=\"master\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" upstream=\"master\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" upstream=\"mad-hatter\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" upstream=\"master\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"7a3aa547b3f5eb3ea28d279a08384609cd2cea7c\" upstream=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" upstream=\"mad-hatter\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" upstream=\"master\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" upstream=\"master\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" upstream=\"master\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" upstream=\"v1.1.0-couchbase\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"ee64d41320d14fabe814a241a5cf4f6a6f6e827a\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" upstream=\"master\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" upstream=\"master\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-04-02T21:10:41.658+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.659+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.659+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:10:41.660+05:30,ns_1@127.0.0.1:ns_config_sup<0.192.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-04-02T21:10:41.660+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.660+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.194.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:10:41.677+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-04-02T21:10:41.677+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:info,2020-04-02T21:10:41.677+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1125]No dynamic config file found. Assuming we're brand new node
[ns_server:debug,2020-04-02T21:10:41.680+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[]]
[ns_server:info,2020-04-02T21:10:41.683+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',cbas_debug_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|-1]},
 {{node,'ns_1@127.0.0.1',cbas_parent_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9122]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9121]},
 {{node,'ns_1@127.0.0.1',cbas_replication_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9120]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9119]},
 {{node,'ns_1@127.0.0.1',cbas_messaging_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9118]},
 {{node,'ns_1@127.0.0.1',cbas_result_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9117]},
 {{node,'ns_1@127.0.0.1',cbas_data_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9116]},
 {{node,'ns_1@127.0.0.1',cbas_cluster_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9115]},
 {{node,'ns_1@127.0.0.1',cbas_console_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9114]},
 {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9113]},
 {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9112]},
 {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9111]},
 {{node,'ns_1@127.0.0.1',cbas_admin_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9110]},
 {{node,'ns_1@127.0.0.1',cbas_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18095]},
 {{node,'ns_1@127.0.0.1',cbas_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8095]},
 {{node,'ns_1@127.0.0.1',eventing_https_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18096]},
 {{node,'ns_1@127.0.0.1',eventing_debug_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9140]},
 {{node,'ns_1@127.0.0.1',eventing_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8096]},
 {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   19130]},
 {{node,'ns_1@127.0.0.1',fts_grpc_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9130]},
 {{node,'ns_1@127.0.0.1',fts_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18094]},
 {{node,'ns_1@127.0.0.1',fts_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8094]},
 {{node,'ns_1@127.0.0.1',indexer_https_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   19102]},
 {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9105]},
 {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9104]},
 {{node,'ns_1@127.0.0.1',indexer_stinit_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9103]},
 {{node,'ns_1@127.0.0.1',indexer_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9102]},
 {{node,'ns_1@127.0.0.1',indexer_scan_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9101]},
 {{node,'ns_1@127.0.0.1',indexer_admin_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9100]},
 {{node,'ns_1@127.0.0.1',ssl_query_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18093]},
 {{node,'ns_1@127.0.0.1',query_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8093]},
 {{node,'ns_1@127.0.0.1',projector_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',projector_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',ssl_capi_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18092]},
 {{node,'ns_1@127.0.0.1',capi_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8092]},
 {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   11206]},
 {{node,'ns_1@127.0.0.1',xdcr_rest_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9998]},
 {{node,'ns_1@127.0.0.1',ssl_rest_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18091]},
 {{node,'ns_1@127.0.0.1',rest},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {port,8091},
   {port_meta,global}]},
 {rest,[{port,8091}]},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {drop_request_memory_threshold_mib,undefined},
 {{request_limit,capi},undefined},
 {{request_limit,rest},undefined},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {replication,[{enabled,true}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {{node,'ns_1@127.0.0.1',ns_log},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@127.0.0.1',port_servers},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
 {{node,'ns_1@127.0.0.1',moxi},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {port,0}]},
 {secure_headers,[]},
 {buckets,[{configs,[]}]},
 {cbas_memory_quota,2174},
 {fts_memory_quota,512},
 {memory_quota,8886},
 {{node,'ns_1@127.0.0.1',memcached_config},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@127.0.0.1',memcached},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@127.0.0.1',memcached_defaults},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {memcached,[]},
 {{node,'ns_1@127.0.0.1',audit},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {{node,'ns_1@127.0.0.1',isasl},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {remote_clusters,[]},
 {rest_creds,null},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {{node,'ns_1@127.0.0.1',membership},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   active]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
 {quorum_nodes,['ns_1@127.0.0.1']},
 {nodes_wanted,['ns_1@127.0.0.1']},
 {{node,'ns_1@127.0.0.1',compaction_daemon},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {max_bucket_count,30},
 {index_aware_rebalance_disabled,false},
 {{node,'ns_1@127.0.0.1',saslauthd_enabled},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',is_enterprise},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',config_version},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   {6,5}]},
 {{node,'ns_1@127.0.0.1',uuid},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   <<"8c43a5102cad1e34db659ab4d5646878">>]}]
[error_logger:info,2020-04-02T21:10:41.685+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.195.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.686+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.201.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.687+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.202.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.687+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T21:10:41.688+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',erl_external_listeners} ->
[{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
 {inet,false},
 {inet6,false}]
[error_logger:info,2020-04-02T21:10:41.688+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.204.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T21:10:41.688+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',node_encryption} ->
[{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|false]
[ns_server:debug,2020-04-02T21:10:41.688+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',address_family} ->
[{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|inet]
[ns_server:debug,2020-04-02T21:10:41.689+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"8c43a5102cad1e34db659ab4d5646878">>} ->
[{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]
[error_logger:info,2020-04-02T21:10:41.690+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.207.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:41.695+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:10:41.695+05:30,ns_1@127.0.0.1:menelaus_barrier<0.211.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-04-02T21:10:41.695+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.695+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.701+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.213.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.702+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.215.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:10:41.863+05:30,ns_1@127.0.0.1:<0.220.0>:goport:handle_eof:582]Stream 'stderr' closed
[ns_server:debug,2020-04-02T21:10:41.863+05:30,ns_1@127.0.0.1:<0.220.0>:goport:handle_eof:582]Stream 'stdout' closed
[ns_server:info,2020-04-02T21:10:41.863+05:30,ns_1@127.0.0.1:<0.220.0>:goport:handle_process_exit:563]Port exited with status 0.
[ns_server:debug,2020-04-02T21:10:41.869+05:30,ns_1@127.0.0.1:ns_ssl_services_setup<0.216.0>:ns_server_cert:generate_cert_and_pkey:83]Generated certificate and private key in 165890 us
[ns_server:debug,2020-04-02T21:10:41.869+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
cert_and_pkey ->
[{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
 {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFgIK71cHor8wDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBkZTZmMzM0MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgZGU2ZjMz\nNDAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC6Epk+5C0GfEqGHL9d\nxsySLywt3gLcVQmCM8lgcMRGWDaGVF6iOP+QyLODyB09I5u2gOcVm+1r3eOZ4rwk\nbttVmFIsdroNf2jG+9baY4LqKoDyZnj"...>>,
  <<"*****">>}]
[ns_server:debug,2020-04-02T21:10:41.869+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"8c43a5102cad1e34db659ab4d5646878">>} ->
[{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{2,63753061241}}]}]
[ns_server:info,2020-04-02T21:10:41.870+05:30,ns_1@127.0.0.1:ns_ssl_services_setup<0.216.0>:ns_ssl_services_setup:maybe_generate_local_cert:617]Detected existing node certificate that did not match cluster certificate. Will re-generate
[ns_server:debug,2020-04-02T21:10:41.922+05:30,ns_1@127.0.0.1:<0.224.0>:goport:handle_eof:582]Stream 'stdout' closed
[ns_server:debug,2020-04-02T21:10:41.922+05:30,ns_1@127.0.0.1:<0.224.0>:goport:handle_eof:582]Stream 'stderr' closed
[ns_server:info,2020-04-02T21:10:41.922+05:30,ns_1@127.0.0.1:<0.224.0>:goport:handle_process_exit:563]Port exited with status 0.
[ns_server:info,2020-04-02T21:10:41.929+05:30,ns_1@127.0.0.1:ns_ssl_services_setup<0.216.0>:ns_ssl_services_setup:do_generate_local_cert:608]Saved local cert for node 'ns_1@127.0.0.1'
[ns_server:debug,2020-04-02T21:10:41.940+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2020-04-02T21:10:41.940+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2020-04-02T21:10:41.940+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2020-04-02T21:10:41.953+05:30,ns_1@127.0.0.1:ns_ssl_services_setup<0.216.0>:ns_ssl_services_setup:init:462]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,2,10,239,87,7,162,
             191,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
             6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
             118,101,114,32,100,101,54,102,51,51,52,48,48,30,23,13,49,51,48,
             49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
             57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
             104,98,97,115,101,32,83,101,114,118,101,114,32,100,101,54,102,
             51,51,52,48,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,
             0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,186,18,153,62,228,45,6,
             124,74,134,28,191,93,198,204,146,47,44,45,222,2,220,85,9,130,51,
             201,96,112,196,70,88,54,134,84,94,162,56,255,144,200,179,131,
             200,29,61,35,155,182,128,231,21,155,237,107,221,227,153,226,188,
             36,110,219,85,152,82,44,118,186,13,127,104,198,251,214,218,99,
             130,234,42,128,242,102,120,217,175,66,222,162,85,28,99,237,30,
             96,142,188,160,220,17,50,5,169,231,114,171,155,206,181,203,93,
             114,200,225,35,121,249,190,36,16,225,102,230,105,29,151,160,141,
             48,74,181,138,153,119,222,150,103,3,179,110,178,89,173,171,183,
             184,160,110,65,57,168,17,218,89,225,87,87,233,78,103,205,70,145,
             146,248,227,184,26,80,201,234,140,45,43,174,45,67,184,20,67,105,
             212,47,15,245,246,33,203,134,58,69,22,95,49,32,70,119,44,53,119,
             196,4,91,24,148,226,127,235,181,244,32,136,221,246,168,236,45,
             99,186,28,194,246,151,13,94,240,31,88,169,90,67,197,229,89,252,
             123,32,98,229,192,54,198,168,26,171,92,82,4,176,252,49,65,196,
             93,48,35,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,255,4,4,
             3,2,2,164,48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,
             15,6,3,85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,134,72,
             134,247,13,1,1,11,5,0,3,130,1,1,0,143,245,168,239,120,74,180,
             213,131,42,255,48,165,16,8,196,221,48,155,26,172,253,187,251,
             227,93,216,82,64,174,235,60,146,220,43,110,116,119,206,139,234,
             120,243,145,45,159,126,182,155,108,225,142,55,238,20,188,213,
             184,77,75,155,220,154,167,78,228,165,35,192,122,174,33,250,214,
             74,150,61,240,159,68,45,253,218,253,113,0,134,144,35,95,227,90,
             197,184,29,10,34,93,185,162,210,232,138,174,193,42,34,223,23,23,
             153,203,92,59,4,121,102,30,183,41,235,56,55,85,83,208,3,120,51,
             100,19,111,128,59,34,211,188,63,248,121,64,209,161,8,67,132,46,
             111,135,190,65,187,216,38,17,95,158,124,210,130,193,120,236,227,
             16,12,60,102,202,160,217,214,6,76,123,6,217,244,250,44,136,215,
             217,126,142,188,197,214,185,67,104,126,169,23,61,165,118,169,
             186,8,191,159,219,166,66,83,212,188,185,115,205,11,181,34,53,
             122,250,13,152,245,178,127,228,210,170,158,108,52,103,30,83,182,
             63,236,24,167,145,191,249,23,26,248,30,53,118,37,130,87,188,70,
             114,119,211,127,83,199>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2020-04-02T21:10:41.954+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.216.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:10:41.965+05:30,ns_1@127.0.0.1:<0.226.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-04-02T21:10:41.966+05:30,ns_1@127.0.0.1:<0.226.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-04-02T21:10:41.966+05:30,ns_1@127.0.0.1:<0.226.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-04-02T21:10:41.966+05:30,ns_1@127.0.0.1:<0.226.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:info,2020-04-02T21:10:41.978+05:30,ns_1@127.0.0.1:<0.226.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-04-02T21:10:41.978+05:30,ns_1@127.0.0.1:<0.226.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-04-02T21:10:41.978+05:30,ns_1@127.0.0.1:<0.226.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-04-02T21:10:41.978+05:30,ns_1@127.0.0.1:<0.226.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-04-02T21:10:41.978+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.226.0>,menelaus_web}
             started: [{pid,<0.227.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 2,10,239,87,7,162,191,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,100,101,54,
                                 102,51,51,52,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,100,
                                 101,54,102,51,51,52,48,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,186,18,153,62,228,
                                 45,6,124,74,134,28,191,93,198,204,146,47,44,
                                 45,222,2,220,85,9,130,51,201,96,112,196,70,
                                 88,54,134,84,94,162,56,255,144,200,179,131,
                                 200,29,61,35,155,182,128,231,21,155,237,107,
                                 221,227,153,226,188,36,110,219,85,152,82,44,
                                 118,186,13,127,104,198,251,214,218,99,130,
                                 234,42,128,242,102,120,217,175,66,222,162,
                                 85,28,99,237,30,96,142,188,160,220,17,50,5,
                                 169,231,114,171,155,206,181,203,93,114,200,
                                 225,35,121,249,190,36,16,225,102,230,105,29,
                                 151,160,141,48,74,181,138,153,119,222,150,
                                 103,3,179,110,178,89,173,171,183,184,160,
                                 110,65,57,168,17,218,89,225,87,87,233,78,
                                 103,205,70,145,146,248,227,184,26,80,201,
                                 234,140,45,43,174,45,67,184,20,67,105,212,
                                 47,15,245,246,33,203,134,58,69,22,95,49,32,
                                 70,119,44,53,119,196,4,91,24,148,226,127,
                                 235,181,244,32,136,221,246,168,236,45,99,
                                 186,28,194,246,151,13,94,240,31,88,169,90,
                                 67,197,229,89,252,123,32,98,229,192,54,198,
                                 168,26,171,92,82,4,176,252,49,65,196,93,48,
                                 35,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,
                                 15,1,1,255,4,4,3,2,2,164,48,19,6,3,85,29,37,
                                 4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,
                                 85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,
                                 42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 143,245,168,239,120,74,180,213,131,42,255,
                                 48,165,16,8,196,221,48,155,26,172,253,187,
                                 251,227,93,216,82,64,174,235,60,146,220,43,
                                 110,116,119,206,139,234,120,243,145,45,159,
                                 126,182,155,108,225,142,55,238,20,188,213,
                                 184,77,75,155,220,154,167,78,228,165,35,192,
                                 122,174,33,250,214,74,150,61,240,159,68,45,
                                 253,218,253,113,0,134,144,35,95,227,90,197,
                                 184,29,10,34,93,185,162,210,232,138,174,193,
                                 42,34,223,23,23,153,203,92,59,4,121,102,30,
                                 183,41,235,56,55,85,83,208,3,120,51,100,19,
                                 111,128,59,34,211,188,63,248,121,64,209,161,
                                 8,67,132,46,111,135,190,65,187,216,38,17,95,
                                 158,124,210,130,193,120,236,227,16,12,60,
                                 102,202,160,217,214,6,76,123,6,217,244,250,
                                 44,136,215,217,126,142,188,197,214,185,67,
                                 104,126,169,23,61,165,118,169,186,8,191,159,
                                 219,166,66,83,212,188,185,115,205,11,181,34,
                                 53,122,250,13,152,245,178,127,228,210,170,
                                 158,108,52,103,30,83,182,63,236,24,167,145,
                                 191,249,23,26,248,30,53,118,37,130,87,188,
                                 70,114,119,211,127,83,199>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:10:41.979+05:30,ns_1@127.0.0.1:<0.225.0>:restartable:start_child:98]Started child process <0.226.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2020-04-02T21:10:41.979+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.226.0>,menelaus_web}
             started: [{pid,<0.245.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 2,10,239,87,7,162,191,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,100,101,54,
                                 102,51,51,52,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,100,
                                 101,54,102,51,51,52,48,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,186,18,153,62,228,
                                 45,6,124,74,134,28,191,93,198,204,146,47,44,
                                 45,222,2,220,85,9,130,51,201,96,112,196,70,
                                 88,54,134,84,94,162,56,255,144,200,179,131,
                                 200,29,61,35,155,182,128,231,21,155,237,107,
                                 221,227,153,226,188,36,110,219,85,152,82,44,
                                 118,186,13,127,104,198,251,214,218,99,130,
                                 234,42,128,242,102,120,217,175,66,222,162,
                                 85,28,99,237,30,96,142,188,160,220,17,50,5,
                                 169,231,114,171,155,206,181,203,93,114,200,
                                 225,35,121,249,190,36,16,225,102,230,105,29,
                                 151,160,141,48,74,181,138,153,119,222,150,
                                 103,3,179,110,178,89,173,171,183,184,160,
                                 110,65,57,168,17,218,89,225,87,87,233,78,
                                 103,205,70,145,146,248,227,184,26,80,201,
                                 234,140,45,43,174,45,67,184,20,67,105,212,
                                 47,15,245,246,33,203,134,58,69,22,95,49,32,
                                 70,119,44,53,119,196,4,91,24,148,226,127,
                                 235,181,244,32,136,221,246,168,236,45,99,
                                 186,28,194,246,151,13,94,240,31,88,169,90,
                                 67,197,229,89,252,123,32,98,229,192,54,198,
                                 168,26,171,92,82,4,176,252,49,65,196,93,48,
                                 35,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,
                                 15,1,1,255,4,4,3,2,2,164,48,19,6,3,85,29,37,
                                 4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,
                                 85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,
                                 42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 143,245,168,239,120,74,180,213,131,42,255,
                                 48,165,16,8,196,221,48,155,26,172,253,187,
                                 251,227,93,216,82,64,174,235,60,146,220,43,
                                 110,116,119,206,139,234,120,243,145,45,159,
                                 126,182,155,108,225,142,55,238,20,188,213,
                                 184,77,75,155,220,154,167,78,228,165,35,192,
                                 122,174,33,250,214,74,150,61,240,159,68,45,
                                 253,218,253,113,0,134,144,35,95,227,90,197,
                                 184,29,10,34,93,185,162,210,232,138,174,193,
                                 42,34,223,23,23,153,203,92,59,4,121,102,30,
                                 183,41,235,56,55,85,83,208,3,120,51,100,19,
                                 111,128,59,34,211,188,63,248,121,64,209,161,
                                 8,67,132,46,111,135,190,65,187,216,38,17,95,
                                 158,124,210,130,193,120,236,227,16,12,60,
                                 102,202,160,217,214,6,76,123,6,217,244,250,
                                 44,136,215,217,126,142,188,197,214,185,67,
                                 104,126,169,23,61,165,118,169,186,8,191,159,
                                 219,166,66,83,212,188,185,115,205,11,181,34,
                                 53,122,250,13,152,245,178,127,228,210,170,
                                 158,108,52,103,30,83,182,63,236,24,167,145,
                                 191,249,23,26,248,30,53,118,37,130,87,188,
                                 70,114,119,211,127,83,199>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.980+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.225.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.980+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.214.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:41.984+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.263.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.985+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.266.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.990+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.268.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:10:41.991+05:30,ns_1@127.0.0.1:users_replicator<0.268.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-04-02T21:10:41.992+05:30,ns_1@127.0.0.1:users_storage<0.269.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.268.0>
[ns_server:debug,2020-04-02T21:10:41.992+05:30,ns_1@127.0.0.1:users_replicator<0.268.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.269.0>
[ns_server:debug,2020-04-02T21:10:41.992+05:30,ns_1@127.0.0.1:users_storage<0.269.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-04-02T21:10:41.993+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.269.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.993+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.267.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T21:10:41.997+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.271.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-04-02T21:10:41.997+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.271.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.999+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.274.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:41.999+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.265.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:41.999+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.277.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:41.999+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.278.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:10:42.007+05:30,ns_1@127.0.0.1:users_storage<0.269.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-04-02T21:10:42.008+05:30,ns_1@127.0.0.1:users_storage<0.269.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 15ms
[ns_server:debug,2020-04-02T21:10:42.009+05:30,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.282.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[error_logger:info,2020-04-02T21:10:42.009+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.281.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:42.009+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:10:42.009+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T21:10:42.009+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.4247871495.979107841.132240>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:10:42.009+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.4247871495.979107841.132240>,
                                  inet_tcp_dist,<0.285.0>,
                                  #Ref<0.4247871495.979107841.132242>}
[ns_server:debug,2020-04-02T21:10:42.009+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.4247871495.979107841.132240>,
                               inet_tcp_dist,<0.285.0>,
                               #Ref<0.4247871495.979107841.132242>}
[error_logger:info,2020-04-02T21:10:42.009+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.285.0>,shutdown}}
[ns_server:debug,2020-04-02T21:10:42.009+05:30,ns_1@127.0.0.1:<0.283.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-04-02T21:10:42.009+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:10:42.010+05:30,ns_1@127.0.0.1:users_replicator<0.268.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[ns_server:debug,2020-04-02T21:10:42.210+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T21:10:42.210+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:10:42.210+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.4247871495.979107844.132244>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:10:42.210+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.4247871495.979107844.132244>,
                                  inet_tcp_dist,<0.288.0>,
                                  #Ref<0.4247871495.979107844.132245>}
[ns_server:debug,2020-04-02T21:10:42.253+05:30,ns_1@127.0.0.1:<0.283.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-04-02T21:10:42.454+05:30,ns_1@127.0.0.1:<0.283.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-04-02T21:10:42.725+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.292.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:10:42.775+05:30,ns_1@127.0.0.1:<0.283.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-04-02T21:10:42.786+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.4247871495.979107844.132244>,
                               inet_tcp_dist,<0.288.0>,
                               #Ref<0.4247871495.979107844.132245>}
[error_logger:info,2020-04-02T21:10:42.786+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.288.0>,connection_closed}}
[ns_server:info,2020-04-02T21:10:42.926+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.281.0>:ns_port_server:log:224]ns_couchdb<0.281.0>: Apache CouchDB  (LogLevel=info) is starting.
ns_couchdb<0.281.0>: Failure to start Mochiweb: eaddrinuse
ns_couchdb<0.281.0>: 12910: Booted. Waiting for shutdown request
ns_couchdb<0.281.0>: [os_mon] memory supervisor port (memsup): Erlang has closed
ns_couchdb<0.281.0>: [os_mon] cpu supervisor port (cpu_sup): Erlang has closed

[ns_server:debug,2020-04-02T21:10:42.976+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T21:10:42.976+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:10:42.976+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.4247871495.979107844.132265>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:10:42.977+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.4247871495.979107844.132265>,
                                  inet_tcp_dist,<0.294.0>,
                                  #Ref<0.4247871495.979107844.132269>}
[ns_server:debug,2020-04-02T21:10:42.977+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.4247871495.979107844.132265>,
                               inet_tcp_dist,<0.294.0>,
                               #Ref<0.4247871495.979107844.132269>}
[ns_server:debug,2020-04-02T21:10:42.978+05:30,ns_1@127.0.0.1:<0.283.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-04-02T21:10:42.978+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.294.0>,shutdown}}
[error_logger:info,2020-04-02T21:10:42.978+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:10:43.179+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T21:10:43.179+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:10:43.179+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.4247871495.979107842.132074>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:10:43.179+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.4247871495.979107842.132074>,
                                  inet_tcp_dist,<0.297.0>,
                                  #Ref<0.4247871495.979107842.132078>}
[ns_server:debug,2020-04-02T21:10:43.180+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.4247871495.979107842.132074>,
                               inet_tcp_dist,<0.297.0>,
                               #Ref<0.4247871495.979107842.132078>}
[ns_server:debug,2020-04-02T21:10:43.181+05:30,ns_1@127.0.0.1:<0.283.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-04-02T21:10:43.181+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.297.0>,shutdown}}
[error_logger:info,2020-04-02T21:10:43.181+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-04-02T21:10:43.382+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:10:43.382+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T21:10:43.383+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.4247871495.979107842.132090>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:10:43.383+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.4247871495.979107842.132090>,
                                  inet_tcp_dist,<0.300.0>,
                                  #Ref<0.4247871495.979107841.132249>}
[ns_server:debug,2020-04-02T21:10:43.384+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.4247871495.979107842.132090>,
                               inet_tcp_dist,<0.300.0>,
                               #Ref<0.4247871495.979107841.132249>}
[error_logger:info,2020-04-02T21:10:43.384+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.300.0>,shutdown}}
[ns_server:debug,2020-04-02T21:10:43.385+05:30,ns_1@127.0.0.1:<0.283.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-04-02T21:10:43.385+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:info,2020-04-02T21:10:43.451+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.281.0>:ns_port_server:log:224]ns_couchdb<0.281.0>: {"Kernel pid terminated",application_controller,"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\"},{line,102}]},{supervisor,do_start_child,2,[{file,\"supervisor.erl\"},{line,365}]},{supervisor,start_children,3,[{file,\"supervisor.erl\"},{line,348}]},{supervisor,init_children,2,[{file,\"supervisor.erl\"},{line,314}]},{gen_server,init_it,2,[{file,\"gen_server.erl\"},{line,365}]},{gen_server,init_it,6,[{file,\"gen_server.erl\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}"}
ns_couchdb<0.281.0>: Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro
ns_couchdb<0.281.0>: 
ns_couchdb<0.281.0>: Crash dump is being written to: erl_crash.dump.1585842038.12600.ns_couchdb...done

[error_logger:error,2020-04-02T21:10:43.452+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]** Generic server ns_couchdb_port terminating 
** Last message in was {#Port<0.5115>,{exit_status,1}}
** When Server state == {state,#Port<0.5115>,
                            {ns_couchdb,"/opt/couchbase/lib/erlang/bin/erl",
                                ["-pa",
                                 "/opt/couchbase/lib/erlang/lib/asn1-5.0.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/compiler-7.1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEvent-2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEventDomain-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosFileTransfer-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosNotification-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosProperty-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTime-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTransactions-1.3.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/crypto-4.2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/dialyzer-3.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/diameter-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/edoc-0.9.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eldap-1.2.3.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_docgen-0.7.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_interface-3.10.2.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erts-9.3.3.9/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eunit-2.3.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/hipe-3.17.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ic-4.4.4.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/inets-6.5.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/mnesia-4.15.3.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/orber-3.8.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/os_mon-2.4.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/otp_mibs-1.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/parsetools-2.1.6/ebin",
                                 "/opt/couchbase/lib/erlang/lib/public_key-1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/reltool-0.7.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/runtime_tools-1.12.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/sasl-3.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/snmp-5.2.11/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssh-4.6.9.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssl-8.2.6.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/syntax_tools-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/tools-2.11.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/xmerl-1.3.16.1/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/gc-couchbase-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/vtree-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/wkb-1.2.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch-1.2.0a-961ad59-git/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_audit-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_dcp-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_index_merger-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_set_view-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_view_parser-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/ejson-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/erlang-oauth/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/etap/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/lhttpc-1.3/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mapreduce-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mochiweb-1.4.1/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/snappy-1.0.4/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ale/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/gen_smtp/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_babysitter/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_couchdb/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/ebin",
                                 "/opt/couchbase/lib/erlang/lib/stdlib-3.4.5.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/kernel-5.4.3.2/ebin",
                                 ".","-couch_ini",
                                 "/opt/couchbase/etc/couchdb/default.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/capi.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/geocouch.ini",
                                 "/opt/couchbase/etc/couchdb/local.ini",
                                 "-kernel","error_logger","false","-kernel",
                                 "error_logger","false","inetrc",
                                 "\"/opt/couchbase/etc/couchbase/hosts.cfg\"",
                                 "dist_config_file",
                                 "\"/opt/couchbase/var/lib/couchbase/config/dist_cfg\"",
                                 "-ssl_dist_optfile",
                                 "/opt/couchbase/etc/couchbase/ssl_dist_opts",
                                 "-setcookie",
                                 "2a158eb185476066b502c8ced82b23b042bd1ae5f1ef74f024e7bb9916796a9b",
                                 "-name","couchdb_ns_1@cb.local","-smp",
                                 "enable","+P","327680","+K","true","-kernel",
                                 "error_logger","false","-sasl",
                                 "sasl_error_logger","false","-nouser",
                                 "-hidden","-proto_dist","cb","-epmd_module",
                                 "cb_epmd","-start_epmd","false","-run",
                                 "child_erlang","child_start","ns_couchdb"],
                                [use_stdio,
                                 {env,
                                     [{"NS_COUCHDB_ENV_ARGS",
                                       "[{ns_server_node,'ns_1@127.0.0.1'},\n {path_config_tmpdir,\"/opt/couchbase/var/lib/couchbase/tmp\"},\n {net_kernel_verbosity,10},\n {loglevel_error_logger,debug},\n {path_config_libdir,\"/opt/couchbase/lib\"},\n {loglevel_stats,debug},\n {loglevel_menelaus,debug},\n {path_config_secdir,\"/opt/couchbase/etc/security\"},\n {loglevel_user,debug},\n {path_config_etcdir,\"/opt/couchbase/etc/couchbase\"},\n {loglevel_ns_server,debug},\n {loglevel_mapreduce_errors,debug},\n {loglevel_rebalance,debug},\n {loglevel_default,debug},\n {disk_sink_opts,[{rotation,[{compress,true},\n                             {size,41943040},\n                             {num_files,10},\n                             {buffer_size_max,52428800}]}]},\n {loglevel_cbas,debug},\n {loglevel_xdcr,debug},\n {loglevel_ns_doctor,debug},\n {loglevel_access,info},\n {error_logger_mf_dir,\"/opt/couchbase/var/lib/couchbase/logs\"},\n {path_config_datadir,\"/opt/couchbase/var/lib/couchbase\"},\n {loglevel_cluster,debug},\n {loglevel_couchdb,info},\n {loglevel_views,debug},\n {path_config_bindir,\"/opt/couchbase/bin\"}]"},
                                      {"ERL_CRASH_DUMP",
                                       "erl_crash.dump.1585842038.12600.ns_couchdb"}]}]},
                            {ringbuffer,1191,1024,
                                {[{<<"Crash dump is being written to: erl_crash.dump.1585842038.12600.ns_couchdb...done">>,
                                   81},
                                  {<<>>,0},
                                  {<<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                                   200}],
                                 [{<<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>,
                                   910}]}},
                            undefined,
                            {ok,{-576460749076,
                                 #Ref<0.4247871495.979107842.132081>}},
                            [<<"Crash dump is being written to: erl_crash.dump.1585842038.12600.ns_couchdb...done">>,
                             <<>>,
                             <<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                             <<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>],
                            0}
** Reason for termination == 
** {abnormal,1}

[ns_server:error,2020-04-02T21:10:43.456+05:30,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.282.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:189]ns_couchdb_port(<0.281.0>) died with reason {abnormal,1}
[ns_server:debug,2020-04-02T21:10:43.456+05:30,ns_1@127.0.0.1:<0.276.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.274.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:10:43.456+05:30,ns_1@127.0.0.1:<0.275.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.274.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:10:43.456+05:30,ns_1@127.0.0.1:<0.273.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.271.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:10:43.456+05:30,ns_1@127.0.0.1:<0.272.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.271.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:10:43.457+05:30,ns_1@127.0.0.1:<0.264.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.263.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:10:43.457+05:30,ns_1@127.0.0.1:<0.225.0>:restartable:shutdown_child:120]Successfully terminated process <0.226.0>
[ns_server:debug,2020-04-02T21:10:43.457+05:30,ns_1@127.0.0.1:<0.217.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.216.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:10:43.458+05:30,ns_1@127.0.0.1:<0.203.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.202.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:10:43.458+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:wait_saver:866]Done waiting for saver.
[error_logger:error,2020-04-02T21:10:43.461+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_port_server:init/1
    pid: <0.281.0>
    registered_name: ns_couchdb_port
    exception exit: {abnormal,1}
      in function  gen_server:handle_common_reply/8 (gen_server.erl, line 726)
    ancestors: [ns_server_nodes_sup,<0.208.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 1
    messages: [{'EXIT',#Port<0.5115>,normal}]
    links: [<0.209.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 11894
  neighbours:

[error_logger:error,2020-04-02T21:10:43.462+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: erlang:apply/2
    pid: <0.282.0>
    registered_name: wait_link_to_couchdb_node
    exception exit: {abnormal,1}
      in function  ns_server_nodes_sup:do_wait_link_to_couchdb_node/1 (src/ns_server_nodes_sup.erl, line 190)
    ancestors: [ns_server_nodes_sup,<0.208.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.209.0>,<0.283.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 3351
  neighbours:
    neighbour:
      pid: <0.283.0>
      registered_name: []
      initial call: ns_server_nodes_sup:'-do_wait_link_to_couchdb_node/1-fun-2-'/0
      current_function: {timer,sleep,1}
      ancestors: [wait_link_to_couchdb_node,ns_server_nodes_sup,<0.208.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
      message_queue_len: 0
      links: [<0.282.0>]
      trap_exit: false
      status: waiting
      heap_size: 2586
      stack_size: 12
      reductions: 10858
      current_stacktrace: [{timer,sleep,1,[{file,"timer.erl"},{line,153}]},
                  {misc,poll_for_condition_rec,3,
                      [{file,"src/misc.erl"},{line,508}]},
                  {ns_server_nodes_sup,
                      '-do_wait_link_to_couchdb_node/1-fun-2-',2,
                      [{file,"src/ns_server_nodes_sup.erl"},{line,159}]},
                  {proc_lib,init_p,3,[{file,"proc_lib.erl"},{line,232}]}]

[error_logger:error,2020-04-02T21:10:43.463+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    start_error
     Reason:     {abnormal,1}
     Offender:   [{pid,undefined},
                  {name,wait_for_couchdb_node},
                  {mfargs,{erlang,apply,
                                  [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:error,2020-04-02T21:10:43.463+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    shutdown_error
     Reason:     {abnormal,1}
     Offender:   [{pid,<0.281.0>},
                  {name,start_couchdb_node},
                  {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                  {restart_type,{permanent,5}},
                  {shutdown,86400000},
                  {child_type,worker}]


[error_logger:error,2020-04-02T21:10:43.463+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_cluster_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,wait_for_couchdb_node,
                         {abnormal,1}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_nodes_sup},
                  {mfargs,
                      {restartable,start_link,
                          [{ns_server_nodes_sup,start_link,[]},infinity]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-04-02T21:10:43.464+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,root_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                             {failed_to_start_child,wait_for_couchdb_node,
                                 {abnormal,1}}}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_cluster_sup},
                  {mfargs,{ns_server_cluster_sup,start_link,[]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-04-02T21:10:43.465+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: application_master:init/4
    pid: <0.117.0>
    registered_name: []
    exception exit: {{shutdown,
                      {failed_to_start_child,ns_server_cluster_sup,
                       {shutdown,
                        {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                          {failed_to_start_child,wait_for_couchdb_node,
                           {abnormal,1}}}}}}},
                     {ns_server,start,[normal,[]]}}
      in function  application_master:init/4 (application_master.erl, line 134)
    ancestors: [<0.116.0>]
    message_queue_len: 1
    messages: [{'EXIT',<0.118.0>,normal}]
    links: [<0.116.0>,<0.33.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 274
  neighbours:

[error_logger:info,2020-04-02T21:10:43.465+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
         application: ns_server
              exited: {{shutdown,
                        {failed_to_start_child,ns_server_cluster_sup,
                         {shutdown,
                          {failed_to_start_child,ns_server_nodes_sup,
                           {shutdown,
                            {failed_to_start_child,wait_for_couchdb_node,
                             {abnormal,1}}}}}}},
                       {ns_server,start,[normal,[]]}}
                type: permanent

[error_logger:info,2020-04-02T21:10:43.466+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,system_memory_high_watermark}

[error_logger:info,2020-04-02T21:10:43.466+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-calculator/544"}}

[error_logger:info,2020-04-02T21:10:43.466+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/8935"}}

[error_logger:info,2020-04-02T21:10:43.467+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-calculator/704"}}

[error_logger:info,2020-04-02T21:10:43.467+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/59"}}

[error_logger:info,2020-04-02T21:10:43.467+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/pycharm-community/188"}}

[error_logger:info,2020-04-02T21:10:43.467+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gnome-system-monitor/127"}}

[error_logger:info,2020-04-02T21:10:43.468+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core18/1705"}}

[error_logger:info,2020-04-02T21:10:43.468+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/8689"}}

[error_logger:info,2020-04-02T21:10:43.468+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gnome-system-monitor/135"}}

[error_logger:info,2020-04-02T21:10:43.468+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-characters/495"}}

[error_logger:info,2020-04-02T21:10:43.468+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-28-1804/116"}}

[error_logger:info,2020-04-02T21:10:43.468+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gtk-common-themes/1474"}}

[error_logger:info,2020-04-02T21:10:43.468+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core18/1668"}}

[error_logger:info,2020-04-02T21:10:43.469+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-logs/81"}}

[error_logger:info,2020-04-02T21:10:43.469+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-logs/93"}}

[error_logger:info,2020-04-02T21:10:43.469+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/98"}}

[error_logger:info,2020-04-02T21:10:43.469+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-characters/399"}}

[error_logger:info,2020-04-02T21:10:43.469+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gtk-common-themes/1440"}}

[ns_server:info,2020-04-02T21:10:50.751+05:30,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-04-02T21:10:50.763+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-04-02T21:10:50.763+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.763+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.763+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.763+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.763+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.763+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.763+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.763+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.763+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.763+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.763+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.763+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.764+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.764+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.764+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.764+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.764+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.764+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.764+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.764+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.764+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.764+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.764+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.764+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.764+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-04-02T21:10:50.764+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-04-02T21:10:50.768+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-04-02T21:10:50.768+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[error_logger:info,2020-04-02T21:10:50.769+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.168.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2020-04-02T21:10:50.769+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.167.0>},
                       {id,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:10:50.771+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@127.0.0.1'
[error_logger:info,2020-04-02T21:10:50.779+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.779+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.779+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:50.781+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.174.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.782+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.176.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T21:10:50.782+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2020-04-02T21:10:50.782+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.177.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:50.782+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.175.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:50.782+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:50.783+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.783+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.784+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.180.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:10:50.785+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-04-02T21:10:50.785+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-04-02T21:10:50.786+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2020-04-02T21:10:50.787+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-04-02T21:10:50.787+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.181.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.787+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.169.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-04-02T21:10:50.788+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-04-02T21:10:50.790+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-04-02T21:10:50.790+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2020-04-02T21:10:50.790+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:10:50.790+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T21:10:50.791+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2948281080.3395551233.63554>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:10:50.791+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2948281080.3395551233.63554>,
                                  inet_tcp_dist,<0.185.0>,
                                  #Ref<0.2948281080.3395551233.63559>}
[ns_server:debug,2020-04-02T21:10:50.795+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-04-02T21:10:50.795+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2020-04-02T21:10:50.795+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-04-02T21:10:50.800+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-04-02T21:10:50.801+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.804+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:10:50.805+05:30,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,4,2},{21,10,50}}},
               {memory,
                   [{total,26301712},
                    {processes,9514792},
                    {processes_used,9509328},
                    {system,16786920},
                    {atom,388625},
                    {atom_used,364409},
                    {binary,107424},
                    {code,8250921},
                    {ets,1509192}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    error_handler,error_logger,gen_server,application,file,
                    proc_lib,supervisor,file_io_server,code,file_server,heart,
                    application_master,kernel,erl_eval,filename,gen_event,gen,
                    application_controller,lists,code_server,erl_lint,ets,
                    erl_parse,erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{os_mon,"CPO  CXC 138 46","2.4.4"},
                    {sasl,"SASL  CXC 138 11","3.1.2"},
                    {ns_server,"Couchbase server","6.5.0-4960-enterprise"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ale,"Another Logger for Erlang","0.0.0"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,131},
               {node,'ns_1@127.0.0.1'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,auth,httpd_sup,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    release_handler,lhttpc_sup,httpc_sup,lhttpc_manager,
                    alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',kernel_safe_sup,
                    inets_sup,'sink-disk_access_int','sink-disk_access',
                    standard_error,'sink-disk_reports',ale_stats_events,
                    'sink-disk_stats','sink-disk_xdcr',timer_server,
                    'sink-disk_debug',inet_gethost_native,ale_sup,
                    'sink-disk_error',inet_db,'sink-disk_default',
                    ssl_pem_cache_dist,ale_dynamic_sup,rex,global_group,
                    net_sup,kernel_sup,ssl_connection_sup,global_name_server,
                    ssl_admin_sup,tftp_sup,ssl_sup,root_sup,erts_code_purger,
                    os_mon_sup,file_server_2,error_logger,cpu_sup,erl_epmd,
                    init,memsup,erl_signal_server,disksup,ale,net_kernel,
                    dist_manager,ssl_pem_cache,ssl_manager,ssl_dist_admin_sup,
                    ssl_dist_connection_sup,ssl_dist_sup,user,
                    ssl_tls_dist_proxy,ssl_manager_dist,sasl_safe_sup,
                    ssl_listen_tracker_sup,inet_gethost_native_sup,
                    code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,0}]
[ns_server:info,2020-04-02T21:10:50.808+05:30,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"cfa0f75f28402d2e1aa254b2a374bead19433526\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" upstream=\"master\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"f2a16b53bb74146f20d18ba2c0443d5f10a9a550\" upstream=\"master\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4960\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" upstream=\"master\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" upstream=\"mad-hatter\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" upstream=\"master\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" upstream=\"master\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" upstream=\"master\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" upstream=\"mad-hatter\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" upstream=\"cb-master\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" upstream=\"master\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" upstream=\"master\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" upstream=\"mad-hatter\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" upstream=\"mad-hatter\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" upstream=\"mad-hatter\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" upstream=\"master\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" upstream=\"master\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" upstream=\"mad-hatter\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" upstream=\"mad-hatter\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" upstream=\"mad-hatter\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" upstream=\"mad-hatter\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" upstream=\"master\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" upstream=\"master\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" upstream=\"master\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" upstream=\"mad-hatter\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" upstream=\"master\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"7a3aa547b3f5eb3ea28d279a08384609cd2cea7c\" upstream=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" upstream=\"mad-hatter\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" upstream=\"master\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" upstream=\"master\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" upstream=\"master\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" upstream=\"v1.1.0-couchbase\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"ee64d41320d14fabe814a241a5cf4f6a6f6e827a\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" upstream=\"master\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" upstream=\"master\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-04-02T21:10:50.811+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.811+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.812+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:10:50.812+05:30,ns_1@127.0.0.1:ns_config_sup<0.192.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-04-02T21:10:50.812+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.812+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.194.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:10:50.827+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-04-02T21:10:50.828+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2020-04-02T21:10:50.835+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[{cert_and_pkey,
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFgIK71cHor8wDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBkZTZmMzM0MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgZGU2ZjMz\nNDAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC6Epk+5C0GfEqGHL9d\nxsySLywt3gLcVQmCM8lgcMRGWDaGVF6iOP+QyLODyB09I5u2gOcVm+1r3eOZ4rwk\nbttVmFIsdroNf2jG+9baY4LqKoDyZnjZr0LeolUcY+0eYI68oNwRMgWp53Krm861\ny11yyOEjefm+JBDhZuZpHZegjTBKtYqZd96WZwOzbrJZrau3uKBuQTmoEdpZ4VdX\n6U5nzUaRkvjjuBpQyeqMLSuuLUO4FENp1C8P9fYhy4Y6RRZfMSBGdyw1d8QEWxiU\n4n/rtfQgiN32qOwtY7ocwvaXDV7wH1ipWkPF5Vn8eyBi5cA2xqgaq1xSBLD8MUHE\nXTAjAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCP9ajveEq01YMq\n/zClEAjE3TCbGqz9u/vjXdhSQK7rPJLcK250d86L6njzkS2ffrabbOGON+4UvNW4\nTUub3JqnTuSlI8B6riH61kqWPfCfRC392v1xAIaQI1/jWsW4HQoiXbmi0uiKrsEq\nIt8XF5nLXDsEeWYetynrODdVU9ADeDNkE2+AOyLTvD/4eUDRoQhDhC5vh75Bu9gm\nEV+efNKCwXjs4xAMPGbKoNnWBkx7Btn0+iyI19l+jrzF1rlDaH6pFz2ldqm6CL+f\n26ZCU9S8uXPNC7UiNXr6DZj1sn/k0qqebDRnHlO2P+wYp5G/+Rca+B41diWCV7xG\ncnfTf1PH\n-----END CERTIFICATE-----\n">>,
     <<"*****">>}]},
  {{node,'ns_1@127.0.0.1',erl_external_listeners},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {inet,false},
    {inet6,false}]},
  {{node,'ns_1@127.0.0.1',node_encryption},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    false]},
  {{node,'ns_1@127.0.0.1',address_family},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    inet]},
  {alert_limits,
   [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
  {audit,
   [{auditd_enabled,false},
    {rotate_interval,86400},
    {rotate_size,20971520},
    {disabled,[]},
    {sync,[]},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
  {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
  {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
  {autocompaction,
   [{database_fragmentation_threshold,{30,undefined}},
    {view_fragmentation_threshold,{30,undefined}}]},
  {buckets,[{configs,[]}]},
  {cbas_memory_quota,2174},
  {drop_request_memory_threshold_mib,undefined},
  {email_alerts,
   [{recipients,["root@localhost"]},
    {sender,"couchbase@localhost"},
    {enabled,false},
    {email_server,
     [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
    {alerts,
     [auto_failover_node,auto_failover_maximum_reached,
      auto_failover_other_nodes_down,auto_failover_cluster_too_small,
      auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
      ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
      ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
  {fts_memory_quota,512},
  {index_aware_rebalance_disabled,false},
  {log_redaction_default_cfg,[{redact_level,none}]},
  {max_bucket_count,30},
  {memcached,[]},
  {memory_quota,8886},
  {nodes_wanted,['ns_1@127.0.0.1']},
  {password_policy,[{min_length,6},{must_present,[]}]},
  {quorum_nodes,['ns_1@127.0.0.1']},
  {remote_clusters,[]},
  {replication,[{enabled,true}]},
  {rest,[{port,8091}]},
  {rest_creds,null},
  {secure_headers,[]},
  {server_groups,
   [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
  {set_view_update_daemon,
   [{update_interval,5000},
    {update_min_changes,5000},
    {replica_update_min_changes,5000}]},
  {{couchdb,max_parallel_indexers},4},
  {{couchdb,max_parallel_replica_indexers},2},
  {{metakv,<<"/indexing/settings/config">>},
   <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
  {{request_limit,capi},undefined},
  {{request_limit,rest},undefined},
  {{node,'ns_1@127.0.0.1',audit},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
  {{node,'ns_1@127.0.0.1',capi_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8092]},
  {{node,'ns_1@127.0.0.1',cbas_admin_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9110]},
  {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9113]},
  {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9112]},
  {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9111]},
  {{node,'ns_1@127.0.0.1',cbas_cluster_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9115]},
  {{node,'ns_1@127.0.0.1',cbas_console_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9114]},
  {{node,'ns_1@127.0.0.1',cbas_data_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9116]},
  {{node,'ns_1@127.0.0.1',cbas_debug_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    -1]},
  {{node,'ns_1@127.0.0.1',cbas_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8095]},
  {{node,'ns_1@127.0.0.1',cbas_messaging_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9118]},
  {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9119]},
  {{node,'ns_1@127.0.0.1',cbas_metadata_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9121]},
  {{node,'ns_1@127.0.0.1',cbas_parent_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9122]},
  {{node,'ns_1@127.0.0.1',cbas_replication_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9120]},
  {{node,'ns_1@127.0.0.1',cbas_result_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9117]},
  {{node,'ns_1@127.0.0.1',cbas_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18095]},
  {{node,'ns_1@127.0.0.1',compaction_daemon},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@127.0.0.1',config_version},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    {6,5}]},
  {{node,'ns_1@127.0.0.1',eventing_debug_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9140]},
  {{node,'ns_1@127.0.0.1',eventing_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8096]},
  {{node,'ns_1@127.0.0.1',eventing_https_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18096]},
  {{node,'ns_1@127.0.0.1',fts_grpc_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9130]},
  {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    19130]},
  {{node,'ns_1@127.0.0.1',fts_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8094]},
  {{node,'ns_1@127.0.0.1',fts_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18094]},
  {{node,'ns_1@127.0.0.1',indexer_admin_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9100]},
  {{node,'ns_1@127.0.0.1',indexer_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9102]},
  {{node,'ns_1@127.0.0.1',indexer_https_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    19102]},
  {{node,'ns_1@127.0.0.1',indexer_scan_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9101]},
  {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9104]},
  {{node,'ns_1@127.0.0.1',indexer_stinit_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9103]},
  {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9105]},
  {{node,'ns_1@127.0.0.1',is_enterprise},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    true]},
  {{node,'ns_1@127.0.0.1',isasl},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@127.0.0.1',membership},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    active]},
  {{node,'ns_1@127.0.0.1',memcached},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {port,11210},
    {dedicated_port,11209},
    {dedicated_ssl_port,11206},
    {ssl_port,11207},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_sleeptime,19},
    {log_rotation_period,39003}]},
  {{node,'ns_1@127.0.0.1',memcached_config},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    {[{interfaces,
       {memcached_config_mgr,omit_missing_mcd_ports,
        [{[{host,<<"*">>},
           {port,port},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_port},
           {system,true},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,ssl_port},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_ssl_port},
           {system,true},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
      {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
      {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {opentracing,
       {[{enabled,opentracing_enabled},
         {module,{"~s",[opentracing_module]}},
         {config,{"~s",[opentracing_config]}}]}},
      {admin,{"~s",[admin_user]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize},
         {sleeptime,log_sleeptime}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
  {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    11206]},
  {{node,'ns_1@127.0.0.1',memcached_defaults},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {max_connections,65000},
    {system_connections,5000},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {opentracing_enabled,false},
    {opentracing_module,[]},
    {opentracing_config,[]},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {tracing_enabled,true},
    {datatype_snappy,true},
    {num_reader_threads,<<"default">>},
    {num_writer_threads,<<"default">>}]},
  {{node,'ns_1@127.0.0.1',moxi},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {port,0}]},
  {{node,'ns_1@127.0.0.1',ns_log},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@127.0.0.1',port_servers},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
  {{node,'ns_1@127.0.0.1',projector_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9999]},
  {{node,'ns_1@127.0.0.1',projector_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9999]},
  {{node,'ns_1@127.0.0.1',query_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8093]},
  {{node,'ns_1@127.0.0.1',rest},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@127.0.0.1',saslauthd_enabled},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    true]},
  {{node,'ns_1@127.0.0.1',ssl_capi_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18092]},
  {{node,'ns_1@127.0.0.1',ssl_query_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18093]},
  {{node,'ns_1@127.0.0.1',ssl_rest_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18091]},
  {{node,'ns_1@127.0.0.1',uuid},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    <<"8c43a5102cad1e34db659ab4d5646878">>]},
  {{node,'ns_1@127.0.0.1',xdcr_rest_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9998]},
  {{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    false]},
  {{local_changes_count,<<"8c43a5102cad1e34db659ab4d5646878">>},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{2,63753061241}}]}]}]]
[ns_server:info,2020-04-02T21:10:50.839+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{local_changes_count,<<"8c43a5102cad1e34db659ab4d5646878">>},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{2,63753061241}}]}]},
 {{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',xdcr_rest_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9998]},
 {{node,'ns_1@127.0.0.1',uuid},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   <<"8c43a5102cad1e34db659ab4d5646878">>]},
 {{node,'ns_1@127.0.0.1',ssl_rest_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18091]},
 {{node,'ns_1@127.0.0.1',ssl_query_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18093]},
 {{node,'ns_1@127.0.0.1',ssl_capi_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18092]},
 {{node,'ns_1@127.0.0.1',saslauthd_enabled},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',rest},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@127.0.0.1',query_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8093]},
 {{node,'ns_1@127.0.0.1',projector_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',projector_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',port_servers},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
 {{node,'ns_1@127.0.0.1',ns_log},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@127.0.0.1',moxi},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {port,0}]},
 {{node,'ns_1@127.0.0.1',memcached_defaults},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   11206]},
 {{node,'ns_1@127.0.0.1',memcached_config},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@127.0.0.1',memcached},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@127.0.0.1',membership},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   active]},
 {{node,'ns_1@127.0.0.1',isasl},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@127.0.0.1',is_enterprise},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9105]},
 {{node,'ns_1@127.0.0.1',indexer_stinit_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9103]},
 {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9104]},
 {{node,'ns_1@127.0.0.1',indexer_scan_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9101]},
 {{node,'ns_1@127.0.0.1',indexer_https_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   19102]},
 {{node,'ns_1@127.0.0.1',indexer_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9102]},
 {{node,'ns_1@127.0.0.1',indexer_admin_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9100]},
 {{node,'ns_1@127.0.0.1',fts_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18094]},
 {{node,'ns_1@127.0.0.1',fts_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8094]},
 {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   19130]},
 {{node,'ns_1@127.0.0.1',fts_grpc_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9130]},
 {{node,'ns_1@127.0.0.1',eventing_https_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18096]},
 {{node,'ns_1@127.0.0.1',eventing_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8096]},
 {{node,'ns_1@127.0.0.1',eventing_debug_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9140]},
 {{node,'ns_1@127.0.0.1',config_version},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   {6,5}]},
 {{node,'ns_1@127.0.0.1',compaction_daemon},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@127.0.0.1',cbas_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18095]},
 {{node,'ns_1@127.0.0.1',cbas_result_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9117]},
 {{node,'ns_1@127.0.0.1',cbas_replication_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9120]},
 {{node,'ns_1@127.0.0.1',cbas_parent_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9122]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9121]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9119]},
 {{node,'ns_1@127.0.0.1',cbas_messaging_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9118]},
 {{node,'ns_1@127.0.0.1',cbas_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8095]},
 {{node,'ns_1@127.0.0.1',cbas_debug_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|-1]},
 {{node,'ns_1@127.0.0.1',cbas_data_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9116]},
 {{node,'ns_1@127.0.0.1',cbas_console_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9114]},
 {{node,'ns_1@127.0.0.1',cbas_cluster_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9115]},
 {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9111]},
 {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9112]},
 {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9113]},
 {{node,'ns_1@127.0.0.1',cbas_admin_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9110]},
 {{node,'ns_1@127.0.0.1',capi_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8092]},
 {{node,'ns_1@127.0.0.1',audit},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
 {{request_limit,rest},undefined},
 {{request_limit,capi},undefined},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
 {secure_headers,[]},
 {rest_creds,null},
 {rest,[{port,8091}]},
 {replication,[{enabled,true}]},
 {remote_clusters,[]},
 {quorum_nodes,['ns_1@127.0.0.1']},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {nodes_wanted,['ns_1@127.0.0.1']},
 {memory_quota,8886},
 {memcached,[]},
 {max_bucket_count,30},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {index_aware_rebalance_disabled,false},
 {fts_memory_quota,512},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {drop_request_memory_threshold_mib,undefined},
 {cbas_memory_quota,2174},
 {buckets,[{configs,[]}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {{node,'ns_1@127.0.0.1',address_family},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   inet]},
 {{node,'ns_1@127.0.0.1',node_encryption},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',erl_external_listeners},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {inet,false},
   {inet6,false}]},
 {cert_and_pkey,
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFgIK71cHor8wDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBkZTZmMzM0MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgZGU2ZjMz\nNDAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC6Epk+5C0GfEqGHL9d\nxsySLywt3gLcVQmCM8lgcMRGWDaGVF6iOP+QyLODyB09I5u2gOcVm+1r3eOZ4rwk\nbttVmFIsdroNf2jG+9baY4LqKoDyZnjZr0LeolUcY+0eYI68oNwRMgWp53Krm861\ny11yyOEjefm+JBDhZuZpHZegjTBKtYqZd96WZwOzbrJZrau3uKBuQTmoEdpZ4VdX\n6U5nzUaRkvjjuBpQyeqMLSuuLUO4FENp1C8P9fYhy4Y6RRZfMSBGdyw1d8QEWxiU\n4n/rtfQgiN32qOwtY7ocwvaXDV7wH1ipWkPF5Vn8eyBi5cA2xqgaq1xSBLD8MUHE\nXTAjAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCP9ajveEq01YMq\n/zClEAjE3TCbGqz9u/vjXdhSQK7rPJLcK250d86L6njzkS2ffrabbOGON+4UvNW4\nTUub3JqnTuSlI8B6riH61kqWPfCfRC392v1xAIaQI1/jWsW4HQoiXbmi0uiKrsEq\nIt8XF5nLXDsEeWYetynrODdVU9ADeDNkE2+AOyLTvD/4eUDRoQhDhC5vh75Bu9gm\nEV+efNKCwXjs4xAMPGbKoNnWBkx7Btn0+iyI19l+jrzF1rlDaH6pFz2ldqm6CL+f\n26ZCU9S8uXPNC7UiNXr6DZj1sn/k0qqebDRnHlO2P+wYp5G/+Rca+B41diWCV7xG\ncnfTf1PH\n-----END CERTIFICATE-----\n">>,
    <<"*****">>}]}]
[error_logger:info,2020-04-02T21:10:50.843+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.195.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.844+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.201.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.845+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.202.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.845+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T21:10:50.846+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"8c43a5102cad1e34db659ab4d5646878">>} ->
[{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{3,63753061250}}]}]
[error_logger:info,2020-04-02T21:10:50.846+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.204.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:50.847+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.207.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:50.852+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:10:50.853+05:30,ns_1@127.0.0.1:menelaus_barrier<0.211.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-04-02T21:10:50.853+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.853+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.854+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.213.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.855+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.215.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:10:50.863+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2020-04-02T21:10:50.863+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2020-04-02T21:10:50.863+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2020-04-02T21:10:50.875+05:30,ns_1@127.0.0.1:ns_ssl_services_setup<0.216.0>:ns_ssl_services_setup:init:462]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,2,10,239,87,7,162,
             191,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
             6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
             118,101,114,32,100,101,54,102,51,51,52,48,48,30,23,13,49,51,48,
             49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
             57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
             104,98,97,115,101,32,83,101,114,118,101,114,32,100,101,54,102,
             51,51,52,48,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,
             0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,186,18,153,62,228,45,6,
             124,74,134,28,191,93,198,204,146,47,44,45,222,2,220,85,9,130,51,
             201,96,112,196,70,88,54,134,84,94,162,56,255,144,200,179,131,
             200,29,61,35,155,182,128,231,21,155,237,107,221,227,153,226,188,
             36,110,219,85,152,82,44,118,186,13,127,104,198,251,214,218,99,
             130,234,42,128,242,102,120,217,175,66,222,162,85,28,99,237,30,
             96,142,188,160,220,17,50,5,169,231,114,171,155,206,181,203,93,
             114,200,225,35,121,249,190,36,16,225,102,230,105,29,151,160,141,
             48,74,181,138,153,119,222,150,103,3,179,110,178,89,173,171,183,
             184,160,110,65,57,168,17,218,89,225,87,87,233,78,103,205,70,145,
             146,248,227,184,26,80,201,234,140,45,43,174,45,67,184,20,67,105,
             212,47,15,245,246,33,203,134,58,69,22,95,49,32,70,119,44,53,119,
             196,4,91,24,148,226,127,235,181,244,32,136,221,246,168,236,45,
             99,186,28,194,246,151,13,94,240,31,88,169,90,67,197,229,89,252,
             123,32,98,229,192,54,198,168,26,171,92,82,4,176,252,49,65,196,
             93,48,35,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,255,4,4,
             3,2,2,164,48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,
             15,6,3,85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,134,72,
             134,247,13,1,1,11,5,0,3,130,1,1,0,143,245,168,239,120,74,180,
             213,131,42,255,48,165,16,8,196,221,48,155,26,172,253,187,251,
             227,93,216,82,64,174,235,60,146,220,43,110,116,119,206,139,234,
             120,243,145,45,159,126,182,155,108,225,142,55,238,20,188,213,
             184,77,75,155,220,154,167,78,228,165,35,192,122,174,33,250,214,
             74,150,61,240,159,68,45,253,218,253,113,0,134,144,35,95,227,90,
             197,184,29,10,34,93,185,162,210,232,138,174,193,42,34,223,23,23,
             153,203,92,59,4,121,102,30,183,41,235,56,55,85,83,208,3,120,51,
             100,19,111,128,59,34,211,188,63,248,121,64,209,161,8,67,132,46,
             111,135,190,65,187,216,38,17,95,158,124,210,130,193,120,236,227,
             16,12,60,102,202,160,217,214,6,76,123,6,217,244,250,44,136,215,
             217,126,142,188,197,214,185,67,104,126,169,23,61,165,118,169,
             186,8,191,159,219,166,66,83,212,188,185,115,205,11,181,34,53,
             122,250,13,152,245,178,127,228,210,170,158,108,52,103,30,83,182,
             63,236,24,167,145,191,249,23,26,248,30,53,118,37,130,87,188,70,
             114,119,211,127,83,199>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2020-04-02T21:10:50.876+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.216.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:10:50.884+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-04-02T21:10:50.884+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-04-02T21:10:50.884+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-04-02T21:10:50.884+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:info,2020-04-02T21:10:50.894+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-04-02T21:10:50.894+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-04-02T21:10:50.894+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-04-02T21:10:50.894+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-04-02T21:10:50.893+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.219.0>,menelaus_web}
             started: [{pid,<0.220.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 2,10,239,87,7,162,191,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,100,101,54,
                                 102,51,51,52,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,100,
                                 101,54,102,51,51,52,48,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,186,18,153,62,228,
                                 45,6,124,74,134,28,191,93,198,204,146,47,44,
                                 45,222,2,220,85,9,130,51,201,96,112,196,70,
                                 88,54,134,84,94,162,56,255,144,200,179,131,
                                 200,29,61,35,155,182,128,231,21,155,237,107,
                                 221,227,153,226,188,36,110,219,85,152,82,44,
                                 118,186,13,127,104,198,251,214,218,99,130,
                                 234,42,128,242,102,120,217,175,66,222,162,
                                 85,28,99,237,30,96,142,188,160,220,17,50,5,
                                 169,231,114,171,155,206,181,203,93,114,200,
                                 225,35,121,249,190,36,16,225,102,230,105,29,
                                 151,160,141,48,74,181,138,153,119,222,150,
                                 103,3,179,110,178,89,173,171,183,184,160,
                                 110,65,57,168,17,218,89,225,87,87,233,78,
                                 103,205,70,145,146,248,227,184,26,80,201,
                                 234,140,45,43,174,45,67,184,20,67,105,212,
                                 47,15,245,246,33,203,134,58,69,22,95,49,32,
                                 70,119,44,53,119,196,4,91,24,148,226,127,
                                 235,181,244,32,136,221,246,168,236,45,99,
                                 186,28,194,246,151,13,94,240,31,88,169,90,
                                 67,197,229,89,252,123,32,98,229,192,54,198,
                                 168,26,171,92,82,4,176,252,49,65,196,93,48,
                                 35,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,
                                 15,1,1,255,4,4,3,2,2,164,48,19,6,3,85,29,37,
                                 4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,
                                 85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,
                                 42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 143,245,168,239,120,74,180,213,131,42,255,
                                 48,165,16,8,196,221,48,155,26,172,253,187,
                                 251,227,93,216,82,64,174,235,60,146,220,43,
                                 110,116,119,206,139,234,120,243,145,45,159,
                                 126,182,155,108,225,142,55,238,20,188,213,
                                 184,77,75,155,220,154,167,78,228,165,35,192,
                                 122,174,33,250,214,74,150,61,240,159,68,45,
                                 253,218,253,113,0,134,144,35,95,227,90,197,
                                 184,29,10,34,93,185,162,210,232,138,174,193,
                                 42,34,223,23,23,153,203,92,59,4,121,102,30,
                                 183,41,235,56,55,85,83,208,3,120,51,100,19,
                                 111,128,59,34,211,188,63,248,121,64,209,161,
                                 8,67,132,46,111,135,190,65,187,216,38,17,95,
                                 158,124,210,130,193,120,236,227,16,12,60,
                                 102,202,160,217,214,6,76,123,6,217,244,250,
                                 44,136,215,217,126,142,188,197,214,185,67,
                                 104,126,169,23,61,165,118,169,186,8,191,159,
                                 219,166,66,83,212,188,185,115,205,11,181,34,
                                 53,122,250,13,152,245,178,127,228,210,170,
                                 158,108,52,103,30,83,182,63,236,24,167,145,
                                 191,249,23,26,248,30,53,118,37,130,87,188,
                                 70,114,119,211,127,83,199>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:10:50.895+05:30,ns_1@127.0.0.1:<0.218.0>:restartable:start_child:98]Started child process <0.219.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2020-04-02T21:10:50.895+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.219.0>,menelaus_web}
             started: [{pid,<0.238.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 2,10,239,87,7,162,191,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,100,101,54,
                                 102,51,51,52,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,100,
                                 101,54,102,51,51,52,48,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,186,18,153,62,228,
                                 45,6,124,74,134,28,191,93,198,204,146,47,44,
                                 45,222,2,220,85,9,130,51,201,96,112,196,70,
                                 88,54,134,84,94,162,56,255,144,200,179,131,
                                 200,29,61,35,155,182,128,231,21,155,237,107,
                                 221,227,153,226,188,36,110,219,85,152,82,44,
                                 118,186,13,127,104,198,251,214,218,99,130,
                                 234,42,128,242,102,120,217,175,66,222,162,
                                 85,28,99,237,30,96,142,188,160,220,17,50,5,
                                 169,231,114,171,155,206,181,203,93,114,200,
                                 225,35,121,249,190,36,16,225,102,230,105,29,
                                 151,160,141,48,74,181,138,153,119,222,150,
                                 103,3,179,110,178,89,173,171,183,184,160,
                                 110,65,57,168,17,218,89,225,87,87,233,78,
                                 103,205,70,145,146,248,227,184,26,80,201,
                                 234,140,45,43,174,45,67,184,20,67,105,212,
                                 47,15,245,246,33,203,134,58,69,22,95,49,32,
                                 70,119,44,53,119,196,4,91,24,148,226,127,
                                 235,181,244,32,136,221,246,168,236,45,99,
                                 186,28,194,246,151,13,94,240,31,88,169,90,
                                 67,197,229,89,252,123,32,98,229,192,54,198,
                                 168,26,171,92,82,4,176,252,49,65,196,93,48,
                                 35,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,
                                 15,1,1,255,4,4,3,2,2,164,48,19,6,3,85,29,37,
                                 4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,
                                 85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,
                                 42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 143,245,168,239,120,74,180,213,131,42,255,
                                 48,165,16,8,196,221,48,155,26,172,253,187,
                                 251,227,93,216,82,64,174,235,60,146,220,43,
                                 110,116,119,206,139,234,120,243,145,45,159,
                                 126,182,155,108,225,142,55,238,20,188,213,
                                 184,77,75,155,220,154,167,78,228,165,35,192,
                                 122,174,33,250,214,74,150,61,240,159,68,45,
                                 253,218,253,113,0,134,144,35,95,227,90,197,
                                 184,29,10,34,93,185,162,210,232,138,174,193,
                                 42,34,223,23,23,153,203,92,59,4,121,102,30,
                                 183,41,235,56,55,85,83,208,3,120,51,100,19,
                                 111,128,59,34,211,188,63,248,121,64,209,161,
                                 8,67,132,46,111,135,190,65,187,216,38,17,95,
                                 158,124,210,130,193,120,236,227,16,12,60,
                                 102,202,160,217,214,6,76,123,6,217,244,250,
                                 44,136,215,217,126,142,188,197,214,185,67,
                                 104,126,169,23,61,165,118,169,186,8,191,159,
                                 219,166,66,83,212,188,185,115,205,11,181,34,
                                 53,122,250,13,152,245,178,127,228,210,170,
                                 158,108,52,103,30,83,182,63,236,24,167,145,
                                 191,249,23,26,248,30,53,118,37,130,87,188,
                                 70,114,119,211,127,83,199>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.896+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.218.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.896+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.214.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:50.900+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.256.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.900+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.259.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.903+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.261.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:10:50.904+05:30,ns_1@127.0.0.1:users_replicator<0.261.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-04-02T21:10:50.905+05:30,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.261.0>
[ns_server:debug,2020-04-02T21:10:50.905+05:30,ns_1@127.0.0.1:users_replicator<0.261.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.262.0>
[ns_server:debug,2020-04-02T21:10:50.906+05:30,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-04-02T21:10:50.906+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.262.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.906+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.260.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T21:10:50.910+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-04-02T21:10:50.910+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.264.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.911+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.267.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.911+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.258.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:50.912+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.270.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:50.912+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.271.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:10:50.919+05:30,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-04-02T21:10:50.919+05:30,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 13ms
[ns_server:debug,2020-04-02T21:10:50.921+05:30,ns_1@127.0.0.1:users_replicator<0.261.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[ns_server:debug,2020-04-02T21:10:50.922+05:30,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.275.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[error_logger:info,2020-04-02T21:10:50.922+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.274.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:50.922+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:10:50.922+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T21:10:50.922+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2948281080.3395551233.63615>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:10:50.922+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2948281080.3395551233.63615>,
                                  inet_tcp_dist,<0.278.0>,
                                  #Ref<0.2948281080.3395551233.63616>}
[ns_server:debug,2020-04-02T21:10:50.922+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2948281080.3395551233.63615>,
                               inet_tcp_dist,<0.278.0>,
                               #Ref<0.2948281080.3395551233.63616>}
[error_logger:info,2020-04-02T21:10:50.922+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.278.0>,shutdown}}
[error_logger:info,2020-04-02T21:10:50.922+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:10:50.922+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-04-02T21:10:51.123+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T21:10:51.123+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:10:51.123+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2948281080.3395551235.63267>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:10:51.123+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2948281080.3395551235.63267>,
                                  inet_tcp_dist,<0.281.0>,
                                  #Ref<0.2948281080.3395551235.63271>}
[ns_server:debug,2020-04-02T21:10:51.152+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-04-02T21:10:51.353+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-04-02T21:10:51.594+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.285.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:10:51.642+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-04-02T21:10:51.651+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2948281080.3395551235.63267>,
                               inet_tcp_dist,<0.281.0>,
                               #Ref<0.2948281080.3395551235.63271>}
[error_logger:info,2020-04-02T21:10:51.651+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.281.0>,connection_closed}}
[ns_server:info,2020-04-02T21:10:51.795+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.274.0>:ns_port_server:log:224]ns_couchdb<0.274.0>: Apache CouchDB  (LogLevel=info) is starting.
ns_couchdb<0.274.0>: Failure to start Mochiweb: eaddrinuse
ns_couchdb<0.274.0>: 13024: Booted. Waiting for shutdown request
ns_couchdb<0.274.0>: [os_mon] memory supervisor port (memsup): Erlang has closed
ns_couchdb<0.274.0>: [os_mon] cpu supervisor port (cpu_sup): Erlang has closed

[ns_server:debug,2020-04-02T21:10:51.842+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T21:10:51.842+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:10:51.842+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2948281080.3395551235.63295>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:10:51.843+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2948281080.3395551235.63295>,
                                  inet_tcp_dist,<0.287.0>,
                                  #Ref<0.2948281080.3395551235.63297>}
[ns_server:debug,2020-04-02T21:10:51.844+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-04-02T21:10:51.843+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2948281080.3395551235.63295>,
                               inet_tcp_dist,<0.287.0>,
                               #Ref<0.2948281080.3395551235.63297>}
[error_logger:info,2020-04-02T21:10:51.843+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.287.0>,shutdown}}
[error_logger:info,2020-04-02T21:10:51.844+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:10:52.045+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T21:10:52.045+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:10:52.045+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2948281080.3395551236.63066>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:10:52.046+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2948281080.3395551236.63066>,
                                  inet_tcp_dist,<0.290.0>,
                                  #Ref<0.2948281080.3395551236.63070>}
[ns_server:debug,2020-04-02T21:10:52.047+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-04-02T21:10:52.047+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.290.0>,shutdown}}
[ns_server:debug,2020-04-02T21:10:52.047+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2948281080.3395551236.63066>,
                               inet_tcp_dist,<0.290.0>,
                               #Ref<0.2948281080.3395551236.63070>}
[error_logger:info,2020-04-02T21:10:52.047+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-04-02T21:10:52.248+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:10:52.248+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T21:10:52.248+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2948281080.3395551234.63350>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:10:52.249+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2948281080.3395551234.63350>,
                                  inet_tcp_dist,<0.293.0>,
                                  #Ref<0.2948281080.3395551236.63080>}
[ns_server:debug,2020-04-02T21:10:52.250+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2948281080.3395551234.63350>,
                               inet_tcp_dist,<0.293.0>,
                               #Ref<0.2948281080.3395551236.63080>}
[error_logger:info,2020-04-02T21:10:52.250+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.293.0>,shutdown}}
[ns_server:debug,2020-04-02T21:10:52.250+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-04-02T21:10:52.251+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:info,2020-04-02T21:10:52.337+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.274.0>:ns_port_server:log:224]ns_couchdb<0.274.0>: {"Kernel pid terminated",application_controller,"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\"},{line,102}]},{supervisor,do_start_child,2,[{file,\"supervisor.erl\"},{line,365}]},{supervisor,start_children,3,[{file,\"supervisor.erl\"},{line,348}]},{supervisor,init_children,2,[{file,\"supervisor.erl\"},{line,314}]},{gen_server,init_it,2,[{file,\"gen_server.erl\"},{line,365}]},{gen_server,init_it,6,[{file,\"gen_server.erl\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}"}
ns_couchdb<0.274.0>: Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro
ns_couchdb<0.274.0>: 
ns_couchdb<0.274.0>: Crash dump is being written to: erl_crash.dump.1585842038.12600.ns_couchdb...done

[error_logger:error,2020-04-02T21:10:52.337+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]** Generic server ns_couchdb_port terminating 
** Last message in was {#Port<0.5097>,{exit_status,1}}
** When Server state == {state,#Port<0.5097>,
                            {ns_couchdb,"/opt/couchbase/lib/erlang/bin/erl",
                                ["-pa",
                                 "/opt/couchbase/lib/erlang/lib/asn1-5.0.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/compiler-7.1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEvent-2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEventDomain-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosFileTransfer-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosNotification-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosProperty-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTime-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTransactions-1.3.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/crypto-4.2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/dialyzer-3.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/diameter-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/edoc-0.9.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eldap-1.2.3.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_docgen-0.7.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_interface-3.10.2.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erts-9.3.3.9/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eunit-2.3.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/hipe-3.17.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ic-4.4.4.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/inets-6.5.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/mnesia-4.15.3.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/orber-3.8.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/os_mon-2.4.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/otp_mibs-1.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/parsetools-2.1.6/ebin",
                                 "/opt/couchbase/lib/erlang/lib/public_key-1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/reltool-0.7.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/runtime_tools-1.12.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/sasl-3.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/snmp-5.2.11/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssh-4.6.9.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssl-8.2.6.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/syntax_tools-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/tools-2.11.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/xmerl-1.3.16.1/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/gc-couchbase-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/vtree-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/wkb-1.2.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch-1.2.0a-961ad59-git/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_audit-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_dcp-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_index_merger-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_set_view-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_view_parser-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/ejson-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/erlang-oauth/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/etap/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/lhttpc-1.3/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mapreduce-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mochiweb-1.4.1/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/snappy-1.0.4/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ale/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/gen_smtp/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_babysitter/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_couchdb/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/ebin",
                                 "/opt/couchbase/lib/erlang/lib/stdlib-3.4.5.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/kernel-5.4.3.2/ebin",
                                 ".","-couch_ini",
                                 "/opt/couchbase/etc/couchdb/default.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/capi.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/geocouch.ini",
                                 "/opt/couchbase/etc/couchdb/local.ini",
                                 "-kernel","error_logger","false","-kernel",
                                 "error_logger","false","inetrc",
                                 "\"/opt/couchbase/etc/couchbase/hosts.cfg\"",
                                 "dist_config_file",
                                 "\"/opt/couchbase/var/lib/couchbase/config/dist_cfg\"",
                                 "-ssl_dist_optfile",
                                 "/opt/couchbase/etc/couchbase/ssl_dist_opts",
                                 "-setcookie",
                                 "2a158eb185476066b502c8ced82b23b042bd1ae5f1ef74f024e7bb9916796a9b",
                                 "-name","couchdb_ns_1@cb.local","-smp",
                                 "enable","+P","327680","+K","true","-kernel",
                                 "error_logger","false","-sasl",
                                 "sasl_error_logger","false","-nouser",
                                 "-hidden","-proto_dist","cb","-epmd_module",
                                 "cb_epmd","-start_epmd","false","-run",
                                 "child_erlang","child_start","ns_couchdb"],
                                [use_stdio,
                                 {env,
                                     [{"NS_COUCHDB_ENV_ARGS",
                                       "[{ns_server_node,'ns_1@127.0.0.1'},\n {path_config_tmpdir,\"/opt/couchbase/var/lib/couchbase/tmp\"},\n {net_kernel_verbosity,10},\n {loglevel_error_logger,debug},\n {path_config_libdir,\"/opt/couchbase/lib\"},\n {loglevel_stats,debug},\n {loglevel_menelaus,debug},\n {path_config_secdir,\"/opt/couchbase/etc/security\"},\n {loglevel_user,debug},\n {path_config_etcdir,\"/opt/couchbase/etc/couchbase\"},\n {loglevel_ns_server,debug},\n {loglevel_mapreduce_errors,debug},\n {loglevel_rebalance,debug},\n {loglevel_default,debug},\n {disk_sink_opts,[{rotation,[{compress,true},\n                             {size,41943040},\n                             {num_files,10},\n                             {buffer_size_max,52428800}]}]},\n {loglevel_cbas,debug},\n {loglevel_xdcr,debug},\n {loglevel_ns_doctor,debug},\n {loglevel_access,info},\n {error_logger_mf_dir,\"/opt/couchbase/var/lib/couchbase/logs\"},\n {path_config_datadir,\"/opt/couchbase/var/lib/couchbase\"},\n {loglevel_cluster,debug},\n {loglevel_couchdb,info},\n {loglevel_views,debug},\n {path_config_bindir,\"/opt/couchbase/bin\"}]"},
                                      {"ERL_CRASH_DUMP",
                                       "erl_crash.dump.1585842038.12600.ns_couchdb"}]}]},
                            {ringbuffer,1191,1024,
                                {[{<<"Crash dump is being written to: erl_crash.dump.1585842038.12600.ns_couchdb...done">>,
                                   81},
                                  {<<>>,0},
                                  {<<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                                   200}],
                                 [{<<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>,
                                   910}]}},
                            undefined,
                            {ok,{-576460749642,
                                 #Ref<0.2948281080.3395551235.63299>}},
                            [<<"Crash dump is being written to: erl_crash.dump.1585842038.12600.ns_couchdb...done">>,
                             <<>>,
                             <<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                             <<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>],
                            0}
** Reason for termination == 
** {abnormal,1}

[ns_server:error,2020-04-02T21:10:52.343+05:30,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.275.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:189]ns_couchdb_port(<0.274.0>) died with reason {abnormal,1}
[ns_server:debug,2020-04-02T21:10:52.344+05:30,ns_1@127.0.0.1:<0.269.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.267.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:10:52.344+05:30,ns_1@127.0.0.1:<0.268.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.267.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:10:52.344+05:30,ns_1@127.0.0.1:<0.266.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.264.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:10:52.344+05:30,ns_1@127.0.0.1:<0.265.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.264.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:10:52.345+05:30,ns_1@127.0.0.1:<0.257.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.256.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:10:52.345+05:30,ns_1@127.0.0.1:<0.218.0>:restartable:shutdown_child:120]Successfully terminated process <0.219.0>
[ns_server:debug,2020-04-02T21:10:52.346+05:30,ns_1@127.0.0.1:<0.217.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.216.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:10:52.346+05:30,ns_1@127.0.0.1:<0.203.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.202.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:10:52.346+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:wait_saver:866]Done waiting for saver.
[error_logger:error,2020-04-02T21:10:52.347+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_port_server:init/1
    pid: <0.274.0>
    registered_name: ns_couchdb_port
    exception exit: {abnormal,1}
      in function  gen_server:handle_common_reply/8 (gen_server.erl, line 726)
    ancestors: [ns_server_nodes_sup,<0.208.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 1
    messages: [{'EXIT',#Port<0.5097>,normal}]
    links: [<0.209.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 11926
  neighbours:

[error_logger:error,2020-04-02T21:10:52.347+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: erlang:apply/2
    pid: <0.275.0>
    registered_name: wait_link_to_couchdb_node
    exception exit: {abnormal,1}
      in function  ns_server_nodes_sup:do_wait_link_to_couchdb_node/1 (src/ns_server_nodes_sup.erl, line 190)
    ancestors: [ns_server_nodes_sup,<0.208.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.209.0>,<0.276.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 3352
  neighbours:
    neighbour:
      pid: <0.276.0>
      registered_name: []
      initial call: ns_server_nodes_sup:'-do_wait_link_to_couchdb_node/1-fun-2-'/0
      current_function: {timer,sleep,1}
      ancestors: [wait_link_to_couchdb_node,ns_server_nodes_sup,<0.208.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
      message_queue_len: 0
      links: [<0.275.0>]
      trap_exit: false
      status: waiting
      heap_size: 2586
      stack_size: 12
      reductions: 10860
      current_stacktrace: [{timer,sleep,1,[{file,"timer.erl"},{line,153}]},
                  {misc,poll_for_condition_rec,3,
                      [{file,"src/misc.erl"},{line,508}]},
                  {ns_server_nodes_sup,
                      '-do_wait_link_to_couchdb_node/1-fun-2-',2,
                      [{file,"src/ns_server_nodes_sup.erl"},{line,159}]},
                  {proc_lib,init_p,3,[{file,"proc_lib.erl"},{line,232}]}]

[error_logger:error,2020-04-02T21:10:52.348+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    start_error
     Reason:     {abnormal,1}
     Offender:   [{pid,undefined},
                  {name,wait_for_couchdb_node},
                  {mfargs,{erlang,apply,
                                  [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:error,2020-04-02T21:10:52.348+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    shutdown_error
     Reason:     {abnormal,1}
     Offender:   [{pid,<0.274.0>},
                  {name,start_couchdb_node},
                  {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                  {restart_type,{permanent,5}},
                  {shutdown,86400000},
                  {child_type,worker}]


[error_logger:error,2020-04-02T21:10:52.348+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_cluster_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,wait_for_couchdb_node,
                         {abnormal,1}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_nodes_sup},
                  {mfargs,
                      {restartable,start_link,
                          [{ns_server_nodes_sup,start_link,[]},infinity]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-04-02T21:10:52.348+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,root_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                             {failed_to_start_child,wait_for_couchdb_node,
                                 {abnormal,1}}}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_cluster_sup},
                  {mfargs,{ns_server_cluster_sup,start_link,[]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-04-02T21:10:52.349+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: application_master:init/4
    pid: <0.117.0>
    registered_name: []
    exception exit: {{shutdown,
                      {failed_to_start_child,ns_server_cluster_sup,
                       {shutdown,
                        {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                          {failed_to_start_child,wait_for_couchdb_node,
                           {abnormal,1}}}}}}},
                     {ns_server,start,[normal,[]]}}
      in function  application_master:init/4 (application_master.erl, line 134)
    ancestors: [<0.116.0>]
    message_queue_len: 1
    messages: [{'EXIT',<0.118.0>,normal}]
    links: [<0.116.0>,<0.33.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 274
  neighbours:

[error_logger:info,2020-04-02T21:10:52.349+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
         application: ns_server
              exited: {{shutdown,
                        {failed_to_start_child,ns_server_cluster_sup,
                         {shutdown,
                          {failed_to_start_child,ns_server_nodes_sup,
                           {shutdown,
                            {failed_to_start_child,wait_for_couchdb_node,
                             {abnormal,1}}}}}}},
                       {ns_server,start,[normal,[]]}}
                type: permanent

[error_logger:info,2020-04-02T21:10:52.349+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,system_memory_high_watermark}

[error_logger:info,2020-04-02T21:10:52.349+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-calculator/544"}}

[error_logger:info,2020-04-02T21:10:52.349+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/8935"}}

[error_logger:info,2020-04-02T21:10:52.349+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-calculator/704"}}

[error_logger:info,2020-04-02T21:10:52.349+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/59"}}

[error_logger:info,2020-04-02T21:10:52.349+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/pycharm-community/188"}}

[error_logger:info,2020-04-02T21:10:52.350+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gnome-system-monitor/127"}}

[error_logger:info,2020-04-02T21:10:52.350+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core18/1705"}}

[error_logger:info,2020-04-02T21:10:52.350+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/8689"}}

[error_logger:info,2020-04-02T21:10:52.350+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gnome-system-monitor/135"}}

[error_logger:info,2020-04-02T21:10:52.350+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-characters/495"}}

[error_logger:info,2020-04-02T21:10:52.351+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-28-1804/116"}}

[error_logger:info,2020-04-02T21:10:52.351+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gtk-common-themes/1474"}}

[error_logger:info,2020-04-02T21:10:52.351+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core18/1668"}}

[error_logger:info,2020-04-02T21:10:52.351+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-logs/81"}}

[error_logger:info,2020-04-02T21:10:52.351+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-logs/93"}}

[error_logger:info,2020-04-02T21:10:52.351+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/98"}}

[error_logger:info,2020-04-02T21:10:52.351+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-characters/399"}}

[error_logger:info,2020-04-02T21:10:52.351+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gtk-common-themes/1440"}}

[ns_server:info,2020-04-02T21:10:59.693+05:30,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-04-02T21:10:59.704+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-04-02T21:10:59.704+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.704+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.704+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.704+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.704+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.704+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.704+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.704+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.705+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.705+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.705+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.705+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.705+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.705+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.705+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.705+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.705+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.705+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.705+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.705+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.705+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.705+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.705+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.705+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.705+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-04-02T21:10:59.705+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-04-02T21:10:59.709+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-04-02T21:10:59.709+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[error_logger:info,2020-04-02T21:10:59.710+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.168.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2020-04-02T21:10:59.711+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.167.0>},
                       {id,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:10:59.712+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@127.0.0.1'
[error_logger:info,2020-04-02T21:10:59.720+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.720+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.720+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:59.722+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.174.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.723+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.176.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T21:10:59.723+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2020-04-02T21:10:59.723+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.177.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:59.723+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.175.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:59.723+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:59.724+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.724+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.725+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.180.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:10:59.726+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-04-02T21:10:59.726+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-04-02T21:10:59.727+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2020-04-02T21:10:59.728+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-04-02T21:10:59.728+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.181.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.728+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.169.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-04-02T21:10:59.729+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-04-02T21:10:59.731+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-04-02T21:10:59.731+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2020-04-02T21:10:59.731+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:10:59.731+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T21:10:59.731+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1051373791.1516240897.238509>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:10:59.731+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1051373791.1516240897.238509>,
                                  inet_tcp_dist,<0.185.0>,
                                  #Ref<0.1051373791.1516240897.238514>}
[ns_server:debug,2020-04-02T21:10:59.733+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-04-02T21:10:59.733+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2020-04-02T21:10:59.733+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-04-02T21:10:59.735+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-04-02T21:10:59.735+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.739+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:10:59.740+05:30,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,4,2},{21,10,59}}},
               {memory,
                   [{total,26310968},
                    {processes,9521872},
                    {processes_used,9516984},
                    {system,16789096},
                    {atom,388625},
                    {atom_used,364409},
                    {binary,109080},
                    {code,8250921},
                    {ets,1509096}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,code,
                    file_io_server,file_server,code_server,kernel,gen_event,
                    application_controller,filename,gen,error_handler,ets,
                    application,heart,erl_lint,file,application_master,
                    erl_eval,error_logger,gen_server,lists,proc_lib,erl_parse,
                    supervisor,erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{os_mon,"CPO  CXC 138 46","2.4.4"},
                    {sasl,"SASL  CXC 138 11","3.1.2"},
                    {ns_server,"Couchbase server","6.5.0-4960-enterprise"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ale,"Another Logger for Erlang","0.0.0"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,131},
               {node,'ns_1@127.0.0.1'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,auth,httpd_sup,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    release_handler,lhttpc_sup,httpc_sup,lhttpc_manager,
                    alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    'sink-disk_xdcr',timer_server,'sink-disk_debug',
                    inet_gethost_native,ale_sup,'sink-disk_error',inet_db,
                    'sink-disk_default',ssl_pem_cache_dist,ale_dynamic_sup,
                    rex,global_group,net_sup,kernel_sup,ssl_connection_sup,
                    global_name_server,ssl_admin_sup,tftp_sup,ssl_sup,
                    root_sup,erts_code_purger,os_mon_sup,file_server_2,
                    error_logger,cpu_sup,erl_epmd,kernel_safe_sup,init,memsup,
                    erl_signal_server,disksup,ale,net_kernel,dist_manager,
                    ssl_pem_cache,ssl_manager,ssl_dist_admin_sup,
                    ssl_dist_connection_sup,ssl_dist_sup,user,
                    ssl_tls_dist_proxy,ssl_manager_dist,sasl_safe_sup,
                    ssl_listen_tracker_sup,inet_gethost_native_sup,
                    code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,0}]
[ns_server:info,2020-04-02T21:10:59.743+05:30,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"cfa0f75f28402d2e1aa254b2a374bead19433526\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" upstream=\"master\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"f2a16b53bb74146f20d18ba2c0443d5f10a9a550\" upstream=\"master\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4960\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" upstream=\"master\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" upstream=\"mad-hatter\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" upstream=\"master\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" upstream=\"master\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" upstream=\"master\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" upstream=\"mad-hatter\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" upstream=\"cb-master\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" upstream=\"master\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" upstream=\"master\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" upstream=\"mad-hatter\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" upstream=\"mad-hatter\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" upstream=\"mad-hatter\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" upstream=\"master\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" upstream=\"master\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" upstream=\"mad-hatter\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" upstream=\"mad-hatter\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" upstream=\"mad-hatter\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" upstream=\"mad-hatter\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" upstream=\"master\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" upstream=\"master\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" upstream=\"master\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" upstream=\"mad-hatter\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" upstream=\"master\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"7a3aa547b3f5eb3ea28d279a08384609cd2cea7c\" upstream=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" upstream=\"mad-hatter\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" upstream=\"master\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" upstream=\"master\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" upstream=\"master\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" upstream=\"v1.1.0-couchbase\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"ee64d41320d14fabe814a241a5cf4f6a6f6e827a\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" upstream=\"master\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" upstream=\"master\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-04-02T21:10:59.745+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.746+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.746+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:10:59.747+05:30,ns_1@127.0.0.1:ns_config_sup<0.192.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-04-02T21:10:59.747+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.747+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.194.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:10:59.763+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-04-02T21:10:59.763+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2020-04-02T21:10:59.770+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[{alert_limits,
   [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
  {audit,
   [{auditd_enabled,false},
    {rotate_interval,86400},
    {rotate_size,20971520},
    {disabled,[]},
    {sync,[]},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
  {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
  {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
  {autocompaction,
   [{database_fragmentation_threshold,{30,undefined}},
    {view_fragmentation_threshold,{30,undefined}}]},
  {buckets,[{configs,[]}]},
  {cbas_memory_quota,2174},
  {cert_and_pkey,
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFgIK71cHor8wDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBkZTZmMzM0MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgZGU2ZjMz\nNDAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC6Epk+5C0GfEqGHL9d\nxsySLywt3gLcVQmCM8lgcMRGWDaGVF6iOP+QyLODyB09I5u2gOcVm+1r3eOZ4rwk\nbttVmFIsdroNf2jG+9baY4LqKoDyZnjZr0LeolUcY+0eYI68oNwRMgWp53Krm861\ny11yyOEjefm+JBDhZuZpHZegjTBKtYqZd96WZwOzbrJZrau3uKBuQTmoEdpZ4VdX\n6U5nzUaRkvjjuBpQyeqMLSuuLUO4FENp1C8P9fYhy4Y6RRZfMSBGdyw1d8QEWxiU\n4n/rtfQgiN32qOwtY7ocwvaXDV7wH1ipWkPF5Vn8eyBi5cA2xqgaq1xSBLD8MUHE\nXTAjAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCP9ajveEq01YMq\n/zClEAjE3TCbGqz9u/vjXdhSQK7rPJLcK250d86L6njzkS2ffrabbOGON+4UvNW4\nTUub3JqnTuSlI8B6riH61kqWPfCfRC392v1xAIaQI1/jWsW4HQoiXbmi0uiKrsEq\nIt8XF5nLXDsEeWYetynrODdVU9ADeDNkE2+AOyLTvD/4eUDRoQhDhC5vh75Bu9gm\nEV+efNKCwXjs4xAMPGbKoNnWBkx7Btn0+iyI19l+jrzF1rlDaH6pFz2ldqm6CL+f\n26ZCU9S8uXPNC7UiNXr6DZj1sn/k0qqebDRnHlO2P+wYp5G/+Rca+B41diWCV7xG\ncnfTf1PH\n-----END CERTIFICATE-----\n">>,
     <<"*****">>}]},
  {drop_request_memory_threshold_mib,undefined},
  {email_alerts,
   [{recipients,["root@localhost"]},
    {sender,"couchbase@localhost"},
    {enabled,false},
    {email_server,
     [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
    {alerts,
     [auto_failover_node,auto_failover_maximum_reached,
      auto_failover_other_nodes_down,auto_failover_cluster_too_small,
      auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
      ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
      ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
  {fts_memory_quota,512},
  {index_aware_rebalance_disabled,false},
  {log_redaction_default_cfg,[{redact_level,none}]},
  {max_bucket_count,30},
  {memcached,[]},
  {memory_quota,8886},
  {nodes_wanted,['ns_1@127.0.0.1']},
  {password_policy,[{min_length,6},{must_present,[]}]},
  {quorum_nodes,['ns_1@127.0.0.1']},
  {remote_clusters,[]},
  {replication,[{enabled,true}]},
  {rest,[{port,8091}]},
  {rest_creds,null},
  {secure_headers,[]},
  {server_groups,
   [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
  {set_view_update_daemon,
   [{update_interval,5000},
    {update_min_changes,5000},
    {replica_update_min_changes,5000}]},
  {{couchdb,max_parallel_indexers},4},
  {{couchdb,max_parallel_replica_indexers},2},
  {{local_changes_count,<<"8c43a5102cad1e34db659ab4d5646878">>},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{3,63753061250}}]}]},
  {{metakv,<<"/indexing/settings/config">>},
   <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
  {{request_limit,capi},undefined},
  {{request_limit,rest},undefined},
  {{node,'ns_1@127.0.0.1',address_family},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    inet]},
  {{node,'ns_1@127.0.0.1',audit},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
  {{node,'ns_1@127.0.0.1',capi_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8092]},
  {{node,'ns_1@127.0.0.1',cbas_admin_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9110]},
  {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9113]},
  {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9112]},
  {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9111]},
  {{node,'ns_1@127.0.0.1',cbas_cluster_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9115]},
  {{node,'ns_1@127.0.0.1',cbas_console_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9114]},
  {{node,'ns_1@127.0.0.1',cbas_data_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9116]},
  {{node,'ns_1@127.0.0.1',cbas_debug_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    -1]},
  {{node,'ns_1@127.0.0.1',cbas_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8095]},
  {{node,'ns_1@127.0.0.1',cbas_messaging_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9118]},
  {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9119]},
  {{node,'ns_1@127.0.0.1',cbas_metadata_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9121]},
  {{node,'ns_1@127.0.0.1',cbas_parent_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9122]},
  {{node,'ns_1@127.0.0.1',cbas_replication_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9120]},
  {{node,'ns_1@127.0.0.1',cbas_result_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9117]},
  {{node,'ns_1@127.0.0.1',cbas_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18095]},
  {{node,'ns_1@127.0.0.1',compaction_daemon},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@127.0.0.1',config_version},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    {6,5}]},
  {{node,'ns_1@127.0.0.1',erl_external_listeners},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {inet,false},
    {inet6,false}]},
  {{node,'ns_1@127.0.0.1',eventing_debug_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9140]},
  {{node,'ns_1@127.0.0.1',eventing_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8096]},
  {{node,'ns_1@127.0.0.1',eventing_https_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18096]},
  {{node,'ns_1@127.0.0.1',fts_grpc_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9130]},
  {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    19130]},
  {{node,'ns_1@127.0.0.1',fts_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8094]},
  {{node,'ns_1@127.0.0.1',fts_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18094]},
  {{node,'ns_1@127.0.0.1',indexer_admin_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9100]},
  {{node,'ns_1@127.0.0.1',indexer_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9102]},
  {{node,'ns_1@127.0.0.1',indexer_https_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    19102]},
  {{node,'ns_1@127.0.0.1',indexer_scan_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9101]},
  {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9104]},
  {{node,'ns_1@127.0.0.1',indexer_stinit_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9103]},
  {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9105]},
  {{node,'ns_1@127.0.0.1',is_enterprise},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    true]},
  {{node,'ns_1@127.0.0.1',isasl},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@127.0.0.1',membership},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    active]},
  {{node,'ns_1@127.0.0.1',memcached},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {port,11210},
    {dedicated_port,11209},
    {dedicated_ssl_port,11206},
    {ssl_port,11207},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_sleeptime,19},
    {log_rotation_period,39003}]},
  {{node,'ns_1@127.0.0.1',memcached_config},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    {[{interfaces,
       {memcached_config_mgr,omit_missing_mcd_ports,
        [{[{host,<<"*">>},
           {port,port},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_port},
           {system,true},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,ssl_port},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_ssl_port},
           {system,true},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
      {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
      {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {opentracing,
       {[{enabled,opentracing_enabled},
         {module,{"~s",[opentracing_module]}},
         {config,{"~s",[opentracing_config]}}]}},
      {admin,{"~s",[admin_user]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize},
         {sleeptime,log_sleeptime}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
  {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    11206]},
  {{node,'ns_1@127.0.0.1',memcached_defaults},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {max_connections,65000},
    {system_connections,5000},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {opentracing_enabled,false},
    {opentracing_module,[]},
    {opentracing_config,[]},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {tracing_enabled,true},
    {datatype_snappy,true},
    {num_reader_threads,<<"default">>},
    {num_writer_threads,<<"default">>}]},
  {{node,'ns_1@127.0.0.1',moxi},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {port,0}]},
  {{node,'ns_1@127.0.0.1',node_encryption},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    false]},
  {{node,'ns_1@127.0.0.1',ns_log},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@127.0.0.1',port_servers},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
  {{node,'ns_1@127.0.0.1',projector_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9999]},
  {{node,'ns_1@127.0.0.1',projector_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9999]},
  {{node,'ns_1@127.0.0.1',query_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8093]},
  {{node,'ns_1@127.0.0.1',rest},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@127.0.0.1',saslauthd_enabled},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    true]},
  {{node,'ns_1@127.0.0.1',ssl_capi_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18092]},
  {{node,'ns_1@127.0.0.1',ssl_query_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18093]},
  {{node,'ns_1@127.0.0.1',ssl_rest_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18091]},
  {{node,'ns_1@127.0.0.1',uuid},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    <<"8c43a5102cad1e34db659ab4d5646878">>]},
  {{node,'ns_1@127.0.0.1',xdcr_rest_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9998]},
  {{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    false]}]]
[ns_server:info,2020-04-02T21:10:59.774+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',xdcr_rest_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9998]},
 {{node,'ns_1@127.0.0.1',uuid},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   <<"8c43a5102cad1e34db659ab4d5646878">>]},
 {{node,'ns_1@127.0.0.1',ssl_rest_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18091]},
 {{node,'ns_1@127.0.0.1',ssl_query_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18093]},
 {{node,'ns_1@127.0.0.1',ssl_capi_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18092]},
 {{node,'ns_1@127.0.0.1',saslauthd_enabled},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',rest},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@127.0.0.1',query_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8093]},
 {{node,'ns_1@127.0.0.1',projector_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',projector_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',port_servers},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
 {{node,'ns_1@127.0.0.1',ns_log},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@127.0.0.1',node_encryption},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',moxi},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {port,0}]},
 {{node,'ns_1@127.0.0.1',memcached_defaults},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   11206]},
 {{node,'ns_1@127.0.0.1',memcached_config},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@127.0.0.1',memcached},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@127.0.0.1',membership},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   active]},
 {{node,'ns_1@127.0.0.1',isasl},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@127.0.0.1',is_enterprise},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9105]},
 {{node,'ns_1@127.0.0.1',indexer_stinit_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9103]},
 {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9104]},
 {{node,'ns_1@127.0.0.1',indexer_scan_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9101]},
 {{node,'ns_1@127.0.0.1',indexer_https_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   19102]},
 {{node,'ns_1@127.0.0.1',indexer_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9102]},
 {{node,'ns_1@127.0.0.1',indexer_admin_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9100]},
 {{node,'ns_1@127.0.0.1',fts_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18094]},
 {{node,'ns_1@127.0.0.1',fts_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8094]},
 {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   19130]},
 {{node,'ns_1@127.0.0.1',fts_grpc_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9130]},
 {{node,'ns_1@127.0.0.1',eventing_https_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18096]},
 {{node,'ns_1@127.0.0.1',eventing_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8096]},
 {{node,'ns_1@127.0.0.1',eventing_debug_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9140]},
 {{node,'ns_1@127.0.0.1',erl_external_listeners},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {inet,false},
   {inet6,false}]},
 {{node,'ns_1@127.0.0.1',config_version},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   {6,5}]},
 {{node,'ns_1@127.0.0.1',compaction_daemon},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@127.0.0.1',cbas_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18095]},
 {{node,'ns_1@127.0.0.1',cbas_result_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9117]},
 {{node,'ns_1@127.0.0.1',cbas_replication_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9120]},
 {{node,'ns_1@127.0.0.1',cbas_parent_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9122]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9121]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9119]},
 {{node,'ns_1@127.0.0.1',cbas_messaging_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9118]},
 {{node,'ns_1@127.0.0.1',cbas_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8095]},
 {{node,'ns_1@127.0.0.1',cbas_debug_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|-1]},
 {{node,'ns_1@127.0.0.1',cbas_data_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9116]},
 {{node,'ns_1@127.0.0.1',cbas_console_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9114]},
 {{node,'ns_1@127.0.0.1',cbas_cluster_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9115]},
 {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9111]},
 {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9112]},
 {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9113]},
 {{node,'ns_1@127.0.0.1',cbas_admin_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9110]},
 {{node,'ns_1@127.0.0.1',capi_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8092]},
 {{node,'ns_1@127.0.0.1',audit},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
 {{node,'ns_1@127.0.0.1',address_family},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   inet]},
 {{request_limit,rest},undefined},
 {{request_limit,capi},undefined},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{local_changes_count,<<"8c43a5102cad1e34db659ab4d5646878">>},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{3,63753061250}}]}]},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
 {secure_headers,[]},
 {rest_creds,null},
 {rest,[{port,8091}]},
 {replication,[{enabled,true}]},
 {remote_clusters,[]},
 {quorum_nodes,['ns_1@127.0.0.1']},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {nodes_wanted,['ns_1@127.0.0.1']},
 {memory_quota,8886},
 {memcached,[]},
 {max_bucket_count,30},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {index_aware_rebalance_disabled,false},
 {fts_memory_quota,512},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {drop_request_memory_threshold_mib,undefined},
 {cert_and_pkey,
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFgIK71cHor8wDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBkZTZmMzM0MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgZGU2ZjMz\nNDAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC6Epk+5C0GfEqGHL9d\nxsySLywt3gLcVQmCM8lgcMRGWDaGVF6iOP+QyLODyB09I5u2gOcVm+1r3eOZ4rwk\nbttVmFIsdroNf2jG+9baY4LqKoDyZnjZr0LeolUcY+0eYI68oNwRMgWp53Krm861\ny11yyOEjefm+JBDhZuZpHZegjTBKtYqZd96WZwOzbrJZrau3uKBuQTmoEdpZ4VdX\n6U5nzUaRkvjjuBpQyeqMLSuuLUO4FENp1C8P9fYhy4Y6RRZfMSBGdyw1d8QEWxiU\n4n/rtfQgiN32qOwtY7ocwvaXDV7wH1ipWkPF5Vn8eyBi5cA2xqgaq1xSBLD8MUHE\nXTAjAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCP9ajveEq01YMq\n/zClEAjE3TCbGqz9u/vjXdhSQK7rPJLcK250d86L6njzkS2ffrabbOGON+4UvNW4\nTUub3JqnTuSlI8B6riH61kqWPfCfRC392v1xAIaQI1/jWsW4HQoiXbmi0uiKrsEq\nIt8XF5nLXDsEeWYetynrODdVU9ADeDNkE2+AOyLTvD/4eUDRoQhDhC5vh75Bu9gm\nEV+efNKCwXjs4xAMPGbKoNnWBkx7Btn0+iyI19l+jrzF1rlDaH6pFz2ldqm6CL+f\n26ZCU9S8uXPNC7UiNXr6DZj1sn/k0qqebDRnHlO2P+wYp5G/+Rca+B41diWCV7xG\ncnfTf1PH\n-----END CERTIFICATE-----\n">>,
    <<"*****">>}]},
 {cbas_memory_quota,2174},
 {buckets,[{configs,[]}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]}]
[error_logger:info,2020-04-02T21:10:59.776+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.195.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.777+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.201.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.777+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.202.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.777+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T21:10:59.779+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"8c43a5102cad1e34db659ab4d5646878">>} ->
[{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{4,63753061259}}]}]
[error_logger:info,2020-04-02T21:10:59.779+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.204.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:59.780+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.207.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:59.784+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:10:59.785+05:30,ns_1@127.0.0.1:menelaus_barrier<0.211.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-04-02T21:10:59.785+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.785+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.788+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.213.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.789+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.215.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:10:59.797+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2020-04-02T21:10:59.797+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2020-04-02T21:10:59.797+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2020-04-02T21:10:59.812+05:30,ns_1@127.0.0.1:ns_ssl_services_setup<0.216.0>:ns_ssl_services_setup:init:462]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,2,10,239,87,7,162,
             191,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
             6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
             118,101,114,32,100,101,54,102,51,51,52,48,48,30,23,13,49,51,48,
             49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
             57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
             104,98,97,115,101,32,83,101,114,118,101,114,32,100,101,54,102,
             51,51,52,48,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,
             0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,186,18,153,62,228,45,6,
             124,74,134,28,191,93,198,204,146,47,44,45,222,2,220,85,9,130,51,
             201,96,112,196,70,88,54,134,84,94,162,56,255,144,200,179,131,
             200,29,61,35,155,182,128,231,21,155,237,107,221,227,153,226,188,
             36,110,219,85,152,82,44,118,186,13,127,104,198,251,214,218,99,
             130,234,42,128,242,102,120,217,175,66,222,162,85,28,99,237,30,
             96,142,188,160,220,17,50,5,169,231,114,171,155,206,181,203,93,
             114,200,225,35,121,249,190,36,16,225,102,230,105,29,151,160,141,
             48,74,181,138,153,119,222,150,103,3,179,110,178,89,173,171,183,
             184,160,110,65,57,168,17,218,89,225,87,87,233,78,103,205,70,145,
             146,248,227,184,26,80,201,234,140,45,43,174,45,67,184,20,67,105,
             212,47,15,245,246,33,203,134,58,69,22,95,49,32,70,119,44,53,119,
             196,4,91,24,148,226,127,235,181,244,32,136,221,246,168,236,45,
             99,186,28,194,246,151,13,94,240,31,88,169,90,67,197,229,89,252,
             123,32,98,229,192,54,198,168,26,171,92,82,4,176,252,49,65,196,
             93,48,35,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,255,4,4,
             3,2,2,164,48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,
             15,6,3,85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,134,72,
             134,247,13,1,1,11,5,0,3,130,1,1,0,143,245,168,239,120,74,180,
             213,131,42,255,48,165,16,8,196,221,48,155,26,172,253,187,251,
             227,93,216,82,64,174,235,60,146,220,43,110,116,119,206,139,234,
             120,243,145,45,159,126,182,155,108,225,142,55,238,20,188,213,
             184,77,75,155,220,154,167,78,228,165,35,192,122,174,33,250,214,
             74,150,61,240,159,68,45,253,218,253,113,0,134,144,35,95,227,90,
             197,184,29,10,34,93,185,162,210,232,138,174,193,42,34,223,23,23,
             153,203,92,59,4,121,102,30,183,41,235,56,55,85,83,208,3,120,51,
             100,19,111,128,59,34,211,188,63,248,121,64,209,161,8,67,132,46,
             111,135,190,65,187,216,38,17,95,158,124,210,130,193,120,236,227,
             16,12,60,102,202,160,217,214,6,76,123,6,217,244,250,44,136,215,
             217,126,142,188,197,214,185,67,104,126,169,23,61,165,118,169,
             186,8,191,159,219,166,66,83,212,188,185,115,205,11,181,34,53,
             122,250,13,152,245,178,127,228,210,170,158,108,52,103,30,83,182,
             63,236,24,167,145,191,249,23,26,248,30,53,118,37,130,87,188,70,
             114,119,211,127,83,199>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2020-04-02T21:10:59.813+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.216.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:10:59.820+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-04-02T21:10:59.820+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-04-02T21:10:59.820+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-04-02T21:10:59.820+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:info,2020-04-02T21:10:59.830+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-04-02T21:10:59.830+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-04-02T21:10:59.830+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-04-02T21:10:59.830+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-04-02T21:10:59.830+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.219.0>,menelaus_web}
             started: [{pid,<0.220.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 2,10,239,87,7,162,191,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,100,101,54,
                                 102,51,51,52,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,100,
                                 101,54,102,51,51,52,48,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,186,18,153,62,228,
                                 45,6,124,74,134,28,191,93,198,204,146,47,44,
                                 45,222,2,220,85,9,130,51,201,96,112,196,70,
                                 88,54,134,84,94,162,56,255,144,200,179,131,
                                 200,29,61,35,155,182,128,231,21,155,237,107,
                                 221,227,153,226,188,36,110,219,85,152,82,44,
                                 118,186,13,127,104,198,251,214,218,99,130,
                                 234,42,128,242,102,120,217,175,66,222,162,
                                 85,28,99,237,30,96,142,188,160,220,17,50,5,
                                 169,231,114,171,155,206,181,203,93,114,200,
                                 225,35,121,249,190,36,16,225,102,230,105,29,
                                 151,160,141,48,74,181,138,153,119,222,150,
                                 103,3,179,110,178,89,173,171,183,184,160,
                                 110,65,57,168,17,218,89,225,87,87,233,78,
                                 103,205,70,145,146,248,227,184,26,80,201,
                                 234,140,45,43,174,45,67,184,20,67,105,212,
                                 47,15,245,246,33,203,134,58,69,22,95,49,32,
                                 70,119,44,53,119,196,4,91,24,148,226,127,
                                 235,181,244,32,136,221,246,168,236,45,99,
                                 186,28,194,246,151,13,94,240,31,88,169,90,
                                 67,197,229,89,252,123,32,98,229,192,54,198,
                                 168,26,171,92,82,4,176,252,49,65,196,93,48,
                                 35,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,
                                 15,1,1,255,4,4,3,2,2,164,48,19,6,3,85,29,37,
                                 4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,
                                 85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,
                                 42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 143,245,168,239,120,74,180,213,131,42,255,
                                 48,165,16,8,196,221,48,155,26,172,253,187,
                                 251,227,93,216,82,64,174,235,60,146,220,43,
                                 110,116,119,206,139,234,120,243,145,45,159,
                                 126,182,155,108,225,142,55,238,20,188,213,
                                 184,77,75,155,220,154,167,78,228,165,35,192,
                                 122,174,33,250,214,74,150,61,240,159,68,45,
                                 253,218,253,113,0,134,144,35,95,227,90,197,
                                 184,29,10,34,93,185,162,210,232,138,174,193,
                                 42,34,223,23,23,153,203,92,59,4,121,102,30,
                                 183,41,235,56,55,85,83,208,3,120,51,100,19,
                                 111,128,59,34,211,188,63,248,121,64,209,161,
                                 8,67,132,46,111,135,190,65,187,216,38,17,95,
                                 158,124,210,130,193,120,236,227,16,12,60,
                                 102,202,160,217,214,6,76,123,6,217,244,250,
                                 44,136,215,217,126,142,188,197,214,185,67,
                                 104,126,169,23,61,165,118,169,186,8,191,159,
                                 219,166,66,83,212,188,185,115,205,11,181,34,
                                 53,122,250,13,152,245,178,127,228,210,170,
                                 158,108,52,103,30,83,182,63,236,24,167,145,
                                 191,249,23,26,248,30,53,118,37,130,87,188,
                                 70,114,119,211,127,83,199>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:10:59.831+05:30,ns_1@127.0.0.1:<0.218.0>:restartable:start_child:98]Started child process <0.219.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2020-04-02T21:10:59.831+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.219.0>,menelaus_web}
             started: [{pid,<0.238.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 2,10,239,87,7,162,191,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,100,101,54,
                                 102,51,51,52,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,100,
                                 101,54,102,51,51,52,48,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,186,18,153,62,228,
                                 45,6,124,74,134,28,191,93,198,204,146,47,44,
                                 45,222,2,220,85,9,130,51,201,96,112,196,70,
                                 88,54,134,84,94,162,56,255,144,200,179,131,
                                 200,29,61,35,155,182,128,231,21,155,237,107,
                                 221,227,153,226,188,36,110,219,85,152,82,44,
                                 118,186,13,127,104,198,251,214,218,99,130,
                                 234,42,128,242,102,120,217,175,66,222,162,
                                 85,28,99,237,30,96,142,188,160,220,17,50,5,
                                 169,231,114,171,155,206,181,203,93,114,200,
                                 225,35,121,249,190,36,16,225,102,230,105,29,
                                 151,160,141,48,74,181,138,153,119,222,150,
                                 103,3,179,110,178,89,173,171,183,184,160,
                                 110,65,57,168,17,218,89,225,87,87,233,78,
                                 103,205,70,145,146,248,227,184,26,80,201,
                                 234,140,45,43,174,45,67,184,20,67,105,212,
                                 47,15,245,246,33,203,134,58,69,22,95,49,32,
                                 70,119,44,53,119,196,4,91,24,148,226,127,
                                 235,181,244,32,136,221,246,168,236,45,99,
                                 186,28,194,246,151,13,94,240,31,88,169,90,
                                 67,197,229,89,252,123,32,98,229,192,54,198,
                                 168,26,171,92,82,4,176,252,49,65,196,93,48,
                                 35,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,
                                 15,1,1,255,4,4,3,2,2,164,48,19,6,3,85,29,37,
                                 4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,
                                 85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,
                                 42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 143,245,168,239,120,74,180,213,131,42,255,
                                 48,165,16,8,196,221,48,155,26,172,253,187,
                                 251,227,93,216,82,64,174,235,60,146,220,43,
                                 110,116,119,206,139,234,120,243,145,45,159,
                                 126,182,155,108,225,142,55,238,20,188,213,
                                 184,77,75,155,220,154,167,78,228,165,35,192,
                                 122,174,33,250,214,74,150,61,240,159,68,45,
                                 253,218,253,113,0,134,144,35,95,227,90,197,
                                 184,29,10,34,93,185,162,210,232,138,174,193,
                                 42,34,223,23,23,153,203,92,59,4,121,102,30,
                                 183,41,235,56,55,85,83,208,3,120,51,100,19,
                                 111,128,59,34,211,188,63,248,121,64,209,161,
                                 8,67,132,46,111,135,190,65,187,216,38,17,95,
                                 158,124,210,130,193,120,236,227,16,12,60,
                                 102,202,160,217,214,6,76,123,6,217,244,250,
                                 44,136,215,217,126,142,188,197,214,185,67,
                                 104,126,169,23,61,165,118,169,186,8,191,159,
                                 219,166,66,83,212,188,185,115,205,11,181,34,
                                 53,122,250,13,152,245,178,127,228,210,170,
                                 158,108,52,103,30,83,182,63,236,24,167,145,
                                 191,249,23,26,248,30,53,118,37,130,87,188,
                                 70,114,119,211,127,83,199>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.832+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.218.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.832+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.214.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:59.836+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.256.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.836+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.259.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.839+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.261.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:10:59.840+05:30,ns_1@127.0.0.1:users_replicator<0.261.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-04-02T21:10:59.841+05:30,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.261.0>
[ns_server:debug,2020-04-02T21:10:59.841+05:30,ns_1@127.0.0.1:users_replicator<0.261.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.262.0>
[ns_server:debug,2020-04-02T21:10:59.842+05:30,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-04-02T21:10:59.842+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.262.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.842+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.260.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T21:10:59.846+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-04-02T21:10:59.846+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.264.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.847+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.267.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:10:59.848+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.258.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:59.848+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.270.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:10:59.848+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.271.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:10:59.856+05:30,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-04-02T21:10:59.856+05:30,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 13ms
[ns_server:debug,2020-04-02T21:10:59.857+05:30,ns_1@127.0.0.1:users_replicator<0.261.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[ns_server:debug,2020-04-02T21:10:59.858+05:30,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.275.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[error_logger:info,2020-04-02T21:10:59.858+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.274.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:10:59.858+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T21:10:59.859+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:10:59.859+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1051373791.1516240899.237671>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:10:59.859+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1051373791.1516240899.237671>,
                                  inet_tcp_dist,<0.278.0>,
                                  #Ref<0.1051373791.1516240899.237675>}
[error_logger:info,2020-04-02T21:10:59.859+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.278.0>,shutdown}}
[ns_server:debug,2020-04-02T21:10:59.859+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1051373791.1516240899.237671>,
                               inet_tcp_dist,<0.278.0>,
                               #Ref<0.1051373791.1516240899.237675>}
[ns_server:debug,2020-04-02T21:10:59.859+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-04-02T21:10:59.859+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:00.060+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T21:11:00.060+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:00.060+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1051373791.1516240898.237431>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:11:00.060+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1051373791.1516240898.237431>,
                                  inet_tcp_dist,<0.281.0>,
                                  #Ref<0.1051373791.1516240898.237433>}
[ns_server:debug,2020-04-02T21:11:00.094+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-04-02T21:11:00.294+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-04-02T21:11:00.571+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.285.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:11:00.620+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-04-02T21:11:00.629+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1051373791.1516240898.237431>,
                               inet_tcp_dist,<0.281.0>,
                               #Ref<0.1051373791.1516240898.237433>}
[error_logger:info,2020-04-02T21:11:00.629+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.281.0>,connection_closed}}
[ns_server:info,2020-04-02T21:11:00.771+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.274.0>:ns_port_server:log:224]ns_couchdb<0.274.0>: Apache CouchDB  (LogLevel=info) is starting.
ns_couchdb<0.274.0>: Failure to start Mochiweb: eaddrinuse
ns_couchdb<0.274.0>: 13145: Booted. Waiting for shutdown request
ns_couchdb<0.274.0>: [os_mon] memory supervisor port (memsup): Erlang has closed
ns_couchdb<0.274.0>: [os_mon] cpu supervisor port (cpu_sup): Erlang has closed

[error_logger:info,2020-04-02T21:11:00.820+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:00.820+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T21:11:00.821+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1051373791.1516240898.237456>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:11:00.821+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1051373791.1516240898.237456>,
                                  inet_tcp_dist,<0.287.0>,
                                  #Ref<0.1051373791.1516240899.237688>}
[ns_server:debug,2020-04-02T21:11:00.822+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1051373791.1516240898.237456>,
                               inet_tcp_dist,<0.287.0>,
                               #Ref<0.1051373791.1516240899.237688>}
[ns_server:debug,2020-04-02T21:11:00.822+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-04-02T21:11:00.822+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.287.0>,shutdown}}
[error_logger:info,2020-04-02T21:11:00.823+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:01.023+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T21:11:01.023+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:01.024+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1051373791.1516240899.237693>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:11:01.024+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1051373791.1516240899.237693>,
                                  inet_tcp_dist,<0.290.0>,
                                  #Ref<0.1051373791.1516240897.238628>}
[ns_server:debug,2020-04-02T21:11:01.025+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-04-02T21:11:01.025+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1051373791.1516240899.237693>,
                               inet_tcp_dist,<0.290.0>,
                               #Ref<0.1051373791.1516240897.238628>}
[error_logger:info,2020-04-02T21:11:01.025+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.290.0>,shutdown}}
[error_logger:info,2020-04-02T21:11:01.025+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-04-02T21:11:01.226+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:01.227+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T21:11:01.227+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1051373791.1516240899.237709>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:11:01.227+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1051373791.1516240899.237709>,
                                  inet_tcp_dist,<0.293.0>,
                                  #Ref<0.1051373791.1516240899.237713>}
[ns_server:debug,2020-04-02T21:11:01.228+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-04-02T21:11:01.228+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1051373791.1516240899.237709>,
                               inet_tcp_dist,<0.293.0>,
                               #Ref<0.1051373791.1516240899.237713>}
[error_logger:info,2020-04-02T21:11:01.228+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.293.0>,shutdown}}
[error_logger:info,2020-04-02T21:11:01.228+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:info,2020-04-02T21:11:01.312+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.274.0>:ns_port_server:log:224]ns_couchdb<0.274.0>: {"Kernel pid terminated",application_controller,"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\"},{line,102}]},{supervisor,do_start_child,2,[{file,\"supervisor.erl\"},{line,365}]},{supervisor,start_children,3,[{file,\"supervisor.erl\"},{line,348}]},{supervisor,init_children,2,[{file,\"supervisor.erl\"},{line,314}]},{gen_server,init_it,2,[{file,\"gen_server.erl\"},{line,365}]},{gen_server,init_it,6,[{file,\"gen_server.erl\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}"}
ns_couchdb<0.274.0>: Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro
ns_couchdb<0.274.0>: 
ns_couchdb<0.274.0>: Crash dump is being written to: erl_crash.dump.1585842038.12600.ns_couchdb...done

[ns_server:error,2020-04-02T21:11:01.316+05:30,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.275.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:189]ns_couchdb_port(<0.274.0>) died with reason {abnormal,1}
[ns_server:debug,2020-04-02T21:11:01.316+05:30,ns_1@127.0.0.1:<0.269.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.267.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:11:01.316+05:30,ns_1@127.0.0.1:<0.268.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.267.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:11:01.316+05:30,ns_1@127.0.0.1:<0.266.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.264.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:11:01.316+05:30,ns_1@127.0.0.1:<0.265.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.264.0>} exited with reason shutdown
[error_logger:error,2020-04-02T21:11:01.312+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]** Generic server ns_couchdb_port terminating 
** Last message in was {#Port<0.5097>,{exit_status,1}}
** When Server state == {state,#Port<0.5097>,
                            {ns_couchdb,"/opt/couchbase/lib/erlang/bin/erl",
                                ["-pa",
                                 "/opt/couchbase/lib/erlang/lib/asn1-5.0.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/compiler-7.1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEvent-2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEventDomain-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosFileTransfer-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosNotification-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosProperty-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTime-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTransactions-1.3.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/crypto-4.2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/dialyzer-3.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/diameter-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/edoc-0.9.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eldap-1.2.3.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_docgen-0.7.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_interface-3.10.2.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erts-9.3.3.9/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eunit-2.3.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/hipe-3.17.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ic-4.4.4.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/inets-6.5.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/mnesia-4.15.3.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/orber-3.8.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/os_mon-2.4.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/otp_mibs-1.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/parsetools-2.1.6/ebin",
                                 "/opt/couchbase/lib/erlang/lib/public_key-1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/reltool-0.7.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/runtime_tools-1.12.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/sasl-3.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/snmp-5.2.11/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssh-4.6.9.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssl-8.2.6.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/syntax_tools-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/tools-2.11.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/xmerl-1.3.16.1/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/gc-couchbase-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/vtree-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/wkb-1.2.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch-1.2.0a-961ad59-git/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_audit-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_dcp-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_index_merger-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_set_view-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_view_parser-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/ejson-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/erlang-oauth/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/etap/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/lhttpc-1.3/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mapreduce-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mochiweb-1.4.1/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/snappy-1.0.4/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ale/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/gen_smtp/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_babysitter/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_couchdb/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/ebin",
                                 "/opt/couchbase/lib/erlang/lib/stdlib-3.4.5.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/kernel-5.4.3.2/ebin",
                                 ".","-couch_ini",
                                 "/opt/couchbase/etc/couchdb/default.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/capi.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/geocouch.ini",
                                 "/opt/couchbase/etc/couchdb/local.ini",
                                 "-kernel","error_logger","false","-kernel",
                                 "error_logger","false","inetrc",
                                 "\"/opt/couchbase/etc/couchbase/hosts.cfg\"",
                                 "dist_config_file",
                                 "\"/opt/couchbase/var/lib/couchbase/config/dist_cfg\"",
                                 "-ssl_dist_optfile",
                                 "/opt/couchbase/etc/couchbase/ssl_dist_opts",
                                 "-setcookie",
                                 "2a158eb185476066b502c8ced82b23b042bd1ae5f1ef74f024e7bb9916796a9b",
                                 "-name","couchdb_ns_1@cb.local","-smp",
                                 "enable","+P","327680","+K","true","-kernel",
                                 "error_logger","false","-sasl",
                                 "sasl_error_logger","false","-nouser",
                                 "-hidden","-proto_dist","cb","-epmd_module",
                                 "cb_epmd","-start_epmd","false","-run",
                                 "child_erlang","child_start","ns_couchdb"],
                                [use_stdio,
                                 {env,
                                     [{"NS_COUCHDB_ENV_ARGS",
                                       "[{ns_server_node,'ns_1@127.0.0.1'},\n {path_config_tmpdir,\"/opt/couchbase/var/lib/couchbase/tmp\"},\n {net_kernel_verbosity,10},\n {loglevel_error_logger,debug},\n {path_config_libdir,\"/opt/couchbase/lib\"},\n {loglevel_stats,debug},\n {loglevel_menelaus,debug},\n {path_config_secdir,\"/opt/couchbase/etc/security\"},\n {loglevel_user,debug},\n {path_config_etcdir,\"/opt/couchbase/etc/couchbase\"},\n {loglevel_ns_server,debug},\n {loglevel_mapreduce_errors,debug},\n {loglevel_rebalance,debug},\n {loglevel_default,debug},\n {disk_sink_opts,[{rotation,[{compress,true},\n                             {size,41943040},\n                             {num_files,10},\n                             {buffer_size_max,52428800}]}]},\n {loglevel_cbas,debug},\n {loglevel_xdcr,debug},\n {loglevel_ns_doctor,debug},\n {loglevel_access,info},\n {error_logger_mf_dir,\"/opt/couchbase/var/lib/couchbase/logs\"},\n {path_config_datadir,\"/opt/couchbase/var/lib/couchbase\"},\n {loglevel_cluster,debug},\n {loglevel_couchdb,info},\n {loglevel_views,debug},\n {path_config_bindir,\"/opt/couchbase/bin\"}]"},
                                      {"ERL_CRASH_DUMP",
                                       "erl_crash.dump.1585842038.12600.ns_couchdb"}]}]},
                            {ringbuffer,1191,1024,
                                {[{<<"Crash dump is being written to: erl_crash.dump.1585842038.12600.ns_couchdb...done">>,
                                   81},
                                  {<<>>,0},
                                  {<<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                                   200}],
                                 [{<<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>,
                                   910}]}},
                            undefined,
                            {ok,{-576460749554,
                                 #Ref<0.1051373791.1516240899.237700>}},
                            [<<"Crash dump is being written to: erl_crash.dump.1585842038.12600.ns_couchdb...done">>,
                             <<>>,
                             <<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                             <<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>],
                            0}
** Reason for termination == 
** {abnormal,1}

[ns_server:debug,2020-04-02T21:11:01.317+05:30,ns_1@127.0.0.1:<0.257.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.256.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:11:01.318+05:30,ns_1@127.0.0.1:<0.218.0>:restartable:shutdown_child:120]Successfully terminated process <0.219.0>
[ns_server:debug,2020-04-02T21:11:01.318+05:30,ns_1@127.0.0.1:<0.217.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.216.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:11:01.318+05:30,ns_1@127.0.0.1:<0.203.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.202.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:11:01.319+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:wait_saver:866]Done waiting for saver.
[error_logger:error,2020-04-02T21:11:01.321+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_port_server:init/1
    pid: <0.274.0>
    registered_name: ns_couchdb_port
    exception exit: {abnormal,1}
      in function  gen_server:handle_common_reply/8 (gen_server.erl, line 726)
    ancestors: [ns_server_nodes_sup,<0.208.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 1
    messages: [{'EXIT',#Port<0.5097>,normal}]
    links: [<0.209.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 11924
  neighbours:

[error_logger:error,2020-04-02T21:11:01.323+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: erlang:apply/2
    pid: <0.275.0>
    registered_name: wait_link_to_couchdb_node
    exception exit: {abnormal,1}
      in function  ns_server_nodes_sup:do_wait_link_to_couchdb_node/1 (src/ns_server_nodes_sup.erl, line 190)
    ancestors: [ns_server_nodes_sup,<0.208.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.209.0>,<0.276.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 3351
  neighbours:
    neighbour:
      pid: <0.276.0>
      registered_name: []
      initial call: ns_server_nodes_sup:'-do_wait_link_to_couchdb_node/1-fun-2-'/0
      current_function: {timer,sleep,1}
      ancestors: [wait_link_to_couchdb_node,ns_server_nodes_sup,<0.208.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
      message_queue_len: 0
      links: [<0.275.0>]
      trap_exit: false
      status: waiting
      heap_size: 2586
      stack_size: 12
      reductions: 10853
      current_stacktrace: [{timer,sleep,1,[{file,"timer.erl"},{line,153}]},
                  {misc,poll_for_condition_rec,3,
                      [{file,"src/misc.erl"},{line,508}]},
                  {ns_server_nodes_sup,
                      '-do_wait_link_to_couchdb_node/1-fun-2-',2,
                      [{file,"src/ns_server_nodes_sup.erl"},{line,159}]},
                  {proc_lib,init_p,3,[{file,"proc_lib.erl"},{line,232}]}]

[error_logger:error,2020-04-02T21:11:01.323+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    start_error
     Reason:     {abnormal,1}
     Offender:   [{pid,undefined},
                  {name,wait_for_couchdb_node},
                  {mfargs,{erlang,apply,
                                  [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:error,2020-04-02T21:11:01.323+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    shutdown_error
     Reason:     {abnormal,1}
     Offender:   [{pid,<0.274.0>},
                  {name,start_couchdb_node},
                  {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                  {restart_type,{permanent,5}},
                  {shutdown,86400000},
                  {child_type,worker}]


[error_logger:error,2020-04-02T21:11:01.324+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_cluster_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,wait_for_couchdb_node,
                         {abnormal,1}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_nodes_sup},
                  {mfargs,
                      {restartable,start_link,
                          [{ns_server_nodes_sup,start_link,[]},infinity]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-04-02T21:11:01.324+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,root_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                             {failed_to_start_child,wait_for_couchdb_node,
                                 {abnormal,1}}}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_cluster_sup},
                  {mfargs,{ns_server_cluster_sup,start_link,[]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-04-02T21:11:01.325+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: application_master:init/4
    pid: <0.117.0>
    registered_name: []
    exception exit: {{shutdown,
                      {failed_to_start_child,ns_server_cluster_sup,
                       {shutdown,
                        {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                          {failed_to_start_child,wait_for_couchdb_node,
                           {abnormal,1}}}}}}},
                     {ns_server,start,[normal,[]]}}
      in function  application_master:init/4 (application_master.erl, line 134)
    ancestors: [<0.116.0>]
    message_queue_len: 1
    messages: [{'EXIT',<0.118.0>,normal}]
    links: [<0.116.0>,<0.33.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 274
  neighbours:

[error_logger:info,2020-04-02T21:11:01.325+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
         application: ns_server
              exited: {{shutdown,
                        {failed_to_start_child,ns_server_cluster_sup,
                         {shutdown,
                          {failed_to_start_child,ns_server_nodes_sup,
                           {shutdown,
                            {failed_to_start_child,wait_for_couchdb_node,
                             {abnormal,1}}}}}}},
                       {ns_server,start,[normal,[]]}}
                type: permanent

[error_logger:info,2020-04-02T21:11:01.326+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,system_memory_high_watermark}

[error_logger:info,2020-04-02T21:11:01.326+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-calculator/544"}}

[error_logger:info,2020-04-02T21:11:01.326+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/8935"}}

[error_logger:info,2020-04-02T21:11:01.326+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-calculator/704"}}

[error_logger:info,2020-04-02T21:11:01.326+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/59"}}

[error_logger:info,2020-04-02T21:11:01.326+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/pycharm-community/188"}}

[error_logger:info,2020-04-02T21:11:01.326+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gnome-system-monitor/127"}}

[error_logger:info,2020-04-02T21:11:01.326+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core18/1705"}}

[error_logger:info,2020-04-02T21:11:01.327+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/8689"}}

[error_logger:info,2020-04-02T21:11:01.327+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gnome-system-monitor/135"}}

[error_logger:info,2020-04-02T21:11:01.327+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-characters/495"}}

[error_logger:info,2020-04-02T21:11:01.327+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-28-1804/116"}}

[error_logger:info,2020-04-02T21:11:01.328+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gtk-common-themes/1474"}}

[error_logger:info,2020-04-02T21:11:01.328+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core18/1668"}}

[error_logger:info,2020-04-02T21:11:01.328+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-logs/81"}}

[error_logger:info,2020-04-02T21:11:01.328+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-logs/93"}}

[error_logger:info,2020-04-02T21:11:01.328+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/98"}}

[error_logger:info,2020-04-02T21:11:01.328+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-characters/399"}}

[error_logger:info,2020-04-02T21:11:01.329+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gtk-common-themes/1440"}}

[ns_server:info,2020-04-02T21:11:08.736+05:30,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-04-02T21:11:08.748+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-04-02T21:11:08.748+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.748+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.748+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.748+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.748+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.748+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.748+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.748+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.749+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.749+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.749+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.749+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.749+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.749+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.749+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.749+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.749+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.749+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.749+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.749+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.749+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.749+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.749+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.749+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.749+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-04-02T21:11:08.749+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-04-02T21:11:08.753+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-04-02T21:11:08.753+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[error_logger:info,2020-04-02T21:11:08.754+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.168.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2020-04-02T21:11:08.754+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.167.0>},
                       {id,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:11:08.756+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@127.0.0.1'
[error_logger:info,2020-04-02T21:11:08.763+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.763+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.764+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:08.765+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.174.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.766+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.176.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T21:11:08.767+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2020-04-02T21:11:08.767+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.177.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:08.767+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.175.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:08.767+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:08.768+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.768+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.769+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.180.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:11:08.770+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-04-02T21:11:08.770+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-04-02T21:11:08.771+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2020-04-02T21:11:08.772+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-04-02T21:11:08.772+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.181.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.772+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.169.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-04-02T21:11:08.773+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-04-02T21:11:08.779+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-04-02T21:11:08.779+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2020-04-02T21:11:08.779+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:08.779+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T21:11:08.779+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2292806322.3932160002.222212>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:11:08.779+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2292806322.3932160002.222212>,
                                  inet_tcp_dist,<0.185.0>,
                                  #Ref<0.2292806322.3932160002.222217>}
[ns_server:debug,2020-04-02T21:11:08.781+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-04-02T21:11:08.781+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2020-04-02T21:11:08.781+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-04-02T21:11:08.783+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-04-02T21:11:08.784+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.798+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:11:08.803+05:30,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,4,2},{21,11,8}}},
               {memory,
                   [{total,26267888},
                    {processes,9479616},
                    {processes_used,9473776},
                    {system,16788272},
                    {atom,388625},
                    {atom_used,364409},
                    {binary,107184},
                    {code,8250921},
                    {ets,1509152}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    error_handler,kernel,application_master,code,error_logger,
                    application,gen_event,application_controller,file_server,
                    file_io_server,heart,ets,gen,file,code_server,erl_lint,
                    proc_lib,filename,gen_server,erl_eval,supervisor,lists,
                    erl_parse,erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{os_mon,"CPO  CXC 138 46","2.4.4"},
                    {sasl,"SASL  CXC 138 11","3.1.2"},
                    {ns_server,"Couchbase server","6.5.0-4960-enterprise"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ale,"Another Logger for Erlang","0.0.0"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,131},
               {node,'ns_1@127.0.0.1'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,auth,httpd_sup,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    kernel_safe_sup,release_handler,lhttpc_sup,httpc_sup,
                    lhttpc_manager,alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    'sink-disk_xdcr',timer_server,'sink-disk_debug',
                    inet_gethost_native,ale_sup,'sink-disk_error',inet_db,
                    'sink-disk_default',ssl_pem_cache_dist,ale_dynamic_sup,
                    rex,global_group,net_sup,kernel_sup,ssl_connection_sup,
                    global_name_server,ssl_admin_sup,tftp_sup,ssl_sup,
                    root_sup,erts_code_purger,os_mon_sup,file_server_2,
                    error_logger,cpu_sup,erl_epmd,init,memsup,
                    erl_signal_server,disksup,ale,net_kernel,dist_manager,
                    ssl_pem_cache,ssl_manager,ssl_dist_admin_sup,
                    ssl_dist_connection_sup,ssl_dist_sup,user,
                    ssl_tls_dist_proxy,ssl_manager_dist,sasl_safe_sup,
                    ssl_listen_tracker_sup,inet_gethost_native_sup,
                    code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,0}]
[ns_server:info,2020-04-02T21:11:08.811+05:30,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"cfa0f75f28402d2e1aa254b2a374bead19433526\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" upstream=\"master\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"f2a16b53bb74146f20d18ba2c0443d5f10a9a550\" upstream=\"master\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4960\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" upstream=\"master\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" upstream=\"mad-hatter\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" upstream=\"master\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" upstream=\"master\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" upstream=\"master\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" upstream=\"mad-hatter\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" upstream=\"cb-master\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" upstream=\"master\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" upstream=\"master\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" upstream=\"mad-hatter\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" upstream=\"mad-hatter\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" upstream=\"mad-hatter\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" upstream=\"master\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" upstream=\"master\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" upstream=\"mad-hatter\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" upstream=\"mad-hatter\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" upstream=\"mad-hatter\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" upstream=\"mad-hatter\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" upstream=\"master\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" upstream=\"master\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" upstream=\"master\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" upstream=\"mad-hatter\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" upstream=\"master\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"7a3aa547b3f5eb3ea28d279a08384609cd2cea7c\" upstream=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" upstream=\"mad-hatter\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" upstream=\"master\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" upstream=\"master\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" upstream=\"master\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" upstream=\"v1.1.0-couchbase\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"ee64d41320d14fabe814a241a5cf4f6a6f6e827a\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" upstream=\"master\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" upstream=\"master\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-04-02T21:11:08.815+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.816+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.816+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:11:08.817+05:30,ns_1@127.0.0.1:ns_config_sup<0.192.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-04-02T21:11:08.817+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.817+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.194.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:11:08.835+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-04-02T21:11:08.836+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2020-04-02T21:11:08.842+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[{alert_limits,
   [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
  {audit,
   [{auditd_enabled,false},
    {rotate_interval,86400},
    {rotate_size,20971520},
    {disabled,[]},
    {sync,[]},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
  {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
  {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
  {autocompaction,
   [{database_fragmentation_threshold,{30,undefined}},
    {view_fragmentation_threshold,{30,undefined}}]},
  {buckets,[{configs,[]}]},
  {cbas_memory_quota,2174},
  {cert_and_pkey,
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFgIK71cHor8wDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBkZTZmMzM0MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgZGU2ZjMz\nNDAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC6Epk+5C0GfEqGHL9d\nxsySLywt3gLcVQmCM8lgcMRGWDaGVF6iOP+QyLODyB09I5u2gOcVm+1r3eOZ4rwk\nbttVmFIsdroNf2jG+9baY4LqKoDyZnjZr0LeolUcY+0eYI68oNwRMgWp53Krm861\ny11yyOEjefm+JBDhZuZpHZegjTBKtYqZd96WZwOzbrJZrau3uKBuQTmoEdpZ4VdX\n6U5nzUaRkvjjuBpQyeqMLSuuLUO4FENp1C8P9fYhy4Y6RRZfMSBGdyw1d8QEWxiU\n4n/rtfQgiN32qOwtY7ocwvaXDV7wH1ipWkPF5Vn8eyBi5cA2xqgaq1xSBLD8MUHE\nXTAjAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCP9ajveEq01YMq\n/zClEAjE3TCbGqz9u/vjXdhSQK7rPJLcK250d86L6njzkS2ffrabbOGON+4UvNW4\nTUub3JqnTuSlI8B6riH61kqWPfCfRC392v1xAIaQI1/jWsW4HQoiXbmi0uiKrsEq\nIt8XF5nLXDsEeWYetynrODdVU9ADeDNkE2+AOyLTvD/4eUDRoQhDhC5vh75Bu9gm\nEV+efNKCwXjs4xAMPGbKoNnWBkx7Btn0+iyI19l+jrzF1rlDaH6pFz2ldqm6CL+f\n26ZCU9S8uXPNC7UiNXr6DZj1sn/k0qqebDRnHlO2P+wYp5G/+Rca+B41diWCV7xG\ncnfTf1PH\n-----END CERTIFICATE-----\n">>,
     <<"*****">>}]},
  {drop_request_memory_threshold_mib,undefined},
  {email_alerts,
   [{recipients,["root@localhost"]},
    {sender,"couchbase@localhost"},
    {enabled,false},
    {email_server,
     [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
    {alerts,
     [auto_failover_node,auto_failover_maximum_reached,
      auto_failover_other_nodes_down,auto_failover_cluster_too_small,
      auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
      ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
      ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
  {fts_memory_quota,512},
  {index_aware_rebalance_disabled,false},
  {log_redaction_default_cfg,[{redact_level,none}]},
  {max_bucket_count,30},
  {memcached,[]},
  {memory_quota,8886},
  {nodes_wanted,['ns_1@127.0.0.1']},
  {password_policy,[{min_length,6},{must_present,[]}]},
  {quorum_nodes,['ns_1@127.0.0.1']},
  {remote_clusters,[]},
  {replication,[{enabled,true}]},
  {rest,[{port,8091}]},
  {rest_creds,null},
  {secure_headers,[]},
  {server_groups,
   [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
  {set_view_update_daemon,
   [{update_interval,5000},
    {update_min_changes,5000},
    {replica_update_min_changes,5000}]},
  {{couchdb,max_parallel_indexers},4},
  {{couchdb,max_parallel_replica_indexers},2},
  {{local_changes_count,<<"8c43a5102cad1e34db659ab4d5646878">>},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{4,63753061259}}]}]},
  {{metakv,<<"/indexing/settings/config">>},
   <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
  {{request_limit,capi},undefined},
  {{request_limit,rest},undefined},
  {{node,'ns_1@127.0.0.1',address_family},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    inet]},
  {{node,'ns_1@127.0.0.1',audit},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
  {{node,'ns_1@127.0.0.1',capi_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8092]},
  {{node,'ns_1@127.0.0.1',cbas_admin_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9110]},
  {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9113]},
  {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9112]},
  {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9111]},
  {{node,'ns_1@127.0.0.1',cbas_cluster_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9115]},
  {{node,'ns_1@127.0.0.1',cbas_console_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9114]},
  {{node,'ns_1@127.0.0.1',cbas_data_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9116]},
  {{node,'ns_1@127.0.0.1',cbas_debug_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    -1]},
  {{node,'ns_1@127.0.0.1',cbas_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8095]},
  {{node,'ns_1@127.0.0.1',cbas_messaging_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9118]},
  {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9119]},
  {{node,'ns_1@127.0.0.1',cbas_metadata_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9121]},
  {{node,'ns_1@127.0.0.1',cbas_parent_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9122]},
  {{node,'ns_1@127.0.0.1',cbas_replication_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9120]},
  {{node,'ns_1@127.0.0.1',cbas_result_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9117]},
  {{node,'ns_1@127.0.0.1',cbas_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18095]},
  {{node,'ns_1@127.0.0.1',compaction_daemon},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@127.0.0.1',config_version},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    {6,5}]},
  {{node,'ns_1@127.0.0.1',erl_external_listeners},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {inet,false},
    {inet6,false}]},
  {{node,'ns_1@127.0.0.1',eventing_debug_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9140]},
  {{node,'ns_1@127.0.0.1',eventing_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8096]},
  {{node,'ns_1@127.0.0.1',eventing_https_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18096]},
  {{node,'ns_1@127.0.0.1',fts_grpc_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9130]},
  {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    19130]},
  {{node,'ns_1@127.0.0.1',fts_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8094]},
  {{node,'ns_1@127.0.0.1',fts_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18094]},
  {{node,'ns_1@127.0.0.1',indexer_admin_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9100]},
  {{node,'ns_1@127.0.0.1',indexer_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9102]},
  {{node,'ns_1@127.0.0.1',indexer_https_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    19102]},
  {{node,'ns_1@127.0.0.1',indexer_scan_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9101]},
  {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9104]},
  {{node,'ns_1@127.0.0.1',indexer_stinit_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9103]},
  {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9105]},
  {{node,'ns_1@127.0.0.1',is_enterprise},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    true]},
  {{node,'ns_1@127.0.0.1',isasl},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@127.0.0.1',membership},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    active]},
  {{node,'ns_1@127.0.0.1',memcached},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {port,11210},
    {dedicated_port,11209},
    {dedicated_ssl_port,11206},
    {ssl_port,11207},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_sleeptime,19},
    {log_rotation_period,39003}]},
  {{node,'ns_1@127.0.0.1',memcached_config},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    {[{interfaces,
       {memcached_config_mgr,omit_missing_mcd_ports,
        [{[{host,<<"*">>},
           {port,port},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_port},
           {system,true},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,ssl_port},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_ssl_port},
           {system,true},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
      {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
      {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {opentracing,
       {[{enabled,opentracing_enabled},
         {module,{"~s",[opentracing_module]}},
         {config,{"~s",[opentracing_config]}}]}},
      {admin,{"~s",[admin_user]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize},
         {sleeptime,log_sleeptime}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
  {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    11206]},
  {{node,'ns_1@127.0.0.1',memcached_defaults},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {max_connections,65000},
    {system_connections,5000},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {opentracing_enabled,false},
    {opentracing_module,[]},
    {opentracing_config,[]},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {tracing_enabled,true},
    {datatype_snappy,true},
    {num_reader_threads,<<"default">>},
    {num_writer_threads,<<"default">>}]},
  {{node,'ns_1@127.0.0.1',moxi},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {port,0}]},
  {{node,'ns_1@127.0.0.1',node_encryption},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    false]},
  {{node,'ns_1@127.0.0.1',ns_log},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@127.0.0.1',port_servers},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
  {{node,'ns_1@127.0.0.1',projector_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9999]},
  {{node,'ns_1@127.0.0.1',projector_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9999]},
  {{node,'ns_1@127.0.0.1',query_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8093]},
  {{node,'ns_1@127.0.0.1',rest},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@127.0.0.1',saslauthd_enabled},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    true]},
  {{node,'ns_1@127.0.0.1',ssl_capi_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18092]},
  {{node,'ns_1@127.0.0.1',ssl_query_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18093]},
  {{node,'ns_1@127.0.0.1',ssl_rest_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18091]},
  {{node,'ns_1@127.0.0.1',uuid},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    <<"8c43a5102cad1e34db659ab4d5646878">>]},
  {{node,'ns_1@127.0.0.1',xdcr_rest_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9998]},
  {{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    false]}]]
[ns_server:info,2020-04-02T21:11:08.847+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',xdcr_rest_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9998]},
 {{node,'ns_1@127.0.0.1',uuid},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   <<"8c43a5102cad1e34db659ab4d5646878">>]},
 {{node,'ns_1@127.0.0.1',ssl_rest_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18091]},
 {{node,'ns_1@127.0.0.1',ssl_query_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18093]},
 {{node,'ns_1@127.0.0.1',ssl_capi_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18092]},
 {{node,'ns_1@127.0.0.1',saslauthd_enabled},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',rest},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@127.0.0.1',query_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8093]},
 {{node,'ns_1@127.0.0.1',projector_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',projector_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',port_servers},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
 {{node,'ns_1@127.0.0.1',ns_log},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@127.0.0.1',node_encryption},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',moxi},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {port,0}]},
 {{node,'ns_1@127.0.0.1',memcached_defaults},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   11206]},
 {{node,'ns_1@127.0.0.1',memcached_config},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@127.0.0.1',memcached},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@127.0.0.1',membership},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   active]},
 {{node,'ns_1@127.0.0.1',isasl},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@127.0.0.1',is_enterprise},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9105]},
 {{node,'ns_1@127.0.0.1',indexer_stinit_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9103]},
 {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9104]},
 {{node,'ns_1@127.0.0.1',indexer_scan_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9101]},
 {{node,'ns_1@127.0.0.1',indexer_https_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   19102]},
 {{node,'ns_1@127.0.0.1',indexer_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9102]},
 {{node,'ns_1@127.0.0.1',indexer_admin_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9100]},
 {{node,'ns_1@127.0.0.1',fts_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18094]},
 {{node,'ns_1@127.0.0.1',fts_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8094]},
 {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   19130]},
 {{node,'ns_1@127.0.0.1',fts_grpc_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9130]},
 {{node,'ns_1@127.0.0.1',eventing_https_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18096]},
 {{node,'ns_1@127.0.0.1',eventing_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8096]},
 {{node,'ns_1@127.0.0.1',eventing_debug_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9140]},
 {{node,'ns_1@127.0.0.1',erl_external_listeners},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {inet,false},
   {inet6,false}]},
 {{node,'ns_1@127.0.0.1',config_version},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   {6,5}]},
 {{node,'ns_1@127.0.0.1',compaction_daemon},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@127.0.0.1',cbas_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18095]},
 {{node,'ns_1@127.0.0.1',cbas_result_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9117]},
 {{node,'ns_1@127.0.0.1',cbas_replication_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9120]},
 {{node,'ns_1@127.0.0.1',cbas_parent_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9122]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9121]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9119]},
 {{node,'ns_1@127.0.0.1',cbas_messaging_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9118]},
 {{node,'ns_1@127.0.0.1',cbas_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8095]},
 {{node,'ns_1@127.0.0.1',cbas_debug_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|-1]},
 {{node,'ns_1@127.0.0.1',cbas_data_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9116]},
 {{node,'ns_1@127.0.0.1',cbas_console_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9114]},
 {{node,'ns_1@127.0.0.1',cbas_cluster_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9115]},
 {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9111]},
 {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9112]},
 {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9113]},
 {{node,'ns_1@127.0.0.1',cbas_admin_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9110]},
 {{node,'ns_1@127.0.0.1',capi_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8092]},
 {{node,'ns_1@127.0.0.1',audit},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
 {{node,'ns_1@127.0.0.1',address_family},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   inet]},
 {{request_limit,rest},undefined},
 {{request_limit,capi},undefined},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{local_changes_count,<<"8c43a5102cad1e34db659ab4d5646878">>},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{4,63753061259}}]}]},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
 {secure_headers,[]},
 {rest_creds,null},
 {rest,[{port,8091}]},
 {replication,[{enabled,true}]},
 {remote_clusters,[]},
 {quorum_nodes,['ns_1@127.0.0.1']},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {nodes_wanted,['ns_1@127.0.0.1']},
 {memory_quota,8886},
 {memcached,[]},
 {max_bucket_count,30},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {index_aware_rebalance_disabled,false},
 {fts_memory_quota,512},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {drop_request_memory_threshold_mib,undefined},
 {cert_and_pkey,
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFgIK71cHor8wDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBkZTZmMzM0MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgZGU2ZjMz\nNDAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC6Epk+5C0GfEqGHL9d\nxsySLywt3gLcVQmCM8lgcMRGWDaGVF6iOP+QyLODyB09I5u2gOcVm+1r3eOZ4rwk\nbttVmFIsdroNf2jG+9baY4LqKoDyZnjZr0LeolUcY+0eYI68oNwRMgWp53Krm861\ny11yyOEjefm+JBDhZuZpHZegjTBKtYqZd96WZwOzbrJZrau3uKBuQTmoEdpZ4VdX\n6U5nzUaRkvjjuBpQyeqMLSuuLUO4FENp1C8P9fYhy4Y6RRZfMSBGdyw1d8QEWxiU\n4n/rtfQgiN32qOwtY7ocwvaXDV7wH1ipWkPF5Vn8eyBi5cA2xqgaq1xSBLD8MUHE\nXTAjAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCP9ajveEq01YMq\n/zClEAjE3TCbGqz9u/vjXdhSQK7rPJLcK250d86L6njzkS2ffrabbOGON+4UvNW4\nTUub3JqnTuSlI8B6riH61kqWPfCfRC392v1xAIaQI1/jWsW4HQoiXbmi0uiKrsEq\nIt8XF5nLXDsEeWYetynrODdVU9ADeDNkE2+AOyLTvD/4eUDRoQhDhC5vh75Bu9gm\nEV+efNKCwXjs4xAMPGbKoNnWBkx7Btn0+iyI19l+jrzF1rlDaH6pFz2ldqm6CL+f\n26ZCU9S8uXPNC7UiNXr6DZj1sn/k0qqebDRnHlO2P+wYp5G/+Rca+B41diWCV7xG\ncnfTf1PH\n-----END CERTIFICATE-----\n">>,
    <<"*****">>}]},
 {cbas_memory_quota,2174},
 {buckets,[{configs,[]}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]}]
[error_logger:info,2020-04-02T21:11:08.851+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.195.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.852+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.201.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.853+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.202.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.853+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T21:11:08.855+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"8c43a5102cad1e34db659ab4d5646878">>} ->
[{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{5,63753061268}}]}]
[error_logger:info,2020-04-02T21:11:08.855+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.204.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:08.856+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.207.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:08.860+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:11:08.861+05:30,ns_1@127.0.0.1:menelaus_barrier<0.211.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-04-02T21:11:08.861+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.861+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.863+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.213.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.863+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.215.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:11:08.871+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2020-04-02T21:11:08.871+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2020-04-02T21:11:08.871+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2020-04-02T21:11:08.883+05:30,ns_1@127.0.0.1:ns_ssl_services_setup<0.216.0>:ns_ssl_services_setup:init:462]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,2,10,239,87,7,162,
             191,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
             6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
             118,101,114,32,100,101,54,102,51,51,52,48,48,30,23,13,49,51,48,
             49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
             57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
             104,98,97,115,101,32,83,101,114,118,101,114,32,100,101,54,102,
             51,51,52,48,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,
             0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,186,18,153,62,228,45,6,
             124,74,134,28,191,93,198,204,146,47,44,45,222,2,220,85,9,130,51,
             201,96,112,196,70,88,54,134,84,94,162,56,255,144,200,179,131,
             200,29,61,35,155,182,128,231,21,155,237,107,221,227,153,226,188,
             36,110,219,85,152,82,44,118,186,13,127,104,198,251,214,218,99,
             130,234,42,128,242,102,120,217,175,66,222,162,85,28,99,237,30,
             96,142,188,160,220,17,50,5,169,231,114,171,155,206,181,203,93,
             114,200,225,35,121,249,190,36,16,225,102,230,105,29,151,160,141,
             48,74,181,138,153,119,222,150,103,3,179,110,178,89,173,171,183,
             184,160,110,65,57,168,17,218,89,225,87,87,233,78,103,205,70,145,
             146,248,227,184,26,80,201,234,140,45,43,174,45,67,184,20,67,105,
             212,47,15,245,246,33,203,134,58,69,22,95,49,32,70,119,44,53,119,
             196,4,91,24,148,226,127,235,181,244,32,136,221,246,168,236,45,
             99,186,28,194,246,151,13,94,240,31,88,169,90,67,197,229,89,252,
             123,32,98,229,192,54,198,168,26,171,92,82,4,176,252,49,65,196,
             93,48,35,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,255,4,4,
             3,2,2,164,48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,
             15,6,3,85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,134,72,
             134,247,13,1,1,11,5,0,3,130,1,1,0,143,245,168,239,120,74,180,
             213,131,42,255,48,165,16,8,196,221,48,155,26,172,253,187,251,
             227,93,216,82,64,174,235,60,146,220,43,110,116,119,206,139,234,
             120,243,145,45,159,126,182,155,108,225,142,55,238,20,188,213,
             184,77,75,155,220,154,167,78,228,165,35,192,122,174,33,250,214,
             74,150,61,240,159,68,45,253,218,253,113,0,134,144,35,95,227,90,
             197,184,29,10,34,93,185,162,210,232,138,174,193,42,34,223,23,23,
             153,203,92,59,4,121,102,30,183,41,235,56,55,85,83,208,3,120,51,
             100,19,111,128,59,34,211,188,63,248,121,64,209,161,8,67,132,46,
             111,135,190,65,187,216,38,17,95,158,124,210,130,193,120,236,227,
             16,12,60,102,202,160,217,214,6,76,123,6,217,244,250,44,136,215,
             217,126,142,188,197,214,185,67,104,126,169,23,61,165,118,169,
             186,8,191,159,219,166,66,83,212,188,185,115,205,11,181,34,53,
             122,250,13,152,245,178,127,228,210,170,158,108,52,103,30,83,182,
             63,236,24,167,145,191,249,23,26,248,30,53,118,37,130,87,188,70,
             114,119,211,127,83,199>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2020-04-02T21:11:08.884+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.216.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:11:08.891+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-04-02T21:11:08.891+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-04-02T21:11:08.891+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-04-02T21:11:08.891+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:info,2020-04-02T21:11:08.902+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-04-02T21:11:08.902+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-04-02T21:11:08.902+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-04-02T21:11:08.902+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-04-02T21:11:08.901+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.219.0>,menelaus_web}
             started: [{pid,<0.220.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 2,10,239,87,7,162,191,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,100,101,54,
                                 102,51,51,52,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,100,
                                 101,54,102,51,51,52,48,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,186,18,153,62,228,
                                 45,6,124,74,134,28,191,93,198,204,146,47,44,
                                 45,222,2,220,85,9,130,51,201,96,112,196,70,
                                 88,54,134,84,94,162,56,255,144,200,179,131,
                                 200,29,61,35,155,182,128,231,21,155,237,107,
                                 221,227,153,226,188,36,110,219,85,152,82,44,
                                 118,186,13,127,104,198,251,214,218,99,130,
                                 234,42,128,242,102,120,217,175,66,222,162,
                                 85,28,99,237,30,96,142,188,160,220,17,50,5,
                                 169,231,114,171,155,206,181,203,93,114,200,
                                 225,35,121,249,190,36,16,225,102,230,105,29,
                                 151,160,141,48,74,181,138,153,119,222,150,
                                 103,3,179,110,178,89,173,171,183,184,160,
                                 110,65,57,168,17,218,89,225,87,87,233,78,
                                 103,205,70,145,146,248,227,184,26,80,201,
                                 234,140,45,43,174,45,67,184,20,67,105,212,
                                 47,15,245,246,33,203,134,58,69,22,95,49,32,
                                 70,119,44,53,119,196,4,91,24,148,226,127,
                                 235,181,244,32,136,221,246,168,236,45,99,
                                 186,28,194,246,151,13,94,240,31,88,169,90,
                                 67,197,229,89,252,123,32,98,229,192,54,198,
                                 168,26,171,92,82,4,176,252,49,65,196,93,48,
                                 35,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,
                                 15,1,1,255,4,4,3,2,2,164,48,19,6,3,85,29,37,
                                 4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,
                                 85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,
                                 42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 143,245,168,239,120,74,180,213,131,42,255,
                                 48,165,16,8,196,221,48,155,26,172,253,187,
                                 251,227,93,216,82,64,174,235,60,146,220,43,
                                 110,116,119,206,139,234,120,243,145,45,159,
                                 126,182,155,108,225,142,55,238,20,188,213,
                                 184,77,75,155,220,154,167,78,228,165,35,192,
                                 122,174,33,250,214,74,150,61,240,159,68,45,
                                 253,218,253,113,0,134,144,35,95,227,90,197,
                                 184,29,10,34,93,185,162,210,232,138,174,193,
                                 42,34,223,23,23,153,203,92,59,4,121,102,30,
                                 183,41,235,56,55,85,83,208,3,120,51,100,19,
                                 111,128,59,34,211,188,63,248,121,64,209,161,
                                 8,67,132,46,111,135,190,65,187,216,38,17,95,
                                 158,124,210,130,193,120,236,227,16,12,60,
                                 102,202,160,217,214,6,76,123,6,217,244,250,
                                 44,136,215,217,126,142,188,197,214,185,67,
                                 104,126,169,23,61,165,118,169,186,8,191,159,
                                 219,166,66,83,212,188,185,115,205,11,181,34,
                                 53,122,250,13,152,245,178,127,228,210,170,
                                 158,108,52,103,30,83,182,63,236,24,167,145,
                                 191,249,23,26,248,30,53,118,37,130,87,188,
                                 70,114,119,211,127,83,199>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:11:08.903+05:30,ns_1@127.0.0.1:<0.218.0>:restartable:start_child:98]Started child process <0.219.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2020-04-02T21:11:08.903+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.219.0>,menelaus_web}
             started: [{pid,<0.238.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 2,10,239,87,7,162,191,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,100,101,54,
                                 102,51,51,52,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,100,
                                 101,54,102,51,51,52,48,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,186,18,153,62,228,
                                 45,6,124,74,134,28,191,93,198,204,146,47,44,
                                 45,222,2,220,85,9,130,51,201,96,112,196,70,
                                 88,54,134,84,94,162,56,255,144,200,179,131,
                                 200,29,61,35,155,182,128,231,21,155,237,107,
                                 221,227,153,226,188,36,110,219,85,152,82,44,
                                 118,186,13,127,104,198,251,214,218,99,130,
                                 234,42,128,242,102,120,217,175,66,222,162,
                                 85,28,99,237,30,96,142,188,160,220,17,50,5,
                                 169,231,114,171,155,206,181,203,93,114,200,
                                 225,35,121,249,190,36,16,225,102,230,105,29,
                                 151,160,141,48,74,181,138,153,119,222,150,
                                 103,3,179,110,178,89,173,171,183,184,160,
                                 110,65,57,168,17,218,89,225,87,87,233,78,
                                 103,205,70,145,146,248,227,184,26,80,201,
                                 234,140,45,43,174,45,67,184,20,67,105,212,
                                 47,15,245,246,33,203,134,58,69,22,95,49,32,
                                 70,119,44,53,119,196,4,91,24,148,226,127,
                                 235,181,244,32,136,221,246,168,236,45,99,
                                 186,28,194,246,151,13,94,240,31,88,169,90,
                                 67,197,229,89,252,123,32,98,229,192,54,198,
                                 168,26,171,92,82,4,176,252,49,65,196,93,48,
                                 35,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,
                                 15,1,1,255,4,4,3,2,2,164,48,19,6,3,85,29,37,
                                 4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,
                                 85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,
                                 42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 143,245,168,239,120,74,180,213,131,42,255,
                                 48,165,16,8,196,221,48,155,26,172,253,187,
                                 251,227,93,216,82,64,174,235,60,146,220,43,
                                 110,116,119,206,139,234,120,243,145,45,159,
                                 126,182,155,108,225,142,55,238,20,188,213,
                                 184,77,75,155,220,154,167,78,228,165,35,192,
                                 122,174,33,250,214,74,150,61,240,159,68,45,
                                 253,218,253,113,0,134,144,35,95,227,90,197,
                                 184,29,10,34,93,185,162,210,232,138,174,193,
                                 42,34,223,23,23,153,203,92,59,4,121,102,30,
                                 183,41,235,56,55,85,83,208,3,120,51,100,19,
                                 111,128,59,34,211,188,63,248,121,64,209,161,
                                 8,67,132,46,111,135,190,65,187,216,38,17,95,
                                 158,124,210,130,193,120,236,227,16,12,60,
                                 102,202,160,217,214,6,76,123,6,217,244,250,
                                 44,136,215,217,126,142,188,197,214,185,67,
                                 104,126,169,23,61,165,118,169,186,8,191,159,
                                 219,166,66,83,212,188,185,115,205,11,181,34,
                                 53,122,250,13,152,245,178,127,228,210,170,
                                 158,108,52,103,30,83,182,63,236,24,167,145,
                                 191,249,23,26,248,30,53,118,37,130,87,188,
                                 70,114,119,211,127,83,199>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.903+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.218.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.903+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.214.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:08.907+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.256.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.908+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.259.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.911+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.261.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:11:08.912+05:30,ns_1@127.0.0.1:users_replicator<0.261.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-04-02T21:11:08.913+05:30,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.261.0>
[ns_server:debug,2020-04-02T21:11:08.913+05:30,ns_1@127.0.0.1:users_replicator<0.261.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.262.0>
[ns_server:debug,2020-04-02T21:11:08.914+05:30,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-04-02T21:11:08.914+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.262.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.914+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.260.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T21:11:08.918+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-04-02T21:11:08.918+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.264.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.919+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.267.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:08.920+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.258.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:08.920+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.270.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:08.920+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.271.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:11:08.928+05:30,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-04-02T21:11:08.928+05:30,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 14ms
[ns_server:debug,2020-04-02T21:11:08.929+05:30,ns_1@127.0.0.1:users_replicator<0.261.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2020-04-02T21:11:08.930+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.274.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:11:08.930+05:30,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.275.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[ns_server:debug,2020-04-02T21:11:08.930+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T21:11:08.930+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:08.930+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2292806322.3932160004.221927>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:11:08.931+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2292806322.3932160004.221927>,
                                  inet_tcp_dist,<0.278.0>,
                                  #Ref<0.2292806322.3932160002.222397>}
[ns_server:debug,2020-04-02T21:11:08.931+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2292806322.3932160004.221927>,
                               inet_tcp_dist,<0.278.0>,
                               #Ref<0.2292806322.3932160002.222397>}
[ns_server:debug,2020-04-02T21:11:08.931+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-04-02T21:11:08.931+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.278.0>,shutdown}}
[error_logger:info,2020-04-02T21:11:08.931+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:09.132+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T21:11:09.132+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:09.132+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2292806322.3932160003.222219>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:11:09.132+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2292806322.3932160003.222219>,
                                  inet_tcp_dist,<0.281.0>,
                                  #Ref<0.2292806322.3932160002.222408>}
[ns_server:debug,2020-04-02T21:11:09.164+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-04-02T21:11:09.365+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-04-02T21:11:09.635+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.285.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:11:09.699+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-04-02T21:11:09.712+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.281.0>,connection_closed}}
[ns_server:debug,2020-04-02T21:11:09.712+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2292806322.3932160003.222219>,
                               inet_tcp_dist,<0.281.0>,
                               #Ref<0.2292806322.3932160002.222408>}
[ns_server:info,2020-04-02T21:11:09.836+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.274.0>:ns_port_server:log:224]ns_couchdb<0.274.0>: Apache CouchDB  (LogLevel=info) is starting.
ns_couchdb<0.274.0>: Failure to start Mochiweb: eaddrinuse
ns_couchdb<0.274.0>: 13276: Booted. Waiting for shutdown request
ns_couchdb<0.274.0>: [os_mon] memory supervisor port (memsup): Erlang has closed
ns_couchdb<0.274.0>: [os_mon] cpu supervisor port (cpu_sup): Erlang has closed

[ns_server:debug,2020-04-02T21:11:09.899+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T21:11:09.899+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:09.899+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2292806322.3932160003.222224>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:11:09.899+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2292806322.3932160003.222224>,
                                  inet_tcp_dist,<0.287.0>,
                                  #Ref<0.2292806322.3932160003.222225>}
[ns_server:debug,2020-04-02T21:11:09.900+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-04-02T21:11:09.900+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2292806322.3932160003.222224>,
                               inet_tcp_dist,<0.287.0>,
                               #Ref<0.2292806322.3932160003.222225>}
[error_logger:info,2020-04-02T21:11:09.900+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.287.0>,shutdown}}
[error_logger:info,2020-04-02T21:11:09.900+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-04-02T21:11:10.100+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:10.100+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T21:11:10.100+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2292806322.3932160003.222230>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:11:10.100+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2292806322.3932160003.222230>,
                                  inet_tcp_dist,<0.290.0>,
                                  #Ref<0.2292806322.3932160002.222440>}
[ns_server:debug,2020-04-02T21:11:10.101+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2292806322.3932160003.222230>,
                               inet_tcp_dist,<0.290.0>,
                               #Ref<0.2292806322.3932160002.222440>}
[error_logger:info,2020-04-02T21:11:10.101+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.290.0>,shutdown}}
[ns_server:debug,2020-04-02T21:11:10.101+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-04-02T21:11:10.101+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:10.301+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T21:11:10.301+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:10.301+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2292806322.3932160003.222235>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:11:10.302+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2292806322.3932160003.222235>,
                                  inet_tcp_dist,<0.293.0>,
                                  #Ref<0.2292806322.3932160003.222237>}
[ns_server:debug,2020-04-02T21:11:10.303+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-04-02T21:11:10.303+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2292806322.3932160003.222235>,
                               inet_tcp_dist,<0.293.0>,
                               #Ref<0.2292806322.3932160003.222237>}
[error_logger:info,2020-04-02T21:11:10.303+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.293.0>,shutdown}}
[error_logger:info,2020-04-02T21:11:10.303+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:info,2020-04-02T21:11:10.357+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.274.0>:ns_port_server:log:224]ns_couchdb<0.274.0>: {"Kernel pid terminated",application_controller,"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\"},{line,102}]},{supervisor,do_start_child,2,[{file,\"supervisor.erl\"},{line,365}]},{supervisor,start_children,3,[{file,\"supervisor.erl\"},{line,348}]},{supervisor,init_children,2,[{file,\"supervisor.erl\"},{line,314}]},{gen_server,init_it,2,[{file,\"gen_server.erl\"},{line,365}]},{gen_server,init_it,6,[{file,\"gen_server.erl\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}"}
ns_couchdb<0.274.0>: Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro
ns_couchdb<0.274.0>: 
ns_couchdb<0.274.0>: Crash dump is being written to: erl_crash.dump.1585842038.12600.ns_couchdb...done

[error_logger:error,2020-04-02T21:11:10.357+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]** Generic server ns_couchdb_port terminating 
** Last message in was {#Port<0.5097>,{exit_status,1}}
** When Server state == {state,#Port<0.5097>,
                            {ns_couchdb,"/opt/couchbase/lib/erlang/bin/erl",
                                ["-pa",
                                 "/opt/couchbase/lib/erlang/lib/asn1-5.0.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/compiler-7.1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEvent-2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEventDomain-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosFileTransfer-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosNotification-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosProperty-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTime-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTransactions-1.3.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/crypto-4.2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/dialyzer-3.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/diameter-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/edoc-0.9.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eldap-1.2.3.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_docgen-0.7.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_interface-3.10.2.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erts-9.3.3.9/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eunit-2.3.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/hipe-3.17.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ic-4.4.4.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/inets-6.5.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/mnesia-4.15.3.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/orber-3.8.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/os_mon-2.4.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/otp_mibs-1.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/parsetools-2.1.6/ebin",
                                 "/opt/couchbase/lib/erlang/lib/public_key-1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/reltool-0.7.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/runtime_tools-1.12.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/sasl-3.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/snmp-5.2.11/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssh-4.6.9.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssl-8.2.6.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/syntax_tools-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/tools-2.11.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/xmerl-1.3.16.1/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/gc-couchbase-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/vtree-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/wkb-1.2.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch-1.2.0a-961ad59-git/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_audit-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_dcp-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_index_merger-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_set_view-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_view_parser-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/ejson-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/erlang-oauth/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/etap/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/lhttpc-1.3/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mapreduce-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mochiweb-1.4.1/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/snappy-1.0.4/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ale/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/gen_smtp/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_babysitter/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_couchdb/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/ebin",
                                 "/opt/couchbase/lib/erlang/lib/stdlib-3.4.5.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/kernel-5.4.3.2/ebin",
                                 ".","-couch_ini",
                                 "/opt/couchbase/etc/couchdb/default.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/capi.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/geocouch.ini",
                                 "/opt/couchbase/etc/couchdb/local.ini",
                                 "-kernel","error_logger","false","-kernel",
                                 "error_logger","false","inetrc",
                                 "\"/opt/couchbase/etc/couchbase/hosts.cfg\"",
                                 "dist_config_file",
                                 "\"/opt/couchbase/var/lib/couchbase/config/dist_cfg\"",
                                 "-ssl_dist_optfile",
                                 "/opt/couchbase/etc/couchbase/ssl_dist_opts",
                                 "-setcookie",
                                 "2a158eb185476066b502c8ced82b23b042bd1ae5f1ef74f024e7bb9916796a9b",
                                 "-name","couchdb_ns_1@cb.local","-smp",
                                 "enable","+P","327680","+K","true","-kernel",
                                 "error_logger","false","-sasl",
                                 "sasl_error_logger","false","-nouser",
                                 "-hidden","-proto_dist","cb","-epmd_module",
                                 "cb_epmd","-start_epmd","false","-run",
                                 "child_erlang","child_start","ns_couchdb"],
                                [use_stdio,
                                 {env,
                                     [{"NS_COUCHDB_ENV_ARGS",
                                       "[{ns_server_node,'ns_1@127.0.0.1'},\n {path_config_tmpdir,\"/opt/couchbase/var/lib/couchbase/tmp\"},\n {net_kernel_verbosity,10},\n {loglevel_error_logger,debug},\n {path_config_libdir,\"/opt/couchbase/lib\"},\n {loglevel_stats,debug},\n {loglevel_menelaus,debug},\n {path_config_secdir,\"/opt/couchbase/etc/security\"},\n {loglevel_user,debug},\n {path_config_etcdir,\"/opt/couchbase/etc/couchbase\"},\n {loglevel_ns_server,debug},\n {loglevel_mapreduce_errors,debug},\n {loglevel_rebalance,debug},\n {loglevel_default,debug},\n {disk_sink_opts,[{rotation,[{compress,true},\n                             {size,41943040},\n                             {num_files,10},\n                             {buffer_size_max,52428800}]}]},\n {loglevel_cbas,debug},\n {loglevel_xdcr,debug},\n {loglevel_ns_doctor,debug},\n {loglevel_access,info},\n {error_logger_mf_dir,\"/opt/couchbase/var/lib/couchbase/logs\"},\n {path_config_datadir,\"/opt/couchbase/var/lib/couchbase\"},\n {loglevel_cluster,debug},\n {loglevel_couchdb,info},\n {loglevel_views,debug},\n {path_config_bindir,\"/opt/couchbase/bin\"}]"},
                                      {"ERL_CRASH_DUMP",
                                       "erl_crash.dump.1585842038.12600.ns_couchdb"}]}]},
                            {ringbuffer,1191,1024,
                                {[{<<"Crash dump is being written to: erl_crash.dump.1585842038.12600.ns_couchdb...done">>,
                                   81},
                                  {<<>>,0},
                                  {<<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                                   200}],
                                 [{<<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>,
                                   910}]}},
                            undefined,
                            {ok,{-576460749432,
                                 #Ref<0.2292806322.3932160004.221940>}},
                            [<<"Crash dump is being written to: erl_crash.dump.1585842038.12600.ns_couchdb...done">>,
                             <<>>,
                             <<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                             <<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>],
                            0}
** Reason for termination == 
** {abnormal,1}

[ns_server:error,2020-04-02T21:11:10.359+05:30,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.275.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:189]ns_couchdb_port(<0.274.0>) died with reason {abnormal,1}
[ns_server:debug,2020-04-02T21:11:10.359+05:30,ns_1@127.0.0.1:<0.268.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.267.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:11:10.359+05:30,ns_1@127.0.0.1:<0.269.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.267.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:11:10.359+05:30,ns_1@127.0.0.1:<0.265.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.264.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:11:10.359+05:30,ns_1@127.0.0.1:<0.266.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.264.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:11:10.359+05:30,ns_1@127.0.0.1:<0.257.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.256.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:11:10.359+05:30,ns_1@127.0.0.1:<0.218.0>:restartable:shutdown_child:120]Successfully terminated process <0.219.0>
[ns_server:debug,2020-04-02T21:11:10.360+05:30,ns_1@127.0.0.1:<0.217.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.216.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:11:10.360+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:wait_saver:866]Done waiting for saver.
[ns_server:debug,2020-04-02T21:11:10.360+05:30,ns_1@127.0.0.1:<0.203.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.202.0>} exited with reason shutdown
[error_logger:error,2020-04-02T21:11:10.361+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_port_server:init/1
    pid: <0.274.0>
    registered_name: ns_couchdb_port
    exception exit: {abnormal,1}
      in function  gen_server:handle_common_reply/8 (gen_server.erl, line 726)
    ancestors: [ns_server_nodes_sup,<0.208.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 1
    messages: [{'EXIT',#Port<0.5097>,normal}]
    links: [<0.209.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 11925
  neighbours:

[error_logger:error,2020-04-02T21:11:10.362+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: erlang:apply/2
    pid: <0.275.0>
    registered_name: wait_link_to_couchdb_node
    exception exit: {abnormal,1}
      in function  ns_server_nodes_sup:do_wait_link_to_couchdb_node/1 (src/ns_server_nodes_sup.erl, line 190)
    ancestors: [ns_server_nodes_sup,<0.208.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.209.0>,<0.276.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 3352
  neighbours:
    neighbour:
      pid: <0.276.0>
      registered_name: []
      initial call: ns_server_nodes_sup:'-do_wait_link_to_couchdb_node/1-fun-2-'/0
      current_function: {timer,sleep,1}
      ancestors: [wait_link_to_couchdb_node,ns_server_nodes_sup,<0.208.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
      message_queue_len: 0
      links: [<0.275.0>]
      trap_exit: false
      status: waiting
      heap_size: 2586
      stack_size: 12
      reductions: 10855
      current_stacktrace: [{timer,sleep,1,[{file,"timer.erl"},{line,153}]},
                  {misc,poll_for_condition_rec,3,
                      [{file,"src/misc.erl"},{line,508}]},
                  {ns_server_nodes_sup,
                      '-do_wait_link_to_couchdb_node/1-fun-2-',2,
                      [{file,"src/ns_server_nodes_sup.erl"},{line,159}]},
                  {proc_lib,init_p,3,[{file,"proc_lib.erl"},{line,232}]}]

[error_logger:error,2020-04-02T21:11:10.362+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    start_error
     Reason:     {abnormal,1}
     Offender:   [{pid,undefined},
                  {name,wait_for_couchdb_node},
                  {mfargs,{erlang,apply,
                                  [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:error,2020-04-02T21:11:10.362+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    shutdown_error
     Reason:     {abnormal,1}
     Offender:   [{pid,<0.274.0>},
                  {name,start_couchdb_node},
                  {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                  {restart_type,{permanent,5}},
                  {shutdown,86400000},
                  {child_type,worker}]


[error_logger:error,2020-04-02T21:11:10.363+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_cluster_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,wait_for_couchdb_node,
                         {abnormal,1}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_nodes_sup},
                  {mfargs,
                      {restartable,start_link,
                          [{ns_server_nodes_sup,start_link,[]},infinity]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-04-02T21:11:10.363+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,root_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                             {failed_to_start_child,wait_for_couchdb_node,
                                 {abnormal,1}}}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_cluster_sup},
                  {mfargs,{ns_server_cluster_sup,start_link,[]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-04-02T21:11:10.363+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: application_master:init/4
    pid: <0.117.0>
    registered_name: []
    exception exit: {{shutdown,
                      {failed_to_start_child,ns_server_cluster_sup,
                       {shutdown,
                        {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                          {failed_to_start_child,wait_for_couchdb_node,
                           {abnormal,1}}}}}}},
                     {ns_server,start,[normal,[]]}}
      in function  application_master:init/4 (application_master.erl, line 134)
    ancestors: [<0.116.0>]
    message_queue_len: 1
    messages: [{'EXIT',<0.118.0>,normal}]
    links: [<0.116.0>,<0.33.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 274
  neighbours:

[error_logger:info,2020-04-02T21:11:10.363+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
         application: ns_server
              exited: {{shutdown,
                        {failed_to_start_child,ns_server_cluster_sup,
                         {shutdown,
                          {failed_to_start_child,ns_server_nodes_sup,
                           {shutdown,
                            {failed_to_start_child,wait_for_couchdb_node,
                             {abnormal,1}}}}}}},
                       {ns_server,start,[normal,[]]}}
                type: permanent

[error_logger:info,2020-04-02T21:11:10.364+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,system_memory_high_watermark}

[error_logger:info,2020-04-02T21:11:10.364+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-calculator/544"}}

[error_logger:info,2020-04-02T21:11:10.364+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/8935"}}

[error_logger:info,2020-04-02T21:11:10.364+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-calculator/704"}}

[error_logger:info,2020-04-02T21:11:10.364+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/59"}}

[error_logger:info,2020-04-02T21:11:10.364+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/pycharm-community/188"}}

[error_logger:info,2020-04-02T21:11:10.364+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gnome-system-monitor/127"}}

[error_logger:info,2020-04-02T21:11:10.364+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core18/1705"}}

[error_logger:info,2020-04-02T21:11:10.364+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/8689"}}

[error_logger:info,2020-04-02T21:11:10.364+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gnome-system-monitor/135"}}

[error_logger:info,2020-04-02T21:11:10.364+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-characters/495"}}

[error_logger:info,2020-04-02T21:11:10.364+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-28-1804/116"}}

[error_logger:info,2020-04-02T21:11:10.365+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gtk-common-themes/1474"}}

[error_logger:info,2020-04-02T21:11:10.365+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core18/1668"}}

[error_logger:info,2020-04-02T21:11:10.365+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-logs/81"}}

[error_logger:info,2020-04-02T21:11:10.365+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-logs/93"}}

[error_logger:info,2020-04-02T21:11:10.365+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/98"}}

[error_logger:info,2020-04-02T21:11:10.365+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-characters/399"}}

[error_logger:info,2020-04-02T21:11:10.365+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gtk-common-themes/1440"}}

[ns_server:info,2020-04-02T21:11:17.790+05:30,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-04-02T21:11:17.801+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-04-02T21:11:17.801+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.801+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.801+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.801+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.801+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-04-02T21:11:17.802+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-04-02T21:11:17.806+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-04-02T21:11:17.806+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[error_logger:info,2020-04-02T21:11:17.807+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.168.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2020-04-02T21:11:17.807+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.167.0>},
                       {id,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:11:17.809+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@127.0.0.1'
[error_logger:info,2020-04-02T21:11:17.817+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.817+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.817+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:17.819+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.174.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.820+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.176.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:17.820+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.177.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:17.820+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.175.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T21:11:17.820+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2020-04-02T21:11:17.820+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:17.821+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.822+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.822+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.180.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:11:17.823+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-04-02T21:11:17.824+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-04-02T21:11:17.824+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2020-04-02T21:11:17.825+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-04-02T21:11:17.825+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.181.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.825+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.169.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-04-02T21:11:17.826+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-04-02T21:11:17.831+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-04-02T21:11:17.831+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2020-04-02T21:11:17.831+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:17.831+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T21:11:17.831+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.857367481.2053373954.51082>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:11:17.831+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.857367481.2053373954.51082>,
                                  inet_tcp_dist,<0.185.0>,
                                  #Ref<0.857367481.2053373954.51083>}
[ns_server:debug,2020-04-02T21:11:17.833+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-04-02T21:11:17.833+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2020-04-02T21:11:17.833+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-04-02T21:11:17.835+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-04-02T21:11:17.835+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.839+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:11:17.840+05:30,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,4,2},{21,11,17}}},
               {memory,
                   [{total,26615344},
                    {processes,9777880},
                    {processes_used,9772632},
                    {system,16837464},
                    {atom,388625},
                    {atom_used,364409},
                    {binary,155392},
                    {code,8250921},
                    {ets,1509096}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    error_handler,application,code,application_master,
                    application_controller,file,heart,kernel,file_server,
                    error_logger,code_server,filename,gen_server,
                    file_io_server,gen_event,lists,ets,gen,supervisor,
                    erl_parse,proc_lib,erl_lint,erl_eval,
                    erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{os_mon,"CPO  CXC 138 46","2.4.4"},
                    {sasl,"SASL  CXC 138 11","3.1.2"},
                    {ns_server,"Couchbase server","6.5.0-4960-enterprise"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ale,"Another Logger for Erlang","0.0.0"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,131},
               {node,'ns_1@127.0.0.1'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,auth,httpd_sup,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    release_handler,lhttpc_sup,httpc_sup,lhttpc_manager,
                    alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    'sink-disk_xdcr',timer_server,'sink-disk_debug',
                    inet_gethost_native,ale_sup,'sink-disk_error',inet_db,
                    'sink-disk_default',ssl_pem_cache_dist,ale_dynamic_sup,
                    rex,kernel_safe_sup,global_group,net_sup,kernel_sup,
                    ssl_connection_sup,global_name_server,ssl_admin_sup,
                    tftp_sup,ssl_sup,root_sup,erts_code_purger,os_mon_sup,
                    file_server_2,error_logger,cpu_sup,erl_epmd,init,memsup,
                    erl_signal_server,disksup,ale,net_kernel,dist_manager,
                    ssl_pem_cache,ssl_manager,ssl_dist_admin_sup,
                    ssl_dist_connection_sup,ssl_dist_sup,user,
                    ssl_tls_dist_proxy,ssl_manager_dist,sasl_safe_sup,
                    ssl_listen_tracker_sup,inet_gethost_native_sup,
                    code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,0}]
[ns_server:info,2020-04-02T21:11:17.845+05:30,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"cfa0f75f28402d2e1aa254b2a374bead19433526\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" upstream=\"master\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"f2a16b53bb74146f20d18ba2c0443d5f10a9a550\" upstream=\"master\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4960\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" upstream=\"master\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" upstream=\"mad-hatter\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" upstream=\"master\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" upstream=\"master\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" upstream=\"master\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" upstream=\"mad-hatter\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" upstream=\"cb-master\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" upstream=\"master\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" upstream=\"master\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" upstream=\"mad-hatter\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" upstream=\"mad-hatter\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" upstream=\"mad-hatter\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" upstream=\"master\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" upstream=\"master\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" upstream=\"mad-hatter\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" upstream=\"mad-hatter\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" upstream=\"mad-hatter\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" upstream=\"mad-hatter\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" upstream=\"master\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" upstream=\"master\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" upstream=\"master\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" upstream=\"mad-hatter\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" upstream=\"master\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"7a3aa547b3f5eb3ea28d279a08384609cd2cea7c\" upstream=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" upstream=\"mad-hatter\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" upstream=\"master\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" upstream=\"master\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" upstream=\"master\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" upstream=\"v1.1.0-couchbase\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"ee64d41320d14fabe814a241a5cf4f6a6f6e827a\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" upstream=\"master\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" upstream=\"master\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-04-02T21:11:17.848+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.849+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.849+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:11:17.851+05:30,ns_1@127.0.0.1:ns_config_sup<0.192.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-04-02T21:11:17.851+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.851+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.194.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:11:17.866+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-04-02T21:11:17.867+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2020-04-02T21:11:17.874+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[{alert_limits,
   [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
  {audit,
   [{auditd_enabled,false},
    {rotate_interval,86400},
    {rotate_size,20971520},
    {disabled,[]},
    {sync,[]},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
  {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
  {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
  {autocompaction,
   [{database_fragmentation_threshold,{30,undefined}},
    {view_fragmentation_threshold,{30,undefined}}]},
  {buckets,[{configs,[]}]},
  {cbas_memory_quota,2174},
  {cert_and_pkey,
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFgIK71cHor8wDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBkZTZmMzM0MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgZGU2ZjMz\nNDAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC6Epk+5C0GfEqGHL9d\nxsySLywt3gLcVQmCM8lgcMRGWDaGVF6iOP+QyLODyB09I5u2gOcVm+1r3eOZ4rwk\nbttVmFIsdroNf2jG+9baY4LqKoDyZnjZr0LeolUcY+0eYI68oNwRMgWp53Krm861\ny11yyOEjefm+JBDhZuZpHZegjTBKtYqZd96WZwOzbrJZrau3uKBuQTmoEdpZ4VdX\n6U5nzUaRkvjjuBpQyeqMLSuuLUO4FENp1C8P9fYhy4Y6RRZfMSBGdyw1d8QEWxiU\n4n/rtfQgiN32qOwtY7ocwvaXDV7wH1ipWkPF5Vn8eyBi5cA2xqgaq1xSBLD8MUHE\nXTAjAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCP9ajveEq01YMq\n/zClEAjE3TCbGqz9u/vjXdhSQK7rPJLcK250d86L6njzkS2ffrabbOGON+4UvNW4\nTUub3JqnTuSlI8B6riH61kqWPfCfRC392v1xAIaQI1/jWsW4HQoiXbmi0uiKrsEq\nIt8XF5nLXDsEeWYetynrODdVU9ADeDNkE2+AOyLTvD/4eUDRoQhDhC5vh75Bu9gm\nEV+efNKCwXjs4xAMPGbKoNnWBkx7Btn0+iyI19l+jrzF1rlDaH6pFz2ldqm6CL+f\n26ZCU9S8uXPNC7UiNXr6DZj1sn/k0qqebDRnHlO2P+wYp5G/+Rca+B41diWCV7xG\ncnfTf1PH\n-----END CERTIFICATE-----\n">>,
     <<"*****">>}]},
  {drop_request_memory_threshold_mib,undefined},
  {email_alerts,
   [{recipients,["root@localhost"]},
    {sender,"couchbase@localhost"},
    {enabled,false},
    {email_server,
     [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
    {alerts,
     [auto_failover_node,auto_failover_maximum_reached,
      auto_failover_other_nodes_down,auto_failover_cluster_too_small,
      auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
      ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
      ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
  {fts_memory_quota,512},
  {index_aware_rebalance_disabled,false},
  {log_redaction_default_cfg,[{redact_level,none}]},
  {max_bucket_count,30},
  {memcached,[]},
  {memory_quota,8886},
  {nodes_wanted,['ns_1@127.0.0.1']},
  {password_policy,[{min_length,6},{must_present,[]}]},
  {quorum_nodes,['ns_1@127.0.0.1']},
  {remote_clusters,[]},
  {replication,[{enabled,true}]},
  {rest,[{port,8091}]},
  {rest_creds,null},
  {secure_headers,[]},
  {server_groups,
   [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
  {set_view_update_daemon,
   [{update_interval,5000},
    {update_min_changes,5000},
    {replica_update_min_changes,5000}]},
  {{couchdb,max_parallel_indexers},4},
  {{couchdb,max_parallel_replica_indexers},2},
  {{local_changes_count,<<"8c43a5102cad1e34db659ab4d5646878">>},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{5,63753061268}}]}]},
  {{metakv,<<"/indexing/settings/config">>},
   <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
  {{request_limit,capi},undefined},
  {{request_limit,rest},undefined},
  {{node,'ns_1@127.0.0.1',address_family},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    inet]},
  {{node,'ns_1@127.0.0.1',audit},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
  {{node,'ns_1@127.0.0.1',capi_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8092]},
  {{node,'ns_1@127.0.0.1',cbas_admin_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9110]},
  {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9113]},
  {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9112]},
  {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9111]},
  {{node,'ns_1@127.0.0.1',cbas_cluster_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9115]},
  {{node,'ns_1@127.0.0.1',cbas_console_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9114]},
  {{node,'ns_1@127.0.0.1',cbas_data_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9116]},
  {{node,'ns_1@127.0.0.1',cbas_debug_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    -1]},
  {{node,'ns_1@127.0.0.1',cbas_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8095]},
  {{node,'ns_1@127.0.0.1',cbas_messaging_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9118]},
  {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9119]},
  {{node,'ns_1@127.0.0.1',cbas_metadata_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9121]},
  {{node,'ns_1@127.0.0.1',cbas_parent_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9122]},
  {{node,'ns_1@127.0.0.1',cbas_replication_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9120]},
  {{node,'ns_1@127.0.0.1',cbas_result_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9117]},
  {{node,'ns_1@127.0.0.1',cbas_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18095]},
  {{node,'ns_1@127.0.0.1',compaction_daemon},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@127.0.0.1',config_version},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    {6,5}]},
  {{node,'ns_1@127.0.0.1',erl_external_listeners},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {inet,false},
    {inet6,false}]},
  {{node,'ns_1@127.0.0.1',eventing_debug_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9140]},
  {{node,'ns_1@127.0.0.1',eventing_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8096]},
  {{node,'ns_1@127.0.0.1',eventing_https_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18096]},
  {{node,'ns_1@127.0.0.1',fts_grpc_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9130]},
  {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    19130]},
  {{node,'ns_1@127.0.0.1',fts_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8094]},
  {{node,'ns_1@127.0.0.1',fts_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18094]},
  {{node,'ns_1@127.0.0.1',indexer_admin_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9100]},
  {{node,'ns_1@127.0.0.1',indexer_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9102]},
  {{node,'ns_1@127.0.0.1',indexer_https_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    19102]},
  {{node,'ns_1@127.0.0.1',indexer_scan_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9101]},
  {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9104]},
  {{node,'ns_1@127.0.0.1',indexer_stinit_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9103]},
  {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9105]},
  {{node,'ns_1@127.0.0.1',is_enterprise},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    true]},
  {{node,'ns_1@127.0.0.1',isasl},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@127.0.0.1',membership},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    active]},
  {{node,'ns_1@127.0.0.1',memcached},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {port,11210},
    {dedicated_port,11209},
    {dedicated_ssl_port,11206},
    {ssl_port,11207},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_sleeptime,19},
    {log_rotation_period,39003}]},
  {{node,'ns_1@127.0.0.1',memcached_config},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    {[{interfaces,
       {memcached_config_mgr,omit_missing_mcd_ports,
        [{[{host,<<"*">>},
           {port,port},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_port},
           {system,true},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,ssl_port},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_ssl_port},
           {system,true},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
      {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
      {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {opentracing,
       {[{enabled,opentracing_enabled},
         {module,{"~s",[opentracing_module]}},
         {config,{"~s",[opentracing_config]}}]}},
      {admin,{"~s",[admin_user]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize},
         {sleeptime,log_sleeptime}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
  {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    11206]},
  {{node,'ns_1@127.0.0.1',memcached_defaults},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {max_connections,65000},
    {system_connections,5000},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {opentracing_enabled,false},
    {opentracing_module,[]},
    {opentracing_config,[]},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {tracing_enabled,true},
    {datatype_snappy,true},
    {num_reader_threads,<<"default">>},
    {num_writer_threads,<<"default">>}]},
  {{node,'ns_1@127.0.0.1',moxi},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {port,0}]},
  {{node,'ns_1@127.0.0.1',node_encryption},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    false]},
  {{node,'ns_1@127.0.0.1',ns_log},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@127.0.0.1',port_servers},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
  {{node,'ns_1@127.0.0.1',projector_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9999]},
  {{node,'ns_1@127.0.0.1',projector_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9999]},
  {{node,'ns_1@127.0.0.1',query_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8093]},
  {{node,'ns_1@127.0.0.1',rest},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@127.0.0.1',saslauthd_enabled},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    true]},
  {{node,'ns_1@127.0.0.1',ssl_capi_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18092]},
  {{node,'ns_1@127.0.0.1',ssl_query_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18093]},
  {{node,'ns_1@127.0.0.1',ssl_rest_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18091]},
  {{node,'ns_1@127.0.0.1',uuid},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    <<"8c43a5102cad1e34db659ab4d5646878">>]},
  {{node,'ns_1@127.0.0.1',xdcr_rest_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9998]},
  {{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    false]}]]
[ns_server:info,2020-04-02T21:11:17.877+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',xdcr_rest_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9998]},
 {{node,'ns_1@127.0.0.1',uuid},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   <<"8c43a5102cad1e34db659ab4d5646878">>]},
 {{node,'ns_1@127.0.0.1',ssl_rest_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18091]},
 {{node,'ns_1@127.0.0.1',ssl_query_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18093]},
 {{node,'ns_1@127.0.0.1',ssl_capi_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18092]},
 {{node,'ns_1@127.0.0.1',saslauthd_enabled},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',rest},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@127.0.0.1',query_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8093]},
 {{node,'ns_1@127.0.0.1',projector_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',projector_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',port_servers},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
 {{node,'ns_1@127.0.0.1',ns_log},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@127.0.0.1',node_encryption},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',moxi},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {port,0}]},
 {{node,'ns_1@127.0.0.1',memcached_defaults},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   11206]},
 {{node,'ns_1@127.0.0.1',memcached_config},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@127.0.0.1',memcached},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@127.0.0.1',membership},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   active]},
 {{node,'ns_1@127.0.0.1',isasl},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@127.0.0.1',is_enterprise},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9105]},
 {{node,'ns_1@127.0.0.1',indexer_stinit_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9103]},
 {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9104]},
 {{node,'ns_1@127.0.0.1',indexer_scan_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9101]},
 {{node,'ns_1@127.0.0.1',indexer_https_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   19102]},
 {{node,'ns_1@127.0.0.1',indexer_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9102]},
 {{node,'ns_1@127.0.0.1',indexer_admin_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9100]},
 {{node,'ns_1@127.0.0.1',fts_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18094]},
 {{node,'ns_1@127.0.0.1',fts_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8094]},
 {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   19130]},
 {{node,'ns_1@127.0.0.1',fts_grpc_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9130]},
 {{node,'ns_1@127.0.0.1',eventing_https_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18096]},
 {{node,'ns_1@127.0.0.1',eventing_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8096]},
 {{node,'ns_1@127.0.0.1',eventing_debug_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9140]},
 {{node,'ns_1@127.0.0.1',erl_external_listeners},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {inet,false},
   {inet6,false}]},
 {{node,'ns_1@127.0.0.1',config_version},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   {6,5}]},
 {{node,'ns_1@127.0.0.1',compaction_daemon},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@127.0.0.1',cbas_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18095]},
 {{node,'ns_1@127.0.0.1',cbas_result_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9117]},
 {{node,'ns_1@127.0.0.1',cbas_replication_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9120]},
 {{node,'ns_1@127.0.0.1',cbas_parent_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9122]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9121]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9119]},
 {{node,'ns_1@127.0.0.1',cbas_messaging_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9118]},
 {{node,'ns_1@127.0.0.1',cbas_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8095]},
 {{node,'ns_1@127.0.0.1',cbas_debug_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|-1]},
 {{node,'ns_1@127.0.0.1',cbas_data_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9116]},
 {{node,'ns_1@127.0.0.1',cbas_console_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9114]},
 {{node,'ns_1@127.0.0.1',cbas_cluster_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9115]},
 {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9111]},
 {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9112]},
 {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9113]},
 {{node,'ns_1@127.0.0.1',cbas_admin_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9110]},
 {{node,'ns_1@127.0.0.1',capi_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8092]},
 {{node,'ns_1@127.0.0.1',audit},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
 {{node,'ns_1@127.0.0.1',address_family},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   inet]},
 {{request_limit,rest},undefined},
 {{request_limit,capi},undefined},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{local_changes_count,<<"8c43a5102cad1e34db659ab4d5646878">>},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{5,63753061268}}]}]},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
 {secure_headers,[]},
 {rest_creds,null},
 {rest,[{port,8091}]},
 {replication,[{enabled,true}]},
 {remote_clusters,[]},
 {quorum_nodes,['ns_1@127.0.0.1']},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {nodes_wanted,['ns_1@127.0.0.1']},
 {memory_quota,8886},
 {memcached,[]},
 {max_bucket_count,30},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {index_aware_rebalance_disabled,false},
 {fts_memory_quota,512},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {drop_request_memory_threshold_mib,undefined},
 {cert_and_pkey,
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFgIK71cHor8wDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBkZTZmMzM0MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgZGU2ZjMz\nNDAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC6Epk+5C0GfEqGHL9d\nxsySLywt3gLcVQmCM8lgcMRGWDaGVF6iOP+QyLODyB09I5u2gOcVm+1r3eOZ4rwk\nbttVmFIsdroNf2jG+9baY4LqKoDyZnjZr0LeolUcY+0eYI68oNwRMgWp53Krm861\ny11yyOEjefm+JBDhZuZpHZegjTBKtYqZd96WZwOzbrJZrau3uKBuQTmoEdpZ4VdX\n6U5nzUaRkvjjuBpQyeqMLSuuLUO4FENp1C8P9fYhy4Y6RRZfMSBGdyw1d8QEWxiU\n4n/rtfQgiN32qOwtY7ocwvaXDV7wH1ipWkPF5Vn8eyBi5cA2xqgaq1xSBLD8MUHE\nXTAjAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCP9ajveEq01YMq\n/zClEAjE3TCbGqz9u/vjXdhSQK7rPJLcK250d86L6njzkS2ffrabbOGON+4UvNW4\nTUub3JqnTuSlI8B6riH61kqWPfCfRC392v1xAIaQI1/jWsW4HQoiXbmi0uiKrsEq\nIt8XF5nLXDsEeWYetynrODdVU9ADeDNkE2+AOyLTvD/4eUDRoQhDhC5vh75Bu9gm\nEV+efNKCwXjs4xAMPGbKoNnWBkx7Btn0+iyI19l+jrzF1rlDaH6pFz2ldqm6CL+f\n26ZCU9S8uXPNC7UiNXr6DZj1sn/k0qqebDRnHlO2P+wYp5G/+Rca+B41diWCV7xG\ncnfTf1PH\n-----END CERTIFICATE-----\n">>,
    <<"*****">>}]},
 {cbas_memory_quota,2174},
 {buckets,[{configs,[]}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]}]
[error_logger:info,2020-04-02T21:11:17.879+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.195.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.880+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.201.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.881+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.202.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.881+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T21:11:17.882+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"8c43a5102cad1e34db659ab4d5646878">>} ->
[{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{6,63753061277}}]}]
[error_logger:info,2020-04-02T21:11:17.882+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.204.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:17.883+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.207.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:17.892+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:11:17.893+05:30,ns_1@127.0.0.1:menelaus_barrier<0.211.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-04-02T21:11:17.893+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.893+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.896+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.213.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.897+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.215.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:11:17.909+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2020-04-02T21:11:17.909+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2020-04-02T21:11:17.909+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2020-04-02T21:11:17.924+05:30,ns_1@127.0.0.1:ns_ssl_services_setup<0.216.0>:ns_ssl_services_setup:init:462]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,2,10,239,87,7,162,
             191,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
             6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
             118,101,114,32,100,101,54,102,51,51,52,48,48,30,23,13,49,51,48,
             49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
             57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
             104,98,97,115,101,32,83,101,114,118,101,114,32,100,101,54,102,
             51,51,52,48,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,
             0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,186,18,153,62,228,45,6,
             124,74,134,28,191,93,198,204,146,47,44,45,222,2,220,85,9,130,51,
             201,96,112,196,70,88,54,134,84,94,162,56,255,144,200,179,131,
             200,29,61,35,155,182,128,231,21,155,237,107,221,227,153,226,188,
             36,110,219,85,152,82,44,118,186,13,127,104,198,251,214,218,99,
             130,234,42,128,242,102,120,217,175,66,222,162,85,28,99,237,30,
             96,142,188,160,220,17,50,5,169,231,114,171,155,206,181,203,93,
             114,200,225,35,121,249,190,36,16,225,102,230,105,29,151,160,141,
             48,74,181,138,153,119,222,150,103,3,179,110,178,89,173,171,183,
             184,160,110,65,57,168,17,218,89,225,87,87,233,78,103,205,70,145,
             146,248,227,184,26,80,201,234,140,45,43,174,45,67,184,20,67,105,
             212,47,15,245,246,33,203,134,58,69,22,95,49,32,70,119,44,53,119,
             196,4,91,24,148,226,127,235,181,244,32,136,221,246,168,236,45,
             99,186,28,194,246,151,13,94,240,31,88,169,90,67,197,229,89,252,
             123,32,98,229,192,54,198,168,26,171,92,82,4,176,252,49,65,196,
             93,48,35,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,255,4,4,
             3,2,2,164,48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,
             15,6,3,85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,134,72,
             134,247,13,1,1,11,5,0,3,130,1,1,0,143,245,168,239,120,74,180,
             213,131,42,255,48,165,16,8,196,221,48,155,26,172,253,187,251,
             227,93,216,82,64,174,235,60,146,220,43,110,116,119,206,139,234,
             120,243,145,45,159,126,182,155,108,225,142,55,238,20,188,213,
             184,77,75,155,220,154,167,78,228,165,35,192,122,174,33,250,214,
             74,150,61,240,159,68,45,253,218,253,113,0,134,144,35,95,227,90,
             197,184,29,10,34,93,185,162,210,232,138,174,193,42,34,223,23,23,
             153,203,92,59,4,121,102,30,183,41,235,56,55,85,83,208,3,120,51,
             100,19,111,128,59,34,211,188,63,248,121,64,209,161,8,67,132,46,
             111,135,190,65,187,216,38,17,95,158,124,210,130,193,120,236,227,
             16,12,60,102,202,160,217,214,6,76,123,6,217,244,250,44,136,215,
             217,126,142,188,197,214,185,67,104,126,169,23,61,165,118,169,
             186,8,191,159,219,166,66,83,212,188,185,115,205,11,181,34,53,
             122,250,13,152,245,178,127,228,210,170,158,108,52,103,30,83,182,
             63,236,24,167,145,191,249,23,26,248,30,53,118,37,130,87,188,70,
             114,119,211,127,83,199>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2020-04-02T21:11:17.925+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.216.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:11:17.932+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-04-02T21:11:17.932+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-04-02T21:11:17.932+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-04-02T21:11:17.932+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:info,2020-04-02T21:11:17.942+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-04-02T21:11:17.942+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-04-02T21:11:17.942+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-04-02T21:11:17.942+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-04-02T21:11:17.941+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.219.0>,menelaus_web}
             started: [{pid,<0.220.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 2,10,239,87,7,162,191,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,100,101,54,
                                 102,51,51,52,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,100,
                                 101,54,102,51,51,52,48,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,186,18,153,62,228,
                                 45,6,124,74,134,28,191,93,198,204,146,47,44,
                                 45,222,2,220,85,9,130,51,201,96,112,196,70,
                                 88,54,134,84,94,162,56,255,144,200,179,131,
                                 200,29,61,35,155,182,128,231,21,155,237,107,
                                 221,227,153,226,188,36,110,219,85,152,82,44,
                                 118,186,13,127,104,198,251,214,218,99,130,
                                 234,42,128,242,102,120,217,175,66,222,162,
                                 85,28,99,237,30,96,142,188,160,220,17,50,5,
                                 169,231,114,171,155,206,181,203,93,114,200,
                                 225,35,121,249,190,36,16,225,102,230,105,29,
                                 151,160,141,48,74,181,138,153,119,222,150,
                                 103,3,179,110,178,89,173,171,183,184,160,
                                 110,65,57,168,17,218,89,225,87,87,233,78,
                                 103,205,70,145,146,248,227,184,26,80,201,
                                 234,140,45,43,174,45,67,184,20,67,105,212,
                                 47,15,245,246,33,203,134,58,69,22,95,49,32,
                                 70,119,44,53,119,196,4,91,24,148,226,127,
                                 235,181,244,32,136,221,246,168,236,45,99,
                                 186,28,194,246,151,13,94,240,31,88,169,90,
                                 67,197,229,89,252,123,32,98,229,192,54,198,
                                 168,26,171,92,82,4,176,252,49,65,196,93,48,
                                 35,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,
                                 15,1,1,255,4,4,3,2,2,164,48,19,6,3,85,29,37,
                                 4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,
                                 85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,
                                 42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 143,245,168,239,120,74,180,213,131,42,255,
                                 48,165,16,8,196,221,48,155,26,172,253,187,
                                 251,227,93,216,82,64,174,235,60,146,220,43,
                                 110,116,119,206,139,234,120,243,145,45,159,
                                 126,182,155,108,225,142,55,238,20,188,213,
                                 184,77,75,155,220,154,167,78,228,165,35,192,
                                 122,174,33,250,214,74,150,61,240,159,68,45,
                                 253,218,253,113,0,134,144,35,95,227,90,197,
                                 184,29,10,34,93,185,162,210,232,138,174,193,
                                 42,34,223,23,23,153,203,92,59,4,121,102,30,
                                 183,41,235,56,55,85,83,208,3,120,51,100,19,
                                 111,128,59,34,211,188,63,248,121,64,209,161,
                                 8,67,132,46,111,135,190,65,187,216,38,17,95,
                                 158,124,210,130,193,120,236,227,16,12,60,
                                 102,202,160,217,214,6,76,123,6,217,244,250,
                                 44,136,215,217,126,142,188,197,214,185,67,
                                 104,126,169,23,61,165,118,169,186,8,191,159,
                                 219,166,66,83,212,188,185,115,205,11,181,34,
                                 53,122,250,13,152,245,178,127,228,210,170,
                                 158,108,52,103,30,83,182,63,236,24,167,145,
                                 191,249,23,26,248,30,53,118,37,130,87,188,
                                 70,114,119,211,127,83,199>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:11:17.943+05:30,ns_1@127.0.0.1:<0.218.0>:restartable:start_child:98]Started child process <0.219.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2020-04-02T21:11:17.943+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.219.0>,menelaus_web}
             started: [{pid,<0.238.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 2,10,239,87,7,162,191,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,100,101,54,
                                 102,51,51,52,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,100,
                                 101,54,102,51,51,52,48,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,186,18,153,62,228,
                                 45,6,124,74,134,28,191,93,198,204,146,47,44,
                                 45,222,2,220,85,9,130,51,201,96,112,196,70,
                                 88,54,134,84,94,162,56,255,144,200,179,131,
                                 200,29,61,35,155,182,128,231,21,155,237,107,
                                 221,227,153,226,188,36,110,219,85,152,82,44,
                                 118,186,13,127,104,198,251,214,218,99,130,
                                 234,42,128,242,102,120,217,175,66,222,162,
                                 85,28,99,237,30,96,142,188,160,220,17,50,5,
                                 169,231,114,171,155,206,181,203,93,114,200,
                                 225,35,121,249,190,36,16,225,102,230,105,29,
                                 151,160,141,48,74,181,138,153,119,222,150,
                                 103,3,179,110,178,89,173,171,183,184,160,
                                 110,65,57,168,17,218,89,225,87,87,233,78,
                                 103,205,70,145,146,248,227,184,26,80,201,
                                 234,140,45,43,174,45,67,184,20,67,105,212,
                                 47,15,245,246,33,203,134,58,69,22,95,49,32,
                                 70,119,44,53,119,196,4,91,24,148,226,127,
                                 235,181,244,32,136,221,246,168,236,45,99,
                                 186,28,194,246,151,13,94,240,31,88,169,90,
                                 67,197,229,89,252,123,32,98,229,192,54,198,
                                 168,26,171,92,82,4,176,252,49,65,196,93,48,
                                 35,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,
                                 15,1,1,255,4,4,3,2,2,164,48,19,6,3,85,29,37,
                                 4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,
                                 85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,
                                 42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 143,245,168,239,120,74,180,213,131,42,255,
                                 48,165,16,8,196,221,48,155,26,172,253,187,
                                 251,227,93,216,82,64,174,235,60,146,220,43,
                                 110,116,119,206,139,234,120,243,145,45,159,
                                 126,182,155,108,225,142,55,238,20,188,213,
                                 184,77,75,155,220,154,167,78,228,165,35,192,
                                 122,174,33,250,214,74,150,61,240,159,68,45,
                                 253,218,253,113,0,134,144,35,95,227,90,197,
                                 184,29,10,34,93,185,162,210,232,138,174,193,
                                 42,34,223,23,23,153,203,92,59,4,121,102,30,
                                 183,41,235,56,55,85,83,208,3,120,51,100,19,
                                 111,128,59,34,211,188,63,248,121,64,209,161,
                                 8,67,132,46,111,135,190,65,187,216,38,17,95,
                                 158,124,210,130,193,120,236,227,16,12,60,
                                 102,202,160,217,214,6,76,123,6,217,244,250,
                                 44,136,215,217,126,142,188,197,214,185,67,
                                 104,126,169,23,61,165,118,169,186,8,191,159,
                                 219,166,66,83,212,188,185,115,205,11,181,34,
                                 53,122,250,13,152,245,178,127,228,210,170,
                                 158,108,52,103,30,83,182,63,236,24,167,145,
                                 191,249,23,26,248,30,53,118,37,130,87,188,
                                 70,114,119,211,127,83,199>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.944+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.218.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.944+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.214.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:17.948+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.256.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.948+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.259.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.951+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.261.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:11:17.952+05:30,ns_1@127.0.0.1:users_replicator<0.261.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-04-02T21:11:17.953+05:30,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.261.0>
[ns_server:debug,2020-04-02T21:11:17.953+05:30,ns_1@127.0.0.1:users_replicator<0.261.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.262.0>
[ns_server:debug,2020-04-02T21:11:17.954+05:30,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-04-02T21:11:17.954+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.262.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.954+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.260.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T21:11:17.958+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-04-02T21:11:17.958+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.264.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.960+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.267.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:17.960+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.258.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:17.960+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.270.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:17.960+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.271.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:11:17.968+05:30,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-04-02T21:11:17.968+05:30,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 14ms
[ns_server:debug,2020-04-02T21:11:17.970+05:30,ns_1@127.0.0.1:users_replicator<0.261.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[ns_server:debug,2020-04-02T21:11:17.971+05:30,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.275.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[error_logger:info,2020-04-02T21:11:17.971+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.274.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:11:17.971+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T21:11:17.971+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:17.971+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.857367481.2053373953.51767>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:11:17.971+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.857367481.2053373953.51767>,
                                  inet_tcp_dist,<0.278.0>,
                                  #Ref<0.857367481.2053373953.51768>}
[ns_server:debug,2020-04-02T21:11:17.971+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.857367481.2053373953.51767>,
                               inet_tcp_dist,<0.278.0>,
                               #Ref<0.857367481.2053373953.51768>}
[ns_server:debug,2020-04-02T21:11:17.972+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-04-02T21:11:17.972+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.278.0>,shutdown}}
[error_logger:info,2020-04-02T21:11:17.972+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:18.172+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T21:11:18.172+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:18.172+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.857367481.2053373954.51142>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:11:18.172+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.857367481.2053373954.51142>,
                                  inet_tcp_dist,<0.281.0>,
                                  #Ref<0.857367481.2053373954.51146>}
[ns_server:debug,2020-04-02T21:11:18.205+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-04-02T21:11:18.406+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-04-02T21:11:18.661+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.285.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:11:18.742+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-04-02T21:11:18.757+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.857367481.2053373954.51142>,
                               inet_tcp_dist,<0.281.0>,
                               #Ref<0.857367481.2053373954.51146>}
[error_logger:info,2020-04-02T21:11:18.757+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.281.0>,connection_closed}}
[ns_server:info,2020-04-02T21:11:18.862+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.274.0>:ns_port_server:log:224]ns_couchdb<0.274.0>: Apache CouchDB  (LogLevel=info) is starting.
ns_couchdb<0.274.0>: Failure to start Mochiweb: eaddrinuse
ns_couchdb<0.274.0>: 13390: Booted. Waiting for shutdown request
ns_couchdb<0.274.0>: [os_mon] cpu supervisor port (cpu_sup): Erlang has closed
ns_couchdb<0.274.0>: [os_mon] memory supervisor port (memsup): Erlang has closed

[error_logger:info,2020-04-02T21:11:18.943+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:18.943+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T21:11:18.943+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.857367481.2053373956.50662>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:11:18.943+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.857367481.2053373956.50662>,
                                  inet_tcp_dist,<0.287.0>,
                                  #Ref<0.857367481.2053373956.50666>}
[ns_server:debug,2020-04-02T21:11:18.944+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.857367481.2053373956.50662>,
                               inet_tcp_dist,<0.287.0>,
                               #Ref<0.857367481.2053373956.50666>}
[error_logger:info,2020-04-02T21:11:18.944+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.287.0>,shutdown}}
[error_logger:info,2020-04-02T21:11:18.944+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:18.944+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-04-02T21:11:19.145+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:19.145+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T21:11:19.145+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.857367481.2053373956.50677>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:11:19.145+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.857367481.2053373956.50677>,
                                  inet_tcp_dist,<0.290.0>,
                                  #Ref<0.857367481.2053373956.50681>}
[ns_server:debug,2020-04-02T21:11:19.145+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.857367481.2053373956.50677>,
                               inet_tcp_dist,<0.290.0>,
                               #Ref<0.857367481.2053373956.50681>}
[error_logger:info,2020-04-02T21:11:19.146+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.290.0>,shutdown}}
[error_logger:info,2020-04-02T21:11:19.146+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:19.146+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-04-02T21:11:19.347+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:19.347+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T21:11:19.347+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.857367481.2053373955.50603>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:11:19.347+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.857367481.2053373955.50603>,
                                  inet_tcp_dist,<0.293.0>,
                                  #Ref<0.857367481.2053373955.50607>}
[ns_server:debug,2020-04-02T21:11:19.348+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.857367481.2053373955.50603>,
                               inet_tcp_dist,<0.293.0>,
                               #Ref<0.857367481.2053373955.50607>}
[error_logger:info,2020-04-02T21:11:19.348+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.293.0>,shutdown}}
[ns_server:debug,2020-04-02T21:11:19.348+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-04-02T21:11:19.348+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:info,2020-04-02T21:11:19.383+05:30,ns_1@127.0.0.1:ns_couchdb_port<0.274.0>:ns_port_server:log:224]ns_couchdb<0.274.0>: {"Kernel pid terminated",application_controller,"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\"},{line,102}]},{supervisor,do_start_child,2,[{file,\"supervisor.erl\"},{line,365}]},{supervisor,start_children,3,[{file,\"supervisor.erl\"},{line,348}]},{supervisor,init_children,2,[{file,\"supervisor.erl\"},{line,314}]},{gen_server,init_it,2,[{file,\"gen_server.erl\"},{line,365}]},{gen_server,init_it,6,[{file,\"gen_server.erl\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}"}
ns_couchdb<0.274.0>: Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro
ns_couchdb<0.274.0>: 
ns_couchdb<0.274.0>: Crash dump is being written to: erl_crash.dump.1585842038.12600.ns_couchdb...done

[ns_server:error,2020-04-02T21:11:19.385+05:30,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.275.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:189]ns_couchdb_port(<0.274.0>) died with reason {abnormal,1}
[ns_server:debug,2020-04-02T21:11:19.385+05:30,ns_1@127.0.0.1:<0.268.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.267.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:11:19.385+05:30,ns_1@127.0.0.1:<0.265.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.264.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:11:19.385+05:30,ns_1@127.0.0.1:<0.266.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.264.0>} exited with reason shutdown
[error_logger:error,2020-04-02T21:11:19.383+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]** Generic server ns_couchdb_port terminating 
** Last message in was {#Port<0.5097>,{exit_status,1}}
** When Server state == {state,#Port<0.5097>,
                            {ns_couchdb,"/opt/couchbase/lib/erlang/bin/erl",
                                ["-pa",
                                 "/opt/couchbase/lib/erlang/lib/asn1-5.0.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/compiler-7.1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEvent-2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosEventDomain-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosFileTransfer-1.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosNotification-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosProperty-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTime-1.2.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/cosTransactions-1.3.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/crypto-4.2.2.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/dialyzer-3.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/diameter-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/edoc-0.9.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eldap-1.2.3.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_docgen-0.7.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erl_interface-3.10.2.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/erts-9.3.3.9/ebin",
                                 "/opt/couchbase/lib/erlang/lib/eunit-2.3.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/hipe-3.17.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ic-4.4.4.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/inets-6.5.2.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/mnesia-4.15.3.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/orber-3.8.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/os_mon-2.4.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/otp_mibs-1.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/parsetools-2.1.6/ebin",
                                 "/opt/couchbase/lib/erlang/lib/public_key-1.5.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/reltool-0.7.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/runtime_tools-1.12.5/ebin",
                                 "/opt/couchbase/lib/erlang/lib/sasl-3.1.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/snmp-5.2.11/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssh-4.6.9.3/ebin",
                                 "/opt/couchbase/lib/erlang/lib/ssl-8.2.6.4/ebin",
                                 "/opt/couchbase/lib/erlang/lib/syntax_tools-2.1.4.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/tools-2.11.2/ebin",
                                 "/opt/couchbase/lib/erlang/lib/xmerl-1.3.16.1/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/gc-couchbase-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/vtree-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/plugins/wkb-1.2.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch-1.2.0a-961ad59-git/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_audit-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_dcp-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_index_merger-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_set_view-1.0.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/couch_view_parser-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/ejson-0.1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/erlang-oauth/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/etap/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/lhttpc-1.3/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mapreduce-1.0/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/mochiweb-1.4.1/ebin",
                                 "/opt/couchbase/lib/couchdb/erlang/lib/snappy-1.0.4/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ale/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/gen_smtp/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_babysitter/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_couchdb/ebin",
                                 "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/ebin",
                                 "/opt/couchbase/lib/erlang/lib/stdlib-3.4.5.1/ebin",
                                 "/opt/couchbase/lib/erlang/lib/kernel-5.4.3.2/ebin",
                                 ".","-couch_ini",
                                 "/opt/couchbase/etc/couchdb/default.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/capi.ini",
                                 "/opt/couchbase/etc/couchdb/default.d/geocouch.ini",
                                 "/opt/couchbase/etc/couchdb/local.ini",
                                 "-kernel","error_logger","false","-kernel",
                                 "error_logger","false","inetrc",
                                 "\"/opt/couchbase/etc/couchbase/hosts.cfg\"",
                                 "dist_config_file",
                                 "\"/opt/couchbase/var/lib/couchbase/config/dist_cfg\"",
                                 "-ssl_dist_optfile",
                                 "/opt/couchbase/etc/couchbase/ssl_dist_opts",
                                 "-setcookie",
                                 "2a158eb185476066b502c8ced82b23b042bd1ae5f1ef74f024e7bb9916796a9b",
                                 "-name","couchdb_ns_1@cb.local","-smp",
                                 "enable","+P","327680","+K","true","-kernel",
                                 "error_logger","false","-sasl",
                                 "sasl_error_logger","false","-nouser",
                                 "-hidden","-proto_dist","cb","-epmd_module",
                                 "cb_epmd","-start_epmd","false","-run",
                                 "child_erlang","child_start","ns_couchdb"],
                                [use_stdio,
                                 {env,
                                     [{"NS_COUCHDB_ENV_ARGS",
                                       "[{ns_server_node,'ns_1@127.0.0.1'},\n {path_config_tmpdir,\"/opt/couchbase/var/lib/couchbase/tmp\"},\n {net_kernel_verbosity,10},\n {loglevel_error_logger,debug},\n {path_config_libdir,\"/opt/couchbase/lib\"},\n {loglevel_stats,debug},\n {loglevel_menelaus,debug},\n {path_config_secdir,\"/opt/couchbase/etc/security\"},\n {loglevel_user,debug},\n {path_config_etcdir,\"/opt/couchbase/etc/couchbase\"},\n {loglevel_ns_server,debug},\n {loglevel_mapreduce_errors,debug},\n {loglevel_rebalance,debug},\n {loglevel_default,debug},\n {disk_sink_opts,[{rotation,[{compress,true},\n                             {size,41943040},\n                             {num_files,10},\n                             {buffer_size_max,52428800}]}]},\n {loglevel_cbas,debug},\n {loglevel_xdcr,debug},\n {loglevel_ns_doctor,debug},\n {loglevel_access,info},\n {error_logger_mf_dir,\"/opt/couchbase/var/lib/couchbase/logs\"},\n {path_config_datadir,\"/opt/couchbase/var/lib/couchbase\"},\n {loglevel_cluster,debug},\n {loglevel_couchdb,info},\n {loglevel_views,debug},\n {path_config_bindir,\"/opt/couchbase/bin\"}]"},
                                      {"ERL_CRASH_DUMP",
                                       "erl_crash.dump.1585842038.12600.ns_couchdb"}]}]},
                            {ringbuffer,1191,1024,
                                {[{<<"Crash dump is being written to: erl_crash.dump.1585842038.12600.ns_couchdb...done">>,
                                   81},
                                  {<<>>,0},
                                  {<<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                                   200}],
                                 [{<<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>,
                                   910}]}},
                            undefined,
                            {ok,{-576460749455,
                                 #Ref<0.857367481.2053373953.51779>}},
                            [<<"Crash dump is being written to: erl_crash.dump.1585842038.12600.ns_couchdb...done">>,
                             <<>>,
                             <<"Kernel pid terminated (application_controller) ({application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{erro">>,
                             <<"{\"Kernel pid terminated\",application_controller,\"{application_start_failure,ns_couchdb,{{shutdown,{failed_to_start_child,cb_couch_sup,{shutdown,{failed_to_start_child,couch_app,{'EXIT',{{badmatch,{error,{shutdown,{failed_to_start_child,couch_secondary_services,{shutdown,{failed_to_start_child,httpd,eaddrinuse}}}}}},[{couch_server_sup,start_server,1,[{file,\\\"/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/couchdb/couch_server_sup.erl\\\"},{line,102}]},{supervisor,do_start_child,2,[{file,\\\"supervisor.erl\\\"},{line,365}]},{supervisor,start_children,3,[{file,\\\"supervisor.erl\\\"},{line,348}]},{supervisor,init_children,2,[{file,\\\"supervisor.erl\\\"},{line,314}]},{gen_server,init_it,2,[{file,\\\"gen_server.erl\\\"},{line,365}]},{gen_server,init_it,6,[{file,\\\"gen_server.erl\\\"},{line,333}]},{proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,247}]}]}}}}}},{ns_couchdb,start,[normal,[]]}}}\"}">>],
                            0}
** Reason for termination == 
** {abnormal,1}

[ns_server:debug,2020-04-02T21:11:19.385+05:30,ns_1@127.0.0.1:<0.257.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.256.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:11:19.385+05:30,ns_1@127.0.0.1:<0.269.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {user_storage_events,<0.267.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:11:19.385+05:30,ns_1@127.0.0.1:<0.218.0>:restartable:shutdown_child:120]Successfully terminated process <0.219.0>
[ns_server:debug,2020-04-02T21:11:19.385+05:30,ns_1@127.0.0.1:<0.217.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.216.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:11:19.386+05:30,ns_1@127.0.0.1:<0.203.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.202.0>} exited with reason shutdown
[ns_server:debug,2020-04-02T21:11:19.386+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:wait_saver:866]Done waiting for saver.
[error_logger:error,2020-04-02T21:11:19.388+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_port_server:init/1
    pid: <0.274.0>
    registered_name: ns_couchdb_port
    exception exit: {abnormal,1}
      in function  gen_server:handle_common_reply/8 (gen_server.erl, line 726)
    ancestors: [ns_server_nodes_sup,<0.208.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 1
    messages: [{'EXIT',#Port<0.5097>,normal}]
    links: [<0.209.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 11926
  neighbours:

[error_logger:error,2020-04-02T21:11:19.388+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: erlang:apply/2
    pid: <0.275.0>
    registered_name: wait_link_to_couchdb_node
    exception exit: {abnormal,1}
      in function  ns_server_nodes_sup:do_wait_link_to_couchdb_node/1 (src/ns_server_nodes_sup.erl, line 190)
    ancestors: [ns_server_nodes_sup,<0.208.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.209.0>,<0.276.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 3352
  neighbours:
    neighbour:
      pid: <0.276.0>
      registered_name: []
      initial call: ns_server_nodes_sup:'-do_wait_link_to_couchdb_node/1-fun-2-'/0
      current_function: {timer,sleep,1}
      ancestors: [wait_link_to_couchdb_node,ns_server_nodes_sup,<0.208.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
      message_queue_len: 0
      links: [<0.275.0>]
      trap_exit: false
      status: waiting
      heap_size: 2586
      stack_size: 12
      reductions: 10861
      current_stacktrace: [{timer,sleep,1,[{file,"timer.erl"},{line,153}]},
                  {misc,poll_for_condition_rec,3,
                      [{file,"src/misc.erl"},{line,508}]},
                  {ns_server_nodes_sup,
                      '-do_wait_link_to_couchdb_node/1-fun-2-',2,
                      [{file,"src/ns_server_nodes_sup.erl"},{line,159}]},
                  {proc_lib,init_p,3,[{file,"proc_lib.erl"},{line,232}]}]

[error_logger:error,2020-04-02T21:11:19.388+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    start_error
     Reason:     {abnormal,1}
     Offender:   [{pid,undefined},
                  {name,wait_for_couchdb_node},
                  {mfargs,{erlang,apply,
                                  [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:error,2020-04-02T21:11:19.388+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_nodes_sup}
     Context:    shutdown_error
     Reason:     {abnormal,1}
     Offender:   [{pid,<0.274.0>},
                  {name,start_couchdb_node},
                  {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                  {restart_type,{permanent,5}},
                  {shutdown,86400000},
                  {child_type,worker}]


[error_logger:error,2020-04-02T21:11:19.389+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_cluster_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,wait_for_couchdb_node,
                         {abnormal,1}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_nodes_sup},
                  {mfargs,
                      {restartable,start_link,
                          [{ns_server_nodes_sup,start_link,[]},infinity]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-04-02T21:11:19.389+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,root_sup}
     Context:    start_error
     Reason:     {shutdown,
                     {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                             {failed_to_start_child,wait_for_couchdb_node,
                                 {abnormal,1}}}}}
     Offender:   [{pid,undefined},
                  {id,ns_server_cluster_sup},
                  {mfargs,{ns_server_cluster_sup,start_link,[]}},
                  {restart_type,permanent},
                  {shutdown,infinity},
                  {child_type,supervisor}]


[error_logger:error,2020-04-02T21:11:19.389+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: application_master:init/4
    pid: <0.117.0>
    registered_name: []
    exception exit: {{shutdown,
                      {failed_to_start_child,ns_server_cluster_sup,
                       {shutdown,
                        {failed_to_start_child,ns_server_nodes_sup,
                         {shutdown,
                          {failed_to_start_child,wait_for_couchdb_node,
                           {abnormal,1}}}}}}},
                     {ns_server,start,[normal,[]]}}
      in function  application_master:init/4 (application_master.erl, line 134)
    ancestors: [<0.116.0>]
    message_queue_len: 1
    messages: [{'EXIT',<0.118.0>,normal}]
    links: [<0.116.0>,<0.33.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 274
  neighbours:

[error_logger:info,2020-04-02T21:11:19.389+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
         application: ns_server
              exited: {{shutdown,
                        {failed_to_start_child,ns_server_cluster_sup,
                         {shutdown,
                          {failed_to_start_child,ns_server_nodes_sup,
                           {shutdown,
                            {failed_to_start_child,wait_for_couchdb_node,
                             {abnormal,1}}}}}}},
                       {ns_server,start,[normal,[]]}}
                type: permanent

[error_logger:info,2020-04-02T21:11:19.389+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,system_memory_high_watermark}

[error_logger:info,2020-04-02T21:11:19.389+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-calculator/544"}}

[error_logger:info,2020-04-02T21:11:19.389+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/8935"}}

[error_logger:info,2020-04-02T21:11:19.389+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-calculator/704"}}

[error_logger:info,2020-04-02T21:11:19.389+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/59"}}

[error_logger:info,2020-04-02T21:11:19.390+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/pycharm-community/188"}}

[error_logger:info,2020-04-02T21:11:19.390+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gnome-system-monitor/127"}}

[error_logger:info,2020-04-02T21:11:19.390+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core18/1705"}}

[error_logger:info,2020-04-02T21:11:19.390+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core/8689"}}

[error_logger:info,2020-04-02T21:11:19.390+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gnome-system-monitor/135"}}

[error_logger:info,2020-04-02T21:11:19.390+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-characters/495"}}

[error_logger:info,2020-04-02T21:11:19.390+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-28-1804/116"}}

[error_logger:info,2020-04-02T21:11:19.390+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gtk-common-themes/1474"}}

[error_logger:info,2020-04-02T21:11:19.390+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/core18/1668"}}

[error_logger:info,2020-04-02T21:11:19.390+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-logs/81"}}

[error_logger:info,2020-04-02T21:11:19.390+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-logs/93"}}

[error_logger:info,2020-04-02T21:11:19.390+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-3-26-1604/98"}}

[error_logger:info,2020-04-02T21:11:19.390+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,{disk_almost_full,"/snap/gnome-characters/399"}}

[error_logger:info,2020-04-02T21:11:19.391+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
       alarm_handler: {clear,
                          {disk_almost_full,"/snap/gtk-common-themes/1440"}}

[ns_server:info,2020-04-02T21:11:26.671+05:30,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-04-02T21:11:26.707+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-04-02T21:11:26.707+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.707+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.707+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.708+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.708+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.708+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.708+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.708+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.708+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.708+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.708+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.708+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.708+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.709+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.709+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.709+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.709+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.709+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.709+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.709+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.709+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.709+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.710+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.710+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.710+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-04-02T21:11:26.710+05:30,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-04-02T21:11:26.720+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-04-02T21:11:26.720+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[error_logger:info,2020-04-02T21:11:26.722+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.168.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2020-04-02T21:11:26.723+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.167.0>},
                       {id,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:11:26.729+05:30,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@127.0.0.1'
[error_logger:info,2020-04-02T21:11:26.739+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:26.739+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:26.739+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:26.741+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.174.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:26.743+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.176.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T21:11:26.743+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2020-04-02T21:11:26.743+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.177.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:26.744+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.175.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:26.744+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:26.745+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:26.746+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:26.747+05:30,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.180.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:11:26.748+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-04-02T21:11:26.748+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-04-02T21:11:26.749+05:30,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2020-04-02T21:11:26.750+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-04-02T21:11:26.750+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.181.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:26.751+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.169.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-04-02T21:11:26.751+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-04-02T21:11:28.464+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-04-02T21:11:28.464+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2020-04-02T21:11:28.464+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:28.464+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T21:11:28.464+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2970197602.174325762.60546>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:11:28.464+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2970197602.174325762.60546>,
                                  inet_tcp_dist,<0.185.0>,
                                  #Ref<0.2970197602.174325762.60548>}
[ns_server:debug,2020-04-02T21:11:28.469+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-04-02T21:11:28.469+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2020-04-02T21:11:28.469+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-04-02T21:11:29.244+05:30,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-04-02T21:11:29.246+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:29.288+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:11:29.292+05:30,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,15,0}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,4,2},{21,11,29}}},
               {memory,
                   [{total,26291776},
                    {processes,9524064},
                    {processes_used,9517912},
                    {system,16767712},
                    {atom,388625},
                    {atom_used,364409},
                    {binary,86312},
                    {code,8250921},
                    {ets,1509176}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    error_handler,application,application_master,kernel,gen,
                    file,code,error_logger,file_io_server,code_server,
                    gen_server,heart,file_server,supervisor,proc_lib,lists,
                    ets,erl_eval,gen_event,filename,application_controller,
                    erl_lint,erl_parse,erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{os_mon,"CPO  CXC 138 46","2.4.4"},
                    {sasl,"SASL  CXC 138 11","3.1.2"},
                    {ns_server,"Couchbase server","6.5.0-4960-enterprise"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ale,"Another Logger for Erlang","0.0.0"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,131},
               {node,'ns_1@127.0.0.1'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,kernel_safe_sup,
                    auth,httpd_sup,dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    release_handler,lhttpc_sup,httpc_sup,lhttpc_manager,
                    alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    'sink-disk_xdcr',timer_server,'sink-disk_debug',
                    inet_gethost_native,ale_sup,'sink-disk_error',inet_db,
                    'sink-disk_default',ssl_pem_cache_dist,ale_dynamic_sup,
                    rex,global_group,net_sup,kernel_sup,ssl_connection_sup,
                    global_name_server,ssl_admin_sup,tftp_sup,ssl_sup,
                    root_sup,erts_code_purger,os_mon_sup,file_server_2,
                    error_logger,cpu_sup,erl_epmd,init,memsup,
                    erl_signal_server,disksup,ale,net_kernel,dist_manager,
                    ssl_pem_cache,ssl_manager,ssl_dist_admin_sup,
                    ssl_dist_connection_sup,ssl_dist_sup,user,
                    ssl_tls_dist_proxy,ssl_manager_dist,sasl_safe_sup,
                    ssl_listen_tracker_sup,inet_gethost_native_sup,
                    code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,3}]
[ns_server:info,2020-04-02T21:11:29.300+05:30,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"cfa0f75f28402d2e1aa254b2a374bead19433526\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" upstream=\"master\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"f2a16b53bb74146f20d18ba2c0443d5f10a9a550\" upstream=\"master\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4960\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" upstream=\"master\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" upstream=\"mad-hatter\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" upstream=\"master\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" upstream=\"master\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" upstream=\"master\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" upstream=\"mad-hatter\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" upstream=\"cb-master\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" upstream=\"master\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" upstream=\"master\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" upstream=\"mad-hatter\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" upstream=\"mad-hatter\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" upstream=\"mad-hatter\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" upstream=\"master\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" upstream=\"master\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" upstream=\"mad-hatter\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" upstream=\"mad-hatter\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" upstream=\"mad-hatter\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" upstream=\"mad-hatter\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" upstream=\"master\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" upstream=\"master\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" upstream=\"master\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" upstream=\"mad-hatter\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" upstream=\"master\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"7a3aa547b3f5eb3ea28d279a08384609cd2cea7c\" upstream=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" upstream=\"mad-hatter\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" upstream=\"master\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" upstream=\"master\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" upstream=\"master\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" upstream=\"v1.1.0-couchbase\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"ee64d41320d14fabe814a241a5cf4f6a6f6e827a\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" upstream=\"master\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" upstream=\"master\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-04-02T21:11:29.305+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:29.306+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:29.307+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:11:29.308+05:30,ns_1@127.0.0.1:ns_config_sup<0.192.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-04-02T21:11:29.308+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:29.308+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.194.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:11:29.333+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-04-02T21:11:29.334+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2020-04-02T21:11:29.342+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[{alert_limits,
   [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
  {audit,
   [{auditd_enabled,false},
    {rotate_interval,86400},
    {rotate_size,20971520},
    {disabled,[]},
    {sync,[]},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
  {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
  {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
  {autocompaction,
   [{database_fragmentation_threshold,{30,undefined}},
    {view_fragmentation_threshold,{30,undefined}}]},
  {buckets,[{configs,[]}]},
  {cbas_memory_quota,2174},
  {cert_and_pkey,
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFgIK71cHor8wDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBkZTZmMzM0MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgZGU2ZjMz\nNDAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC6Epk+5C0GfEqGHL9d\nxsySLywt3gLcVQmCM8lgcMRGWDaGVF6iOP+QyLODyB09I5u2gOcVm+1r3eOZ4rwk\nbttVmFIsdroNf2jG+9baY4LqKoDyZnjZr0LeolUcY+0eYI68oNwRMgWp53Krm861\ny11yyOEjefm+JBDhZuZpHZegjTBKtYqZd96WZwOzbrJZrau3uKBuQTmoEdpZ4VdX\n6U5nzUaRkvjjuBpQyeqMLSuuLUO4FENp1C8P9fYhy4Y6RRZfMSBGdyw1d8QEWxiU\n4n/rtfQgiN32qOwtY7ocwvaXDV7wH1ipWkPF5Vn8eyBi5cA2xqgaq1xSBLD8MUHE\nXTAjAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCP9ajveEq01YMq\n/zClEAjE3TCbGqz9u/vjXdhSQK7rPJLcK250d86L6njzkS2ffrabbOGON+4UvNW4\nTUub3JqnTuSlI8B6riH61kqWPfCfRC392v1xAIaQI1/jWsW4HQoiXbmi0uiKrsEq\nIt8XF5nLXDsEeWYetynrODdVU9ADeDNkE2+AOyLTvD/4eUDRoQhDhC5vh75Bu9gm\nEV+efNKCwXjs4xAMPGbKoNnWBkx7Btn0+iyI19l+jrzF1rlDaH6pFz2ldqm6CL+f\n26ZCU9S8uXPNC7UiNXr6DZj1sn/k0qqebDRnHlO2P+wYp5G/+Rca+B41diWCV7xG\ncnfTf1PH\n-----END CERTIFICATE-----\n">>,
     <<"*****">>}]},
  {drop_request_memory_threshold_mib,undefined},
  {email_alerts,
   [{recipients,["root@localhost"]},
    {sender,"couchbase@localhost"},
    {enabled,false},
    {email_server,
     [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
    {alerts,
     [auto_failover_node,auto_failover_maximum_reached,
      auto_failover_other_nodes_down,auto_failover_cluster_too_small,
      auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
      ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
      ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
  {fts_memory_quota,512},
  {index_aware_rebalance_disabled,false},
  {log_redaction_default_cfg,[{redact_level,none}]},
  {max_bucket_count,30},
  {memcached,[]},
  {memory_quota,8886},
  {nodes_wanted,['ns_1@127.0.0.1']},
  {password_policy,[{min_length,6},{must_present,[]}]},
  {quorum_nodes,['ns_1@127.0.0.1']},
  {remote_clusters,[]},
  {replication,[{enabled,true}]},
  {rest,[{port,8091}]},
  {rest_creds,null},
  {secure_headers,[]},
  {server_groups,
   [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
  {set_view_update_daemon,
   [{update_interval,5000},
    {update_min_changes,5000},
    {replica_update_min_changes,5000}]},
  {{couchdb,max_parallel_indexers},4},
  {{couchdb,max_parallel_replica_indexers},2},
  {{local_changes_count,<<"8c43a5102cad1e34db659ab4d5646878">>},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{6,63753061277}}]}]},
  {{metakv,<<"/indexing/settings/config">>},
   <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
  {{request_limit,capi},undefined},
  {{request_limit,rest},undefined},
  {{node,'ns_1@127.0.0.1',address_family},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    inet]},
  {{node,'ns_1@127.0.0.1',audit},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
  {{node,'ns_1@127.0.0.1',capi_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8092]},
  {{node,'ns_1@127.0.0.1',cbas_admin_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9110]},
  {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9113]},
  {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9112]},
  {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9111]},
  {{node,'ns_1@127.0.0.1',cbas_cluster_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9115]},
  {{node,'ns_1@127.0.0.1',cbas_console_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9114]},
  {{node,'ns_1@127.0.0.1',cbas_data_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9116]},
  {{node,'ns_1@127.0.0.1',cbas_debug_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    -1]},
  {{node,'ns_1@127.0.0.1',cbas_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8095]},
  {{node,'ns_1@127.0.0.1',cbas_messaging_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9118]},
  {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9119]},
  {{node,'ns_1@127.0.0.1',cbas_metadata_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9121]},
  {{node,'ns_1@127.0.0.1',cbas_parent_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9122]},
  {{node,'ns_1@127.0.0.1',cbas_replication_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9120]},
  {{node,'ns_1@127.0.0.1',cbas_result_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9117]},
  {{node,'ns_1@127.0.0.1',cbas_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18095]},
  {{node,'ns_1@127.0.0.1',compaction_daemon},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@127.0.0.1',config_version},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    {6,5}]},
  {{node,'ns_1@127.0.0.1',erl_external_listeners},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {inet,false},
    {inet6,false}]},
  {{node,'ns_1@127.0.0.1',eventing_debug_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9140]},
  {{node,'ns_1@127.0.0.1',eventing_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8096]},
  {{node,'ns_1@127.0.0.1',eventing_https_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18096]},
  {{node,'ns_1@127.0.0.1',fts_grpc_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9130]},
  {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    19130]},
  {{node,'ns_1@127.0.0.1',fts_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8094]},
  {{node,'ns_1@127.0.0.1',fts_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18094]},
  {{node,'ns_1@127.0.0.1',indexer_admin_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9100]},
  {{node,'ns_1@127.0.0.1',indexer_http_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9102]},
  {{node,'ns_1@127.0.0.1',indexer_https_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    19102]},
  {{node,'ns_1@127.0.0.1',indexer_scan_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9101]},
  {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9104]},
  {{node,'ns_1@127.0.0.1',indexer_stinit_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9103]},
  {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9105]},
  {{node,'ns_1@127.0.0.1',is_enterprise},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    true]},
  {{node,'ns_1@127.0.0.1',isasl},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@127.0.0.1',membership},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    active]},
  {{node,'ns_1@127.0.0.1',memcached},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {port,11210},
    {dedicated_port,11209},
    {dedicated_ssl_port,11206},
    {ssl_port,11207},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_sleeptime,19},
    {log_rotation_period,39003}]},
  {{node,'ns_1@127.0.0.1',memcached_config},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    {[{interfaces,
       {memcached_config_mgr,omit_missing_mcd_ports,
        [{[{host,<<"*">>},
           {port,port},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_port},
           {system,true},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,ssl_port},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_ssl_port},
           {system,true},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
      {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
      {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {opentracing,
       {[{enabled,opentracing_enabled},
         {module,{"~s",[opentracing_module]}},
         {config,{"~s",[opentracing_config]}}]}},
      {admin,{"~s",[admin_user]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize},
         {sleeptime,log_sleeptime}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
  {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    11206]},
  {{node,'ns_1@127.0.0.1',memcached_defaults},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {max_connections,65000},
    {system_connections,5000},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {opentracing_enabled,false},
    {opentracing_module,[]},
    {opentracing_config,[]},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {tracing_enabled,true},
    {datatype_snappy,true},
    {num_reader_threads,<<"default">>},
    {num_writer_threads,<<"default">>}]},
  {{node,'ns_1@127.0.0.1',moxi},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {port,0}]},
  {{node,'ns_1@127.0.0.1',node_encryption},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    false]},
  {{node,'ns_1@127.0.0.1',ns_log},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@127.0.0.1',port_servers},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
  {{node,'ns_1@127.0.0.1',projector_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9999]},
  {{node,'ns_1@127.0.0.1',projector_ssl_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9999]},
  {{node,'ns_1@127.0.0.1',query_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    8093]},
  {{node,'ns_1@127.0.0.1',rest},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@127.0.0.1',saslauthd_enabled},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    true]},
  {{node,'ns_1@127.0.0.1',ssl_capi_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18092]},
  {{node,'ns_1@127.0.0.1',ssl_query_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18093]},
  {{node,'ns_1@127.0.0.1',ssl_rest_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    18091]},
  {{node,'ns_1@127.0.0.1',uuid},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    <<"8c43a5102cad1e34db659ab4d5646878">>]},
  {{node,'ns_1@127.0.0.1',xdcr_rest_port},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    9998]},
  {{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
    false]}]]
[ns_server:info,2020-04-02T21:11:29.348+05:30,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',xdcr_rest_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9998]},
 {{node,'ns_1@127.0.0.1',uuid},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   <<"8c43a5102cad1e34db659ab4d5646878">>]},
 {{node,'ns_1@127.0.0.1',ssl_rest_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18091]},
 {{node,'ns_1@127.0.0.1',ssl_query_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18093]},
 {{node,'ns_1@127.0.0.1',ssl_capi_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18092]},
 {{node,'ns_1@127.0.0.1',saslauthd_enabled},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',rest},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@127.0.0.1',query_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8093]},
 {{node,'ns_1@127.0.0.1',projector_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',projector_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',port_servers},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
 {{node,'ns_1@127.0.0.1',ns_log},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@127.0.0.1',node_encryption},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',moxi},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {port,0}]},
 {{node,'ns_1@127.0.0.1',memcached_defaults},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   11206]},
 {{node,'ns_1@127.0.0.1',memcached_config},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@127.0.0.1',memcached},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@127.0.0.1',membership},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   active]},
 {{node,'ns_1@127.0.0.1',isasl},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@127.0.0.1',is_enterprise},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9105]},
 {{node,'ns_1@127.0.0.1',indexer_stinit_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9103]},
 {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9104]},
 {{node,'ns_1@127.0.0.1',indexer_scan_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9101]},
 {{node,'ns_1@127.0.0.1',indexer_https_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   19102]},
 {{node,'ns_1@127.0.0.1',indexer_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9102]},
 {{node,'ns_1@127.0.0.1',indexer_admin_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9100]},
 {{node,'ns_1@127.0.0.1',fts_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18094]},
 {{node,'ns_1@127.0.0.1',fts_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8094]},
 {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   19130]},
 {{node,'ns_1@127.0.0.1',fts_grpc_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9130]},
 {{node,'ns_1@127.0.0.1',eventing_https_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18096]},
 {{node,'ns_1@127.0.0.1',eventing_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8096]},
 {{node,'ns_1@127.0.0.1',eventing_debug_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9140]},
 {{node,'ns_1@127.0.0.1',erl_external_listeners},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {inet,false},
   {inet6,false}]},
 {{node,'ns_1@127.0.0.1',config_version},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   {6,5}]},
 {{node,'ns_1@127.0.0.1',compaction_daemon},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@127.0.0.1',cbas_ssl_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   18095]},
 {{node,'ns_1@127.0.0.1',cbas_result_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9117]},
 {{node,'ns_1@127.0.0.1',cbas_replication_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9120]},
 {{node,'ns_1@127.0.0.1',cbas_parent_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9122]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9121]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9119]},
 {{node,'ns_1@127.0.0.1',cbas_messaging_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9118]},
 {{node,'ns_1@127.0.0.1',cbas_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8095]},
 {{node,'ns_1@127.0.0.1',cbas_debug_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|-1]},
 {{node,'ns_1@127.0.0.1',cbas_data_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9116]},
 {{node,'ns_1@127.0.0.1',cbas_console_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9114]},
 {{node,'ns_1@127.0.0.1',cbas_cluster_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9115]},
 {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9111]},
 {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9112]},
 {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9113]},
 {{node,'ns_1@127.0.0.1',cbas_admin_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   9110]},
 {{node,'ns_1@127.0.0.1',capi_port},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   8092]},
 {{node,'ns_1@127.0.0.1',audit},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}]},
 {{node,'ns_1@127.0.0.1',address_family},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   inet]},
 {{request_limit,rest},undefined},
 {{request_limit,capi},undefined},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{local_changes_count,<<"8c43a5102cad1e34db659ab4d5646878">>},
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{6,63753061277}}]}]},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
 {secure_headers,[]},
 {rest_creds,null},
 {rest,[{port,8091}]},
 {replication,[{enabled,true}]},
 {remote_clusters,[]},
 {quorum_nodes,['ns_1@127.0.0.1']},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {nodes_wanted,['ns_1@127.0.0.1']},
 {memory_quota,8886},
 {memcached,[]},
 {max_bucket_count,30},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {index_aware_rebalance_disabled,false},
 {fts_memory_quota,512},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {drop_request_memory_threshold_mib,undefined},
 {cert_and_pkey,
  [{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{1,63753061241}}]}|
   {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFgIK71cHor8wDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBkZTZmMzM0MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgZGU2ZjMz\nNDAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC6Epk+5C0GfEqGHL9d\nxsySLywt3gLcVQmCM8lgcMRGWDaGVF6iOP+QyLODyB09I5u2gOcVm+1r3eOZ4rwk\nbttVmFIsdroNf2jG+9baY4LqKoDyZnjZr0LeolUcY+0eYI68oNwRMgWp53Krm861\ny11yyOEjefm+JBDhZuZpHZegjTBKtYqZd96WZwOzbrJZrau3uKBuQTmoEdpZ4VdX\n6U5nzUaRkvjjuBpQyeqMLSuuLUO4FENp1C8P9fYhy4Y6RRZfMSBGdyw1d8QEWxiU\n4n/rtfQgiN32qOwtY7ocwvaXDV7wH1ipWkPF5Vn8eyBi5cA2xqgaq1xSBLD8MUHE\nXTAjAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQCP9ajveEq01YMq\n/zClEAjE3TCbGqz9u/vjXdhSQK7rPJLcK250d86L6njzkS2ffrabbOGON+4UvNW4\nTUub3JqnTuSlI8B6riH61kqWPfCfRC392v1xAIaQI1/jWsW4HQoiXbmi0uiKrsEq\nIt8XF5nLXDsEeWYetynrODdVU9ADeDNkE2+AOyLTvD/4eUDRoQhDhC5vh75Bu9gm\nEV+efNKCwXjs4xAMPGbKoNnWBkx7Btn0+iyI19l+jrzF1rlDaH6pFz2ldqm6CL+f\n26ZCU9S8uXPNC7UiNXr6DZj1sn/k0qqebDRnHlO2P+wYp5G/+Rca+B41diWCV7xG\ncnfTf1PH\n-----END CERTIFICATE-----\n">>,
    <<"*****">>}]},
 {cbas_memory_quota,2174},
 {buckets,[{configs,[]}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]}]
[error_logger:info,2020-04-02T21:11:29.351+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.195.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:29.352+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.201.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:29.353+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.202.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:29.353+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T21:11:29.356+05:30,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"8c43a5102cad1e34db659ab4d5646878">>} ->
[{'_vclock',[{<<"8c43a5102cad1e34db659ab4d5646878">>,{7,63753061289}}]}]
[error_logger:info,2020-04-02T21:11:29.356+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.204.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:29.357+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.207.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:29.363+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:11:29.364+05:30,ns_1@127.0.0.1:menelaus_barrier<0.211.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-04-02T21:11:29.364+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:29.364+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:29.977+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.213.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:29.981+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.215.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:11:30.342+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2020-04-02T21:11:30.343+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2020-04-02T21:11:30.343+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2020-04-02T21:11:30.400+05:30,ns_1@127.0.0.1:ns_ssl_services_setup<0.216.0>:ns_ssl_services_setup:init:462]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,2,10,239,87,7,162,
             191,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
             6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
             118,101,114,32,100,101,54,102,51,51,52,48,48,30,23,13,49,51,48,
             49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
             57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
             104,98,97,115,101,32,83,101,114,118,101,114,32,100,101,54,102,
             51,51,52,48,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,
             0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,186,18,153,62,228,45,6,
             124,74,134,28,191,93,198,204,146,47,44,45,222,2,220,85,9,130,51,
             201,96,112,196,70,88,54,134,84,94,162,56,255,144,200,179,131,
             200,29,61,35,155,182,128,231,21,155,237,107,221,227,153,226,188,
             36,110,219,85,152,82,44,118,186,13,127,104,198,251,214,218,99,
             130,234,42,128,242,102,120,217,175,66,222,162,85,28,99,237,30,
             96,142,188,160,220,17,50,5,169,231,114,171,155,206,181,203,93,
             114,200,225,35,121,249,190,36,16,225,102,230,105,29,151,160,141,
             48,74,181,138,153,119,222,150,103,3,179,110,178,89,173,171,183,
             184,160,110,65,57,168,17,218,89,225,87,87,233,78,103,205,70,145,
             146,248,227,184,26,80,201,234,140,45,43,174,45,67,184,20,67,105,
             212,47,15,245,246,33,203,134,58,69,22,95,49,32,70,119,44,53,119,
             196,4,91,24,148,226,127,235,181,244,32,136,221,246,168,236,45,
             99,186,28,194,246,151,13,94,240,31,88,169,90,67,197,229,89,252,
             123,32,98,229,192,54,198,168,26,171,92,82,4,176,252,49,65,196,
             93,48,35,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,255,4,4,
             3,2,2,164,48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,
             15,6,3,85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,134,72,
             134,247,13,1,1,11,5,0,3,130,1,1,0,143,245,168,239,120,74,180,
             213,131,42,255,48,165,16,8,196,221,48,155,26,172,253,187,251,
             227,93,216,82,64,174,235,60,146,220,43,110,116,119,206,139,234,
             120,243,145,45,159,126,182,155,108,225,142,55,238,20,188,213,
             184,77,75,155,220,154,167,78,228,165,35,192,122,174,33,250,214,
             74,150,61,240,159,68,45,253,218,253,113,0,134,144,35,95,227,90,
             197,184,29,10,34,93,185,162,210,232,138,174,193,42,34,223,23,23,
             153,203,92,59,4,121,102,30,183,41,235,56,55,85,83,208,3,120,51,
             100,19,111,128,59,34,211,188,63,248,121,64,209,161,8,67,132,46,
             111,135,190,65,187,216,38,17,95,158,124,210,130,193,120,236,227,
             16,12,60,102,202,160,217,214,6,76,123,6,217,244,250,44,136,215,
             217,126,142,188,197,214,185,67,104,126,169,23,61,165,118,169,
             186,8,191,159,219,166,66,83,212,188,185,115,205,11,181,34,53,
             122,250,13,152,245,178,127,228,210,170,158,108,52,103,30,83,182,
             63,236,24,167,145,191,249,23,26,248,30,53,118,37,130,87,188,70,
             114,119,211,127,83,199>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2020-04-02T21:11:30.402+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.216.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:11:30.415+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-04-02T21:11:30.415+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-04-02T21:11:30.416+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-04-02T21:11:30.416+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:info,2020-04-02T21:11:30.431+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-04-02T21:11:30.432+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-04-02T21:11:30.432+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[error_logger:info,2020-04-02T21:11:30.431+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.219.0>,menelaus_web}
             started: [{pid,<0.220.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 2,10,239,87,7,162,191,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,100,101,54,
                                 102,51,51,52,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,100,
                                 101,54,102,51,51,52,48,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,186,18,153,62,228,
                                 45,6,124,74,134,28,191,93,198,204,146,47,44,
                                 45,222,2,220,85,9,130,51,201,96,112,196,70,
                                 88,54,134,84,94,162,56,255,144,200,179,131,
                                 200,29,61,35,155,182,128,231,21,155,237,107,
                                 221,227,153,226,188,36,110,219,85,152,82,44,
                                 118,186,13,127,104,198,251,214,218,99,130,
                                 234,42,128,242,102,120,217,175,66,222,162,
                                 85,28,99,237,30,96,142,188,160,220,17,50,5,
                                 169,231,114,171,155,206,181,203,93,114,200,
                                 225,35,121,249,190,36,16,225,102,230,105,29,
                                 151,160,141,48,74,181,138,153,119,222,150,
                                 103,3,179,110,178,89,173,171,183,184,160,
                                 110,65,57,168,17,218,89,225,87,87,233,78,
                                 103,205,70,145,146,248,227,184,26,80,201,
                                 234,140,45,43,174,45,67,184,20,67,105,212,
                                 47,15,245,246,33,203,134,58,69,22,95,49,32,
                                 70,119,44,53,119,196,4,91,24,148,226,127,
                                 235,181,244,32,136,221,246,168,236,45,99,
                                 186,28,194,246,151,13,94,240,31,88,169,90,
                                 67,197,229,89,252,123,32,98,229,192,54,198,
                                 168,26,171,92,82,4,176,252,49,65,196,93,48,
                                 35,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,
                                 15,1,1,255,4,4,3,2,2,164,48,19,6,3,85,29,37,
                                 4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,
                                 85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,
                                 42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 143,245,168,239,120,74,180,213,131,42,255,
                                 48,165,16,8,196,221,48,155,26,172,253,187,
                                 251,227,93,216,82,64,174,235,60,146,220,43,
                                 110,116,119,206,139,234,120,243,145,45,159,
                                 126,182,155,108,225,142,55,238,20,188,213,
                                 184,77,75,155,220,154,167,78,228,165,35,192,
                                 122,174,33,250,214,74,150,61,240,159,68,45,
                                 253,218,253,113,0,134,144,35,95,227,90,197,
                                 184,29,10,34,93,185,162,210,232,138,174,193,
                                 42,34,223,23,23,153,203,92,59,4,121,102,30,
                                 183,41,235,56,55,85,83,208,3,120,51,100,19,
                                 111,128,59,34,211,188,63,248,121,64,209,161,
                                 8,67,132,46,111,135,190,65,187,216,38,17,95,
                                 158,124,210,130,193,120,236,227,16,12,60,
                                 102,202,160,217,214,6,76,123,6,217,244,250,
                                 44,136,215,217,126,142,188,197,214,185,67,
                                 104,126,169,23,61,165,118,169,186,8,191,159,
                                 219,166,66,83,212,188,185,115,205,11,181,34,
                                 53,122,250,13,152,245,178,127,228,210,170,
                                 158,108,52,103,30,83,182,63,236,24,167,145,
                                 191,249,23,26,248,30,53,118,37,130,87,188,
                                 70,114,119,211,127,83,199>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:11:30.432+05:30,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2020-04-02T21:11:30.433+05:30,ns_1@127.0.0.1:<0.218.0>:restartable:start_child:98]Started child process <0.219.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2020-04-02T21:11:30.433+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.219.0>,menelaus_web}
             started: [{pid,<0.238.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 2,10,239,87,7,162,191,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,100,101,54,
                                 102,51,51,52,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,100,
                                 101,54,102,51,51,52,48,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,186,18,153,62,228,
                                 45,6,124,74,134,28,191,93,198,204,146,47,44,
                                 45,222,2,220,85,9,130,51,201,96,112,196,70,
                                 88,54,134,84,94,162,56,255,144,200,179,131,
                                 200,29,61,35,155,182,128,231,21,155,237,107,
                                 221,227,153,226,188,36,110,219,85,152,82,44,
                                 118,186,13,127,104,198,251,214,218,99,130,
                                 234,42,128,242,102,120,217,175,66,222,162,
                                 85,28,99,237,30,96,142,188,160,220,17,50,5,
                                 169,231,114,171,155,206,181,203,93,114,200,
                                 225,35,121,249,190,36,16,225,102,230,105,29,
                                 151,160,141,48,74,181,138,153,119,222,150,
                                 103,3,179,110,178,89,173,171,183,184,160,
                                 110,65,57,168,17,218,89,225,87,87,233,78,
                                 103,205,70,145,146,248,227,184,26,80,201,
                                 234,140,45,43,174,45,67,184,20,67,105,212,
                                 47,15,245,246,33,203,134,58,69,22,95,49,32,
                                 70,119,44,53,119,196,4,91,24,148,226,127,
                                 235,181,244,32,136,221,246,168,236,45,99,
                                 186,28,194,246,151,13,94,240,31,88,169,90,
                                 67,197,229,89,252,123,32,98,229,192,54,198,
                                 168,26,171,92,82,4,176,252,49,65,196,93,48,
                                 35,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,
                                 15,1,1,255,4,4,3,2,2,164,48,19,6,3,85,29,37,
                                 4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,
                                 85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,
                                 42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 143,245,168,239,120,74,180,213,131,42,255,
                                 48,165,16,8,196,221,48,155,26,172,253,187,
                                 251,227,93,216,82,64,174,235,60,146,220,43,
                                 110,116,119,206,139,234,120,243,145,45,159,
                                 126,182,155,108,225,142,55,238,20,188,213,
                                 184,77,75,155,220,154,167,78,228,165,35,192,
                                 122,174,33,250,214,74,150,61,240,159,68,45,
                                 253,218,253,113,0,134,144,35,95,227,90,197,
                                 184,29,10,34,93,185,162,210,232,138,174,193,
                                 42,34,223,23,23,153,203,92,59,4,121,102,30,
                                 183,41,235,56,55,85,83,208,3,120,51,100,19,
                                 111,128,59,34,211,188,63,248,121,64,209,161,
                                 8,67,132,46,111,135,190,65,187,216,38,17,95,
                                 158,124,210,130,193,120,236,227,16,12,60,
                                 102,202,160,217,214,6,76,123,6,217,244,250,
                                 44,136,215,217,126,142,188,197,214,185,67,
                                 104,126,169,23,61,165,118,169,186,8,191,159,
                                 219,166,66,83,212,188,185,115,205,11,181,34,
                                 53,122,250,13,152,245,178,127,228,210,170,
                                 158,108,52,103,30,83,182,63,236,24,167,145,
                                 191,249,23,26,248,30,53,118,37,130,87,188,
                                 70,114,119,211,127,83,199>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:30.434+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.218.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:30.434+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.214.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:30.440+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.256.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:30.441+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.259.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:30.444+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.261.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:11:30.445+05:30,ns_1@127.0.0.1:users_replicator<0.261.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-04-02T21:11:30.446+05:30,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.261.0>
[ns_server:debug,2020-04-02T21:11:30.446+05:30,ns_1@127.0.0.1:users_replicator<0.261.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.262.0>
[ns_server:debug,2020-04-02T21:11:30.447+05:30,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-04-02T21:11:30.447+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.262.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:30.447+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.260.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-04-02T21:11:30.448+05:30,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-04-02T21:11:30.448+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.264.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:30.453+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.267.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-04-02T21:11:30.453+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.258.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:30.456+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.271.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-04-02T21:11:30.456+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.272.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2020-04-02T21:11:30.464+05:30,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-04-02T21:11:30.464+05:30,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 17ms
[ns_server:debug,2020-04-02T21:11:30.466+05:30,ns_1@127.0.0.1:users_replicator<0.261.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2020-04-02T21:11:30.472+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.270.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-04-02T21:11:30.472+05:30,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.275.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[error_logger:info,2020-04-02T21:11:30.472+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:30.472+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-04-02T21:11:30.472+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2970197602.174325762.60627>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:11:30.472+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2970197602.174325762.60627>,
                                  inet_tcp_dist,<0.278.0>,
                                  #Ref<0.2970197602.174325762.60631>}
[ns_server:debug,2020-04-02T21:11:30.472+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2970197602.174325762.60627>,
                               inet_tcp_dist,<0.278.0>,
                               #Ref<0.2970197602.174325762.60631>}
[error_logger:info,2020-04-02T21:11:30.472+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.278.0>,shutdown}}
[ns_server:debug,2020-04-02T21:11:30.473+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-04-02T21:11:30.473+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:30.673+05:30,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-04-02T21:11:30.673+05:30,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-04-02T21:11:30.673+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2970197602.174325761.61089>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-04-02T21:11:30.673+05:30,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2970197602.174325761.61089>,
                                  inet_tcp_dist,<0.281.0>,
                                  #Ref<0.2970197602.174325761.61093>}
[ns_server:debug,2020-04-02T21:11:30.711+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-04-02T21:11:30.912+05:30,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
